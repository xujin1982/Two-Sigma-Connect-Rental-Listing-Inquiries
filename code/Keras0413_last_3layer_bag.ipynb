{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,ParametricSoftplus,ThresholdedReLU,SReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 222) (74659, 222) (49352, 3)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "\n",
    "train_df=pd.read_json('../input/train.json').reset_index(drop = True)\n",
    "target_num_map = {'high':2, 'medium':1, 'low':0}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "train_X = pd.read_csv(data_path + 'train_CV_MS_52571.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_CV_MS_52571.csv')\n",
    "\n",
    "\n",
    "\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "\n",
    "ntrain = train_X.shape[0]\n",
    "sub_id = test_X_0322.listing_id.astype('int32').values\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 223)\n",
      "(74659, 223)\n"
     ]
    }
   ],
   "source": [
    "time_feature = pd.read_csv(data_path + 'listing_image_time.csv')\n",
    "time_feature.columns = ['listing_id','time_stamp']\n",
    "train_X = train_X.merge(time_feature,on='listing_id',how='left')\n",
    "test_X = test_X.merge(time_feature,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 223)\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.concat([train_X,test_X])\n",
    "print full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['latitude', 'longitude', 'num_pricePerBed', 'num_bedBathSum',\n",
       "       'num_pricePerBath', 'num_pricePerRoom', 'num_bedPerBath',\n",
       "       'num_bedBathDiff', 'num_bedsPerc', 'num_photo_count',\n",
       "       'num_features', 'num_desc_wordcount', 'num_desc_length_null',\n",
       "       'listing_id', 'num_dist_from_center', 'num_OutlierAggregated',\n",
       "       'num_pos_density', 'num_building_null', 'num_created_weekday',\n",
       "       'num_created_weekofyear', 'num_created_day', 'num_created_month',\n",
       "       'num_created_hour', 'num_bathrooms', 'num_bedrooms', 'num_price',\n",
       "       'num_price_q', 'num_priceXroom', 'num_even_bathrooms',\n",
       "       'display_address', 'manager_id', 'building_id', 'street_address',\n",
       "       'position', 'num_location_6_3', 'num_location_6_1',\n",
       "       'num_location_6_0', 'num_location_6_5', 'num_location_6_4',\n",
       "       'num_location_6_2', 'num_room_type_0', 'num_room_type_1',\n",
       "       'num_room_type_2', 'num_room_type_3', 'num_room_type_4',\n",
       "       'num_room_type_5', 'num_room_type_6', 'num_room_type_7',\n",
       "       'num_room_type_8', 'num_room_type_9', 'num_room_type_10',\n",
       "       'num_room_type_11', 'num_room_type_12', 'num_room_type_13',\n",
       "       'num_room_type_14', 'num_room_type_15', 'num_room_type_16',\n",
       "       'num_room_type_17', 'num_room_type_18', 'num_room_type_19',\n",
       "       'num_6_median_price', 'num_6_price_ratio', 'num_6_price_diff',\n",
       "       'num_6_median_price_bedroom', 'num_6_price_ratio_bedroom',\n",
       "       'num_6_price_diff_bedroom', 'feature_washer', 'feature_laundry',\n",
       "       'feature_prewar', 'feature_furnished', 'feature_parking',\n",
       "       'feature_utilities', 'feature_elevator', 'feature_marble',\n",
       "       'feature_concierge', 'feature_cats', 'feature_health',\n",
       "       'feature_pool', 'feature_onemounthfree', 'feature_parquet',\n",
       "       'feature_lowfee', 'feature_luxury', 'feature_nofee',\n",
       "       'feature_fireplace', 'feature_dogs', 'feature_transport',\n",
       "       'feature_loft', 'median_price_bed', 'ratio_bed', 'compound', 'neg',\n",
       "       'neu', 'pos', 'street', 'avenue', 'east', 'west', 'north', 'south',\n",
       "       'other_address', 'Zero_building_id', 'top_10_building',\n",
       "       'top_25_building', 'top_5_building', 'top_50_building',\n",
       "       'top_1_building', 'top_2_building', 'top_15_building',\n",
       "       'top_20_building', 'top_30_building', 'manager_level_low',\n",
       "       'manager_level_medium', 'manager_level_high',\n",
       "       'manager_id_price_low_median', 'manager_id_price_medium_median',\n",
       "       'manager_id_price_high_median', 'manager_id_price_low_mean',\n",
       "       'manager_id_price_medium_mean', 'manager_id_price_high_mean',\n",
       "       'manager_id_price_low_max', 'manager_id_price_medium_max',\n",
       "       'manager_id_price_high_max', 'manager_id_price_low_min',\n",
       "       'manager_id_price_medium_min', 'manager_id_price_high_min',\n",
       "       'manager_id_num_created_hour_low_median',\n",
       "       'manager_id_num_created_hour_medium_median',\n",
       "       'manager_id_num_created_hour_high_median',\n",
       "       'manager_id_num_created_hour_low_mean',\n",
       "       'manager_id_num_created_hour_medium_mean',\n",
       "       'manager_id_num_created_hour_high_mean',\n",
       "       'manager_id_num_created_hour_low_max',\n",
       "       'manager_id_num_created_hour_medium_max',\n",
       "       'manager_id_num_created_hour_high_max',\n",
       "       'manager_id_num_created_hour_low_min',\n",
       "       'manager_id_num_created_hour_medium_min',\n",
       "       'manager_id_num_created_hour_high_min',\n",
       "       'manager_id_num_6_price_diff_bedroom_low_median',\n",
       "       'manager_id_num_6_price_diff_bedroom_medium_median',\n",
       "       'manager_id_num_6_price_diff_bedroom_high_median',\n",
       "       'manager_id_num_6_price_diff_bedroom_low_mean',\n",
       "       'manager_id_num_6_price_diff_bedroom_medium_mean',\n",
       "       'manager_id_num_6_price_diff_bedroom_high_mean',\n",
       "       'manager_id_num_6_price_diff_bedroom_low_max',\n",
       "       'manager_id_num_6_price_diff_bedroom_medium_max',\n",
       "       'manager_id_num_6_price_diff_bedroom_high_max',\n",
       "       'manager_id_num_6_price_diff_bedroom_low_min',\n",
       "       'manager_id_num_6_price_diff_bedroom_medium_min',\n",
       "       'manager_id_num_6_price_diff_bedroom_high_min',\n",
       "       'manager_id_bedrooms_low_median',\n",
       "       'manager_id_bedrooms_medium_median',\n",
       "       'manager_id_bedrooms_high_median', 'manager_id_bedrooms_low_mean',\n",
       "       'manager_id_bedrooms_medium_mean', 'manager_id_bedrooms_high_mean',\n",
       "       'manager_id_bedrooms_low_max', 'manager_id_bedrooms_medium_max',\n",
       "       'manager_id_bedrooms_high_max', 'manager_id_bedrooms_low_min',\n",
       "       'manager_id_bedrooms_medium_min', 'manager_id_bedrooms_high_min',\n",
       "       'manager_id_num_photo_count_low_median',\n",
       "       'manager_id_num_photo_count_medium_median',\n",
       "       'manager_id_num_photo_count_high_median',\n",
       "       'manager_id_num_photo_count_low_mean',\n",
       "       'manager_id_num_photo_count_medium_mean',\n",
       "       'manager_id_num_photo_count_high_mean',\n",
       "       'manager_id_num_photo_count_low_max',\n",
       "       'manager_id_num_photo_count_medium_max',\n",
       "       'manager_id_num_photo_count_high_max',\n",
       "       'manager_id_num_photo_count_low_min',\n",
       "       'manager_id_num_photo_count_medium_min',\n",
       "       'manager_id_num_photo_count_high_min',\n",
       "       'manager_id_Zero_building_id_low_median',\n",
       "       'manager_id_Zero_building_id_medium_median',\n",
       "       'manager_id_Zero_building_id_high_median',\n",
       "       'manager_id_Zero_building_id_low_mean',\n",
       "       'manager_id_Zero_building_id_medium_mean',\n",
       "       'manager_id_Zero_building_id_high_mean',\n",
       "       'manager_id_Zero_building_id_low_max',\n",
       "       'manager_id_Zero_building_id_medium_max',\n",
       "       'manager_id_Zero_building_id_high_max',\n",
       "       'manager_id_Zero_building_id_low_min',\n",
       "       'manager_id_Zero_building_id_medium_min',\n",
       "       'manager_id_Zero_building_id_high_min',\n",
       "       'manager_id_feature_nofee_low_median',\n",
       "       'manager_id_feature_nofee_medium_median',\n",
       "       'manager_id_feature_nofee_high_median',\n",
       "       'manager_id_feature_nofee_low_mean',\n",
       "       'manager_id_feature_nofee_medium_mean',\n",
       "       'manager_id_feature_nofee_high_mean',\n",
       "       'manager_id_feature_nofee_low_max',\n",
       "       'manager_id_feature_nofee_medium_max',\n",
       "       'manager_id_feature_nofee_high_max',\n",
       "       'manager_id_feature_nofee_low_min',\n",
       "       'manager_id_feature_nofee_medium_min',\n",
       "       'manager_id_feature_nofee_high_min',\n",
       "       'manager_id_longitude_low_median',\n",
       "       'manager_id_longitude_medium_median',\n",
       "       'manager_id_longitude_high_median', 'manager_id_longitude_low_mean',\n",
       "       'manager_id_longitude_medium_mean',\n",
       "       'manager_id_longitude_high_mean', 'manager_id_longitude_low_max',\n",
       "       'manager_id_longitude_medium_max', 'manager_id_longitude_high_max',\n",
       "       'manager_id_longitude_low_min', 'manager_id_longitude_medium_min',\n",
       "       'manager_id_longitude_high_min', 'manager_id_latitude_low_median',\n",
       "       'manager_id_latitude_medium_median',\n",
       "       'manager_id_latitude_high_median', 'manager_id_latitude_low_mean',\n",
       "       'manager_id_latitude_medium_mean', 'manager_id_latitude_high_mean',\n",
       "       'manager_id_latitude_low_max', 'manager_id_latitude_medium_max',\n",
       "       'manager_id_latitude_high_max', 'manager_id_latitude_low_min',\n",
       "       'manager_id_latitude_medium_min', 'manager_id_latitude_high_min',\n",
       "       'num_nan', 'time_stamp'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_to_use = ['latitude', 'longitude', 'num_pricePerBed', 'num_bedBathSum',\n",
    "       'num_pricePerBath', 'num_pricePerRoom', 'num_bedPerBath',\n",
    "       'num_bedBathDiff', 'num_bedsPerc', 'num_photo_count',\n",
    "       'num_features', 'num_desc_wordcount', \n",
    "#                'num_desc_length_null',\n",
    "       'listing_id', 'num_dist_from_center', 'num_OutlierAggregated',\n",
    "       'num_pos_density', \n",
    "#                'num_building_null', \n",
    "               'num_created_weekday',\n",
    "       'num_created_weekofyear', 'num_created_day', 'num_created_month',\n",
    "       'num_created_hour', 'num_bathrooms', 'num_bedrooms', 'num_price',\n",
    "       'num_price_q', 'num_priceXroom', \n",
    "#                'num_even_bathrooms',\n",
    "       'display_address', 'manager_id', 'building_id', 'street_address',\n",
    "       'position', \n",
    "#                'num_location_6_3', 'num_location_6_1',\n",
    "#        'num_location_6_0', 'num_location_6_5', 'num_location_6_4',\n",
    "#        'num_location_6_2', 'num_room_type_0', 'num_room_type_1',\n",
    "#        'num_room_type_2', 'num_room_type_3', 'num_room_type_4',\n",
    "#        'num_room_type_5', 'num_room_type_6', 'num_room_type_7',\n",
    "#        'num_room_type_8', 'num_room_type_9', 'num_room_type_10',\n",
    "#        'num_room_type_11', 'num_room_type_12', 'num_room_type_13',\n",
    "#        'num_room_type_14', 'num_room_type_15', 'num_room_type_16',\n",
    "#        'num_room_type_17', 'num_room_type_18', 'num_room_type_19',\n",
    "       'num_6_median_price', 'num_6_price_ratio', 'num_6_price_diff',\n",
    "       'num_6_median_price_bedroom', 'num_6_price_ratio_bedroom',\n",
    "       'num_6_price_diff_bedroom', \n",
    "#                'feature_washer', 'feature_laundry',\n",
    "#        'feature_prewar', 'feature_furnished', 'feature_parking',\n",
    "#        'feature_utilities', 'feature_elevator', 'feature_marble',\n",
    "#        'feature_concierge', 'feature_cats', 'feature_health',\n",
    "#        'feature_pool', 'feature_onemounthfree', 'feature_parquet',\n",
    "#        'feature_lowfee', 'feature_luxury', 'feature_nofee',\n",
    "#        'feature_fireplace', 'feature_dogs', 'feature_transport',\n",
    "#        'feature_loft', \n",
    "               'median_price_bed', 'ratio_bed', 'compound', 'neg',\n",
    "       'neu', 'pos', \n",
    "#                'street', 'avenue', 'east', 'west', 'north', 'south',\n",
    "#        'other_address', 'Zero_building_id', 'top_10_building',\n",
    "#        'top_25_building', 'top_5_building', 'top_50_building',\n",
    "#        'top_1_building', 'top_2_building', 'top_15_building',\n",
    "#        'top_20_building', 'top_30_building', \n",
    "               'manager_level_low',\n",
    "       'manager_level_medium', 'manager_level_high',\n",
    "       'manager_id_price_low_median', 'manager_id_price_medium_median',\n",
    "       'manager_id_price_high_median', 'manager_id_price_low_mean',\n",
    "       'manager_id_price_medium_mean', 'manager_id_price_high_mean',\n",
    "       'manager_id_price_low_max', 'manager_id_price_medium_max',\n",
    "       'manager_id_price_high_max', 'manager_id_price_low_min',\n",
    "       'manager_id_price_medium_min', 'manager_id_price_high_min',\n",
    "       'manager_id_num_created_hour_low_median',\n",
    "       'manager_id_num_created_hour_medium_median',\n",
    "       'manager_id_num_created_hour_high_median',\n",
    "       'manager_id_num_created_hour_low_mean',\n",
    "       'manager_id_num_created_hour_medium_mean',\n",
    "       'manager_id_num_created_hour_high_mean',\n",
    "       'manager_id_num_created_hour_low_max',\n",
    "       'manager_id_num_created_hour_medium_max',\n",
    "       'manager_id_num_created_hour_high_max',\n",
    "       'manager_id_num_created_hour_low_min',\n",
    "       'manager_id_num_created_hour_medium_min',\n",
    "       'manager_id_num_created_hour_high_min',\n",
    "       'manager_id_num_6_price_diff_bedroom_low_median',\n",
    "       'manager_id_num_6_price_diff_bedroom_medium_median',\n",
    "       'manager_id_num_6_price_diff_bedroom_high_median',\n",
    "       'manager_id_num_6_price_diff_bedroom_low_mean',\n",
    "       'manager_id_num_6_price_diff_bedroom_medium_mean',\n",
    "       'manager_id_num_6_price_diff_bedroom_high_mean',\n",
    "       'manager_id_num_6_price_diff_bedroom_low_max',\n",
    "       'manager_id_num_6_price_diff_bedroom_medium_max',\n",
    "       'manager_id_num_6_price_diff_bedroom_high_max',\n",
    "       'manager_id_num_6_price_diff_bedroom_low_min',\n",
    "       'manager_id_num_6_price_diff_bedroom_medium_min',\n",
    "       'manager_id_num_6_price_diff_bedroom_high_min',\n",
    "       'manager_id_bedrooms_low_median',\n",
    "       'manager_id_bedrooms_medium_median',\n",
    "       'manager_id_bedrooms_high_median', 'manager_id_bedrooms_low_mean',\n",
    "       'manager_id_bedrooms_medium_mean', 'manager_id_bedrooms_high_mean',\n",
    "       'manager_id_bedrooms_low_max', 'manager_id_bedrooms_medium_max',\n",
    "       'manager_id_bedrooms_high_max', 'manager_id_bedrooms_low_min',\n",
    "       'manager_id_bedrooms_medium_min', 'manager_id_bedrooms_high_min',\n",
    "       'manager_id_num_photo_count_low_median',\n",
    "       'manager_id_num_photo_count_medium_median',\n",
    "       'manager_id_num_photo_count_high_median',\n",
    "       'manager_id_num_photo_count_low_mean',\n",
    "       'manager_id_num_photo_count_medium_mean',\n",
    "       'manager_id_num_photo_count_high_mean',\n",
    "       'manager_id_num_photo_count_low_max',\n",
    "       'manager_id_num_photo_count_medium_max',\n",
    "       'manager_id_num_photo_count_high_max',\n",
    "       'manager_id_num_photo_count_low_min',\n",
    "       'manager_id_num_photo_count_medium_min',\n",
    "       'manager_id_num_photo_count_high_min',\n",
    "       'manager_id_Zero_building_id_low_median',\n",
    "       'manager_id_Zero_building_id_medium_median',\n",
    "       'manager_id_Zero_building_id_high_median',\n",
    "       'manager_id_Zero_building_id_low_mean',\n",
    "       'manager_id_Zero_building_id_medium_mean',\n",
    "       'manager_id_Zero_building_id_high_mean',\n",
    "       'manager_id_Zero_building_id_low_max',\n",
    "       'manager_id_Zero_building_id_medium_max',\n",
    "       'manager_id_Zero_building_id_high_max',\n",
    "       'manager_id_Zero_building_id_low_min',\n",
    "       'manager_id_Zero_building_id_medium_min',\n",
    "       'manager_id_Zero_building_id_high_min',\n",
    "       'manager_id_feature_nofee_low_median',\n",
    "       'manager_id_feature_nofee_medium_median',\n",
    "       'manager_id_feature_nofee_high_median',\n",
    "       'manager_id_feature_nofee_low_mean',\n",
    "       'manager_id_feature_nofee_medium_mean',\n",
    "       'manager_id_feature_nofee_high_mean',\n",
    "       'manager_id_feature_nofee_low_max',\n",
    "       'manager_id_feature_nofee_medium_max',\n",
    "       'manager_id_feature_nofee_high_max',\n",
    "       'manager_id_feature_nofee_low_min',\n",
    "       'manager_id_feature_nofee_medium_min',\n",
    "       'manager_id_feature_nofee_high_min',\n",
    "       'manager_id_longitude_low_median',\n",
    "       'manager_id_longitude_medium_median',\n",
    "       'manager_id_longitude_high_median', 'manager_id_longitude_low_mean',\n",
    "       'manager_id_longitude_medium_mean',\n",
    "       'manager_id_longitude_high_mean', 'manager_id_longitude_low_max',\n",
    "       'manager_id_longitude_medium_max', 'manager_id_longitude_high_max',\n",
    "       'manager_id_longitude_low_min', 'manager_id_longitude_medium_min',\n",
    "       'manager_id_longitude_high_min', 'manager_id_latitude_low_median',\n",
    "       'manager_id_latitude_medium_median',\n",
    "       'manager_id_latitude_high_median', 'manager_id_latitude_low_mean',\n",
    "       'manager_id_latitude_medium_mean', 'manager_id_latitude_high_mean',\n",
    "       'manager_id_latitude_low_max', 'manager_id_latitude_medium_max',\n",
    "       'manager_id_latitude_high_max', 'manager_id_latitude_low_min',\n",
    "       'manager_id_latitude_medium_min', 'manager_id_latitude_high_min',\n",
    "       'num_nan','time_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 223)\n",
      "(74659, 223)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_data.fillna(0)\n",
    "\n",
    "for col in feat_to_use:\n",
    "    full_data.loc[:,col] = (full_data[col]-full_data[col].mean())/full_data[col].std()\n",
    "train_df_nn = full_data[:ntrain]\n",
    "test_df_nn = full_data[ntrain:]\n",
    "\n",
    "train_df_nn = sparse.csr_matrix(train_df_nn)\n",
    "test_df_nn = sparse.csr_matrix(test_df_nn)\n",
    "\n",
    "\n",
    "print train_df_nn.shape\n",
    "print test_df_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df_nn, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.8929"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/keras/engine/training.py:1480: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49408/49352 [==============================] - 6s - loss: 0.8923 - val_loss: 0.6193\n",
      "Epoch 2/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.6667 - val_loss: 0.5957\n",
      "Epoch 3/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.6253 - val_loss: 0.5887\n",
      "Epoch 4/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.6110 - val_loss: 0.5788\n",
      "Epoch 5/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.6035 - val_loss: 0.5749\n",
      "Epoch 6/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.6016 - val_loss: 0.5755\n",
      "Epoch 7/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5939 - val_loss: 0.5707\n",
      "Epoch 8/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5932 - val_loss: 0.5668\n",
      "Epoch 9/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5867 - val_loss: 0.5663\n",
      "Epoch 10/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5880 - val_loss: 0.5635\n",
      "Epoch 11/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5838 - val_loss: 0.5631\n",
      "Epoch 12/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5813 - val_loss: 0.5620\n",
      "Epoch 13/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5788 - val_loss: 0.5600\n",
      "Epoch 14/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5756 - val_loss: 0.5538\n",
      "Epoch 15/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5761 - val_loss: 0.5571\n",
      "Epoch 16/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5733 - val_loss: 0.5523\n",
      "Epoch 17/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5710 - val_loss: 0.5523\n",
      "Epoch 18/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5675 - val_loss: 0.5512\n",
      "Epoch 19/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5709 - val_loss: 0.5464\n",
      "Epoch 20/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5680 - val_loss: 0.5470\n",
      "Epoch 21/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5675 - val_loss: 0.5467\n",
      "Epoch 22/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5617 - val_loss: 0.5437\n",
      "Epoch 23/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5609 - val_loss: 0.5470\n",
      "Epoch 24/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5637 - val_loss: 0.5449\n",
      "Epoch 25/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5593 - val_loss: 0.5416\n",
      "Epoch 26/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5610 - val_loss: 0.5405\n",
      "Epoch 27/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5539 - val_loss: 0.5408\n",
      "Epoch 28/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5593 - val_loss: 0.5416\n",
      "Epoch 29/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5557 - val_loss: 0.5415\n",
      "Epoch 30/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5559 - val_loss: 0.5404\n",
      "Epoch 31/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5543 - val_loss: 0.5371\n",
      "Epoch 32/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5527 - val_loss: 0.5387\n",
      "Epoch 33/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5533 - val_loss: 0.5373\n",
      "Epoch 34/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5483 - val_loss: 0.5373\n",
      "Epoch 35/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5523 - val_loss: 0.5364\n",
      "Epoch 36/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5492 - val_loss: 0.5361\n",
      "Epoch 37/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5511 - val_loss: 0.5356\n",
      "Epoch 38/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5506 - val_loss: 0.5355\n",
      "Epoch 39/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5486 - val_loss: 0.5340\n",
      "Epoch 40/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5474 - val_loss: 0.5366\n",
      "Epoch 41/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5438 - val_loss: 0.5345\n",
      "Epoch 42/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5456 - val_loss: 0.5341\n",
      "Epoch 43/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5442 - val_loss: 0.5339\n",
      "Epoch 44/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5445 - val_loss: 0.5335\n",
      "Epoch 45/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5440 - val_loss: 0.5332\n",
      "Epoch 46/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5415 - val_loss: 0.5326\n",
      "Epoch 47/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5425 - val_loss: 0.5328\n",
      "Epoch 48/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5427 - val_loss: 0.5332\n",
      "Epoch 49/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5419 - val_loss: 0.5339\n",
      "Epoch 50/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5391 - val_loss: 0.5323\n",
      "Epoch 51/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5388 - val_loss: 0.5329\n",
      "Epoch 52/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5385 - val_loss: 0.5325\n",
      "Epoch 53/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5387 - val_loss: 0.5317\n",
      "Epoch 54/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5337 - val_loss: 0.5321\n",
      "Epoch 55/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5396 - val_loss: 0.5326\n",
      "Epoch 56/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5366 - val_loss: 0.5322\n",
      "Epoch 57/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5379 - val_loss: 0.5314\n",
      "Epoch 58/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5386 - val_loss: 0.5332\n",
      "Epoch 59/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5311 - val_loss: 0.5328\n",
      "Epoch 60/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5339 - val_loss: 0.5310\n",
      "Epoch 61/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5319 - val_loss: 0.5312\n",
      "Epoch 62/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5341 - val_loss: 0.5307\n",
      "Epoch 63/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5339 - val_loss: 0.5319\n",
      "Epoch 64/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5327 - val_loss: 0.5310\n",
      "Epoch 65/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5311 - val_loss: 0.5309\n",
      "Epoch 66/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5319 - val_loss: 0.5329\n",
      "Epoch 67/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5291 - val_loss: 0.5305\n",
      "Epoch 68/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5317 - val_loss: 0.5324\n",
      "Epoch 69/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5304 - val_loss: 0.5311\n",
      "Epoch 70/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5243 - val_loss: 0.5312\n",
      "Epoch 71/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5296 - val_loss: 0.5315\n",
      "Epoch 72/1000\n",
      "49408/49352 [==============================] - 6s - loss: 0.5256 - val_loss: 0.5318\n",
      "Epoch 73/1000\n",
      "49408/49352 [==============================] - 7s - loss: 0.5295 - val_loss: 0.5312\n",
      "0.530487747169\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    \n",
    "    model.add(Dense(100, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim, # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(40,init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(15,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                   init = init,\n",
    "                   activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "#                   metrics=[mae_log],\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=ntrain,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )\n",
    "\n",
    "print min(fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 150 0.4 50 0.4 15 0.4 'glorot_uniform' 'adam' 0.543332518312\n",
    "# 150 0.5 50 0.5 15 0.5 'glorot_uniform' 'adam' 0.544221234441\n",
    "# 100 0.5 25 0.5 9 0.5 'glorot_uniform' 'adam' 0.54719870663\n",
    "# 100 0.4 30 0.4 9 0.4 'glorot_uniform' 'adam' 0.544765821466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 0.5 50 0.5 'glorot_uniform' 'adam' 0.54511465921\n",
    "# 100 0.5 25 0.5 'glorot_uniform' 'adam' 0.543118116239\n",
    "# 100 0.4 30 0.4 'glorot_uniform' 'adam' 0.541240284411\n",
    "# 100 0.4 30 0.4 'glorot_uniform' 'Adamax' 0.542234674134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 0.5 'glorot_uniform' 'adam' 0.543593994089\n",
    "# 100 0.4 'glorot_uniform' 'adam' 0.542798319778\n",
    "# 100 0.3 'glorot_uniform' 'adam' 0.54412619796\n",
    "# 100 0.2 'glorot_uniform' 'adam' 0.547258452084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 70 0.4 'glorot_uniform' 'adam' 0.544325359686\n",
    "# 100 0.4 'glorot_uniform' 'adam' 0.542798319778\n",
    "# 150 0.4 'glorot_uniform' 'adam' 0.544117797527\n",
    "# 200 0.4 'glorot_uniform' 'adam' 0.545132525568\n",
    "# 300 0.4 'glorot_uniform' 'adam' 0.545077440339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 0.4 'glorot_uniform' 'adam' 0.542798319778\n",
    "# 100 0.4 'glorot_uniform' 'RMSprop' 0.547544663425\n",
    "# 100 0.4 'glorot_uniform' 'Adamax' 0.542127071416\n",
    "# 100 0.4 'glorot_uniform' 'Nadam' 0.544927256042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 0.4 'he_uniform' 'adam' 0.54419020321\n",
    "# 100 0.4 'he_normal' 'adam' 0.543867479612\n",
    "# 100 0.4 'glorot_uniform' 'adam' 0.542798319778\n",
    "# 100 0.4 'glorot_normal' 'adam' 0.546524272962\n",
    "# 100 0.4 'lecun_uniform' 'adam' 0.544478366113\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 150 0.4 50 0.5 20 0.5 'he_normal'  val 0.543127303723\n",
    "# 150 0.4 50 0.5 20 0.5 'he_uniform'  val 0.542279646729\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<74659x412 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 6084908 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.hdf5\")\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict_proba(x=test_df_nn.toarray(),batch_size = 128,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.98445845e-01,   4.90590602e-01,   1.10963546e-01],\n",
       "       [  9.91283834e-01,   8.24213494e-03,   4.74051310e-04],\n",
       "       [  9.88962233e-01,   1.05425483e-02,   4.95240442e-04],\n",
       "       ..., \n",
       "       [  9.92257774e-01,   7.49900285e-03,   2.43204151e-04],\n",
       "       [  9.84745502e-01,   1.47970403e-02,   4.57483169e-04],\n",
       "       [  6.17200851e-01,   3.49761784e-01,   3.30373496e-02]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_Keras_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(pred_y)\n",
    "out_df.columns = [\"low\", \"medium\",\"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    model.add(Dense(params['input_size'], # number of input units: needs to be tuned\n",
    "                    input_dim = params['input_dim'], # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(params['input_drop_out'])) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(params['hidden_size'],\n",
    "                    init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out'])) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(params['hidden_size1'],init=init)) # number of hidden2 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out1'])) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                    init = init,\n",
    "                    activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128,randseed=0):\n",
    "    N_params = len(parameters)\n",
    "#     print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = KFold(n_splits=fold,shuffle=True,random_state=randseed)\n",
    "    N_class = train_y.shape[1]\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    fold_start = time.time() \n",
    "\n",
    "    \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "#         print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "#             print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "\n",
    "            model = nn_model(nn_params)\n",
    "#             print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, 128, True),\n",
    "                                     nb_epoch=70,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     verbose = 0,\n",
    "                                     callbacks=[ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                                                                monitor='val_loss', \n",
    "                                                                verbose=0, save_best_only=True)]\n",
    "                                    )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "#             print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )\n",
    "            \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_proba(x=val_x_fold.toarray(),verbose=0)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "#             print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = model.predict_proba(x=test_x.toarray(),verbose=0)\n",
    "#             print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, )            \n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "#         print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print \"Score for blended models is %f in %0.3fm\" % (np.mean(scores), (time.time() - fold_start)/60)\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting............\n",
      "Score for blended models is 0.533909 in 22.402m\n",
      "Score for blended models is 0.532641 in 22.312m\n",
      "Score for blended models is 0.532599 in 22.288m\n",
      "Score for blended models is 0.532401 in 22.329m\n",
      "Score for blended models is 0.533066 in 22.199m\n",
      "Score for blended models is 0.533293 in 22.153m\n",
      "Score for blended models is 0.532767 in 22.277m\n",
      "Score for blended models is 0.532134 in 22.349m\n",
      "Score for blended models is 0.532149 in 22.332m\n",
      "Score for blended models is 0.532735 in 22.367m\n",
      "Score for blended models is 0.532439 in 22.257m\n",
      "Score for blended models is 0.533261 in 22.330m\n",
      "Score for blended models is 0.532618 in 22.324m\n",
      "Score for blended models is 0.533259 in 22.289m\n",
      "Score for blended models is 0.532986 in 22.378m\n",
      "Score for blended models is 0.533128 in 22.275m\n",
      "Score for blended models is 0.532502 in 22.281m\n",
      "Score for blended models is 0.532666 in 22.733m\n",
      "Score for blended models is 0.533299 in 22.321m\n",
      "Score for blended models is 0.532571 in 22.116m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_df_nn.shape[0],3))\n",
    "test_total = np.zeros((test_df_nn.shape[0],3))\n",
    "name_train_blend = '../tmp/train_kares.csv'\n",
    "name_test_blend = '../tmp/test_kares.csv'\n",
    "score_total = 0\n",
    "count = 20\n",
    "\n",
    "print 'Starting............'\n",
    "for n in range(count):\n",
    "    nn_parameters = [\n",
    "        { 'input_size' :100 ,\n",
    "         'input_dim' : train_X.shape[1],\n",
    "         'input_drop_out' : 0.4 ,\n",
    "         'hidden_size' : 40 ,\n",
    "         'hidden_drop_out' :0.4,\n",
    "        'hidden_size1':15,\n",
    "        'hidden_drop_out1':0.4},\n",
    "\n",
    "    ]\n",
    "\n",
    "    (train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_df_nn, train_y, test_df_nn,\n",
    "                                                             5,\n",
    "                                                             10,128,n)\n",
    "    train_total += train_blend_x\n",
    "    test_total += test_blend_x\n",
    "    score_total += np.mean(blend_scores)\n",
    "    \n",
    "\n",
    "\n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x = pd.DataFrame(train_total)\n",
    "train_blend_x.columns = [\"low\", \"medium\", \"hig\"]\n",
    "train_blend_x[\"listing_id\"] = train_X.listing_id.values\n",
    "\n",
    "test_blend_x = pd.DataFrame(test_total)\n",
    "test_blend_x.columns = [\"low\", \"medium\", \"hig\"]\n",
    "test_blend_x[\"listing_id\"] = test_X.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train = train_X_0322[['listing_id']].merge(train_blend_x,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"hig\"]].values\n",
    "tmp_test_mean = test_X_0322[['listing_id']].merge(test_blend_x,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"hig\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53257061]\n",
      "[ 59.]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_Keras_last_3layer_20bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_Keras_mean_last_3layer_20bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores,axis=0))\n",
    "print (np.mean(best_round,axis=0))\n",
    "np.savetxt(name_train_blend,tmp_train, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,tmp_test_mean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "sub_name = '../output/sub_Keras_mean_last_3layer_20bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(tmp_test_mean)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X_0322.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
