{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 428) (74659, 428) (49352L,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_csv(data_path + 'train_BM_0401.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_BM_0401.csv')\n",
    "train_y = np.ravel(pd.read_csv(data_path + 'labels_BrandenMurray.csv'))\n",
    "sub_id = test_X.listing_id.astype('int32').values\n",
    "\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 428)\n",
      "(9871, 428)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "\n",
    "# import sys  \n",
    "# stdi,stdo,stde=sys.stdin,sys.stdout,sys.stderr\n",
    "# reload(sys)  \n",
    "# sys.stdin,sys.stdout,sys.stderr=stdi,stdo,stde\n",
    "# sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[25]\tvalid_0's multi_logloss: 0.602854\n",
      "[50]\tvalid_0's multi_logloss: 0.5569\n",
      "[75]\tvalid_0's multi_logloss: 0.545154\n",
      "[100]\tvalid_0's multi_logloss: 0.539556\n",
      "[125]\tvalid_0's multi_logloss: 0.536975\n",
      "[150]\tvalid_0's multi_logloss: 0.535236\n",
      "[175]\tvalid_0's multi_logloss: 0.534269\n",
      "[200]\tvalid_0's multi_logloss: 0.533852\n",
      "[225]\tvalid_0's multi_logloss: 0.533521\n",
      "[250]\tvalid_0's multi_logloss: 0.533808\n",
      "Early stopping, best iteration is:\n",
      "[223]\tvalid_0's multi_logloss: 0.533439\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(learning_rate = 0.1)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "        \n",
    "clf = clf.fit(X_train, y_train,\n",
    "              eval_set = [(X_val,y_val)],\n",
    "              eval_metric = 'multi_logloss',\n",
    "              early_stopping_rounds = 50,\n",
    "              verbose = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.87241777e-01,   4.59754244e-01,   5.30039791e-02],\n",
       "       [  9.39061072e-01,   2.87975851e-02,   3.21413429e-02],\n",
       "       [  9.00711190e-01,   8.74221884e-02,   1.18666212e-02],\n",
       "       ..., \n",
       "       [  9.79164336e-01,   1.99874435e-02,   8.48220964e-04],\n",
       "       [  9.85101415e-01,   1.46340209e-02,   2.64564034e-04],\n",
       "       [  6.40233923e-01,   3.43340830e-01,   1.64252476e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = clf.predict_proba(test_X, num_iteration = clf.best_iteration)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# sub_name = '../output/sub_LightGBM_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "# out_df = pd.DataFrame(pred_y[:,:3])\n",
    "# out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "# out_df[\"listing_id\"] = sub_id\n",
    "# out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(learning_rate = 0.1)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "\n",
    "tmp  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  \t0.534184914604 614\n",
      "15  \t0.533866649924 384\n",
      "31  \t0.533438877335 223\n",
      "63  \t0.534966878101 128\n",
      "127  \t0.536485297438 84\n",
      "255  \t0.540957829211 64\n"
     ]
    }
   ],
   "source": [
    "for x in [8,15,31,63,127,255]:\n",
    "    clf.set_params(num_leaves = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        num_leaves = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print num_leaves\n",
    "clf.set_params(num_leaves = num_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20  \t0.532594819811 313\n",
      "30  \t0.53261320808 212\n",
      "50  \t0.533936088415 231\n",
      "70  \t0.533932672968 185\n",
      "80  \t0.53273199529 282\n",
      "90  \t0.533575376848 223\n",
      "100  \t0.532221825493 226\n",
      "110  \t0.531233211105 255\n",
      "120  \t0.532680049982 215\n",
      "150  \t0.532639182712 278\n",
      "170  \t0.532213958499 252\n",
      "200  \t0.532006011641 234\n",
      "230  \t0.532597043533 230\n",
      "260  \t0.531974482836 238\n"
     ]
    }
   ],
   "source": [
    "min_child_samples = 10\n",
    "\n",
    "for x in [20, 30, 50, 70, 80,90,100,110,120,150,170,200,230,260]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  \t0.531377819839 282\n",
      "350  \t0.530773757817 239\n",
      "400  \t0.532280104266 217\n",
      "450  \t0.530762448236 275\n",
      "500  \t0.53200893953 228\n"
     ]
    }
   ],
   "source": [
    "for x in [300,350,400,450,500]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550  \t0.535720729063 308\n",
      "600  \t0.535027375437 254\n",
      "650  \t0.536249586582 298\n",
      "700  \t0.534918784239 221\n",
      "800  \t0.53543830493 306\n"
     ]
    }
   ],
   "source": [
    "for x in [550,600,650,700,800]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=450, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print min_child_samples\n",
    "clf.set_params(min_child_samples = min_child_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2  \t0.528559521572 405\n",
      "0.3  \t0.529150420199 296\n",
      "0.4  \t0.527694767146 292\n",
      "0.5  \t0.529164261697 248\n",
      "0.6  \t0.530537690773 239\n",
      "0.7  \t0.531266177296 273\n",
      "0.8  \t0.531278855888 237\n",
      "0.9  \t0.532617743425 270\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = 1\n",
    "for x in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    clf.set_params(colsample_bytree = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        colsample_bytree = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=450, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print colsample_bytree\n",
    "\n",
    "clf.set_params(colsample_bytree = colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5  \t0.540031036293 277\n",
      "0.6  \t0.538408178072 253\n",
      "0.7  \t0.535956199754 280\n",
      "0.8  \t0.534108633235 278\n",
      "0.9  \t0.530413587987 213\n"
     ]
    }
   ],
   "source": [
    "subsample = 1.0\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    clf.set_params(subsample = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        subsample = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=450, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1.0, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print subsample\n",
    "clf.set_params(subsample = subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  \t0.530507777215 374\n",
      "31  \t0.529957865758 378\n",
      "63  \t0.527968757557 281\n",
      "127  \t0.527666103496 352\n",
      "511  \t0.529702379124 244\n",
      "1023  \t0.528091154786 283\n",
      "2047  \t0.530109284354 233\n"
     ]
    }
   ],
   "source": [
    "max_bin = 255\n",
    "\n",
    "for x in [15,31,63, 127, 511, 1023, 2047]: #[200,300,400]:#\n",
    "    clf.set_params(max_bin = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        max_bin = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  \t0.529340700139 361\n",
      "80  \t0.528698390057 371\n",
      "110  \t0.527778555464 308\n",
      "150  \t0.528243765336 309\n",
      "180  \t0.52797289274 297\n",
      "210  \t0.528762026309 260\n",
      "300  \t0.529304104592 283\n",
      "350  \t0.530034916459 240\n",
      "400  \t0.527407117844 337\n",
      "450  \t0.53038124861 331\n",
      "550  \t0.529340949386 306\n",
      "600  \t0.529412443326 309\n"
     ]
    }
   ],
   "source": [
    "for x in [50,80, 110, 150,180, 210, 300, 350,400,450, 550,600]:\n",
    "    clf.set_params(max_bin = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        max_bin = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=400, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=450, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1.0, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print max_bin\n",
    "clf.set_params(max_bin = max_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_samples |   num_leaves |   subsample | \n",
      "    1 | 01m34s | \u001b[35m  -0.52729\u001b[0m | \u001b[32m            0.3133\u001b[0m | \u001b[32m 589.9460\u001b[0m | \u001b[32m           268.8357\u001b[0m | \u001b[32m     50.7648\u001b[0m | \u001b[32m     0.8742\u001b[0m | \n",
      "    2 | 02m29s |   -0.52932 |             0.6879 |  740.7174 |            405.9792 |      11.3948 |      0.7703 | \n",
      "    3 | 02m48s |   -0.52774 |             0.5401 |  598.6027 |            375.8580 |      24.1757 |      0.8293 | \n",
      "    4 | 02m28s | \u001b[35m  -0.52714\u001b[0m | \u001b[32m            0.6169\u001b[0m | \u001b[32m 749.5119\u001b[0m | \u001b[32m           266.9397\u001b[0m | \u001b[32m     53.4135\u001b[0m | \u001b[32m     0.8691\u001b[0m | \n",
      "    5 | 01m56s |   -0.52757 |             0.5187 |  666.6244 |             95.1477 |      48.5823 |      0.7286 | \n",
      "    6 | 02m04s |   -0.52721 |             0.5061 |  333.3588 |            420.4724 |      23.3229 |      0.8764 | \n",
      "    7 | 03m31s | \u001b[35m  -0.52628\u001b[0m | \u001b[32m            0.6182\u001b[0m | \u001b[32m 633.9438\u001b[0m | \u001b[32m           163.7338\u001b[0m | \u001b[32m     63.9277\u001b[0m | \u001b[32m     0.9465\u001b[0m | \n",
      "    8 | 01m54s |   -0.52840 |             0.4254 |  617.8299 |            206.5749 |      56.5477 |      0.7769 | \n",
      "    9 | 01m56s |   -0.53293 |             0.5755 |  620.1698 |            451.4668 |      60.9436 |      0.7599 | \n",
      "   10 | 02m19s |   -0.52945 |             0.5944 |  785.0828 |            252.4206 |      75.9417 |      0.7188 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_samples |   num_leaves |   subsample | \n",
      "   11 | 02m06s | \u001b[35m  -0.52569\u001b[0m | \u001b[32m            0.4504\u001b[0m | \u001b[32m 232.3831\u001b[0m | \u001b[32m           117.1663\u001b[0m | \u001b[32m     25.5335\u001b[0m | \u001b[32m     0.8960\u001b[0m | \n",
      "   12 | 02m37s |   -0.52813 |             0.3978 |  213.7640 |            318.6238 |      27.4315 |      0.7556 | \n",
      "   13 | 01m51s |   -0.52756 |             0.5198 |  322.2758 |             82.8404 |      77.1192 |      0.9757 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  2.54016213e-05]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 01m44s | \u001b[35m  -0.52559\u001b[0m | \u001b[32m            0.4713\u001b[0m | \u001b[32m 207.0710\u001b[0m | \u001b[32m            62.3656\u001b[0m | \u001b[32m     19.4815\u001b[0m | \u001b[32m     0.7813\u001b[0m | \n",
      "   15 | 02m37s | \u001b[35m  -0.52546\u001b[0m | \u001b[32m            0.4535\u001b[0m | \u001b[32m 397.8426\u001b[0m | \u001b[32m           106.9241\u001b[0m | \u001b[32m     10.4720\u001b[0m | \u001b[32m     0.9240\u001b[0m | \n",
      "   16 | 02m59s |   -0.52699 |             0.5011 |  468.3995 |            287.2805 |      12.8670 |      0.8117 | \n",
      "   17 | 02m28s |   -0.52958 |             0.4877 |  215.4646 |            483.0462 |       8.0135 |      0.9121 | \n",
      "   18 | 03m00s |   -0.52708 |             0.4797 |  303.5365 |            227.8765 |      11.4661 |      0.8788 | \n",
      "   19 | 02m55s |   -0.52819 |             0.6839 |  487.7124 |            103.6519 |      69.0829 |      0.7694 | \n",
      "   20 | 03m24s |   -0.52557 |             0.4204 |  712.7941 |            182.3048 |      14.5081 |      0.9308 | \n",
      "   21 | 02m51s |   -0.52666 |             0.4651 |  320.2883 |            107.1398 |      14.2495 |      0.7624 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -8.29820775e-05]), 'nit': 5, 'funcalls': 60}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 02m59s |   -0.52574 |             0.6102 |  597.5082 |             93.1867 |      14.7655 |      0.8099 | \n",
      "   23 | 03m42s |   -0.52759 |             0.5549 |  593.9066 |            280.2198 |       9.8184 |      0.7990 | \n",
      "   24 | 02m11s |   -0.52586 |             0.5272 |  214.8638 |            166.8518 |      59.1982 |      0.9403 | \n",
      "   25 | 02m14s |   -0.52672 |             0.3630 |  381.0544 |            327.9845 |      50.2107 |      0.9700 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -5.54875789e-05]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 02m57s | \u001b[35m  -0.52507\u001b[0m | \u001b[32m            0.4978\u001b[0m | \u001b[32m 559.9400\u001b[0m | \u001b[32m            64.2226\u001b[0m | \u001b[32m     17.1944\u001b[0m | \u001b[32m     0.9345\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00014571]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 | 03m05s |   -0.52866 |             0.4651 |  728.8284 |            237.5077 |      12.7237 |      0.7018 | \n",
      "   28 | 02m09s |   -0.52598 |             0.4760 |  466.4442 |             78.4595 |      21.9301 |      0.7044 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00067073]), 'nit': 7, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 02m42s |   -0.52910 |             0.4290 |  657.2222 |            332.2029 |      72.5399 |      0.9146 | \n",
      "   30 | 02m10s |   -0.52796 |             0.5623 |  217.0109 |            183.0951 |      16.8085 |      0.7138 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.45410304e-05]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 | 02m41s |   -0.52923 |             0.3964 |  218.4037 |            467.5303 |      73.4901 |      0.8638 | \n",
      "   32 | 03m23s |   -0.52569 |             0.4058 |  536.0100 |             68.1054 |      13.2079 |      0.9545 | \n",
      "   33 | 02m01s |   -0.52890 |             0.3984 |  270.7654 |            304.7278 |      79.0575 |      0.8079 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00135481]), 'nit': 4, 'funcalls': 56}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00025358]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 03m40s |   -0.52642 |             0.4951 |  230.9936 |             94.5305 |      69.8697 |      0.8894 | \n",
      "   35 | 02m44s | \u001b[35m  -0.52425\u001b[0m | \u001b[32m            0.3070\u001b[0m | \u001b[32m 376.6407\u001b[0m | \u001b[32m            64.9630\u001b[0m | \u001b[32m     22.3820\u001b[0m | \u001b[32m     0.9476\u001b[0m | \n",
      "   36 | 02m26s |   -0.52871 |             0.6149 |  438.7484 |            456.7444 |      49.7697 |      0.8943 | \n",
      "   37 | 03m21s |   -0.52582 |             0.6680 |  798.6898 |             91.7745 |      14.1578 |      0.8281 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00044447]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00025702]), 'nit': 3, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 | 03m02s |   -0.52684 |             0.5978 |  792.3995 |             65.6243 |      70.4431 |      0.9093 | \n",
      "   39 | 03m09s |   -0.52639 |             0.4983 |  449.7922 |            352.9653 |      14.2125 |      0.8703 | \n",
      "   40 | 03m27s |   -0.52619 |             0.6727 |  747.0193 |            148.8777 |      27.6480 |      0.8322 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00014958]), 'nit': 7, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 03m42s |   -0.52710 |             0.6931 |  468.2321 |            335.8678 |      73.0873 |      0.9586 | \n",
      "   42 | 02m06s |   -0.52489 |             0.4257 |  421.2139 |             66.0217 |      31.6154 |      0.8991 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00041938]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 | 02m07s |   -0.52605 |             0.4459 |  715.5591 |             68.9222 |      17.5104 |      0.8731 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00104474]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44 | 01m57s |   -0.53350 |             0.4252 |  791.8792 |            457.1748 |      79.0675 |      0.7146 | \n",
      "   45 | 02m16s |   -0.52825 |             0.4205 |  374.5565 |            204.9129 |      56.4942 |      0.7647 | \n",
      "   46 | 02m47s |   -0.52777 |             0.6794 |  354.5173 |            366.8474 |      11.5122 |      0.8544 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00124031]), 'nit': 6, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47 | 02m50s |   -0.52649 |             0.6697 |  717.5593 |            145.0487 |      77.6690 |      0.9634 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00015787]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48 | 02m25s |   -0.53328 |             0.6209 |  347.2920 |            497.5252 |      64.5198 |      0.7667 | \n",
      "   49 | 01m52s |   -0.52863 |             0.3022 |  511.6160 |            384.0916 |      18.7794 |      0.7010 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -8.18923186e-05]), 'nit': 5, 'funcalls': 59}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00073678]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 | 02m43s |   -0.52509 |             0.6233 |  357.6424 |             66.2162 |      16.3892 |      0.9344 | \n"
     ]
    }
   ],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_child_samples, colsample_bytree, subsample, learning_rate=0.1):\n",
    "    skf = KFold(n_splits=5,random_state=seed)\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf.split(train_X)):\n",
    "        est=lgb.LGBMClassifier(learning_rate=0.1,\n",
    "                               max_bin=int(max_bin),\n",
    "                               num_leaves=int(num_leaves),\n",
    "                               min_child_samples=int(min_child_samples),\n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               subsample=subsample,\n",
    "                               subsample_freq = 1\n",
    "                              )\n",
    " \n",
    "        train_x_fold = train_X.iloc[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_X.iloc[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( n_estimators=100000)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                eval_set=[(val_x_fold, val_y_fold)],\n",
    "                eval_metric='multi_logloss',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False\n",
    "               )\n",
    "        val_y_predict_fold = est.predict_proba(val_x_fold)\n",
    "        score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv, \n",
    "                               {\n",
    "                                'max_bin': (200,800),\n",
    "                                'num_leaves': (8,80),\n",
    "                                'min_child_samples' :(60,500),\n",
    "                                'colsample_bytree': (0.3,0.7),\n",
    "                                'subsample' : (0.7,1)})\n",
    "\n",
    "lgbm_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  \tnum_leaves \t \tmin_child_samples \tmax_bin \tcolsample_bytree \tsubsample \tscore\n",
    "# 34 \t17.342582 \t158.175569 \t \t \t453.587691 \t0.309807 \t \t \t0.951246 \t-0.523952\n",
    "# 36 \t34.809317 \t175.689702 \t \t \t431.124869 \t0.420417 \t \t \t0.980390 \t-0.524356\n",
    "# 16 \t13.123686 \t120.179447 \t \t \t689.223522 \t0.706641 \t \t \t0.769943 \t-0.524734\n",
    "# 33 \t18.297130 \t121.709972 \t \t \t452.154081 \t0.467294 \t \t \t0.913592 \t-0.524832\n",
    "# 24 \t8.498056 \t122.380717 \t \t \t540.797144 \t0.318956 \t \t \t0.953308 \t-0.524889\n",
    "\n",
    "# 0331 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_child_samples</th>\n",
       "      <th>max_bin</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22.382049</td>\n",
       "      <td>64.962952</td>\n",
       "      <td>376.640680</td>\n",
       "      <td>0.306975</td>\n",
       "      <td>0.947558</td>\n",
       "      <td>-0.524254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31.615417</td>\n",
       "      <td>66.021705</td>\n",
       "      <td>421.213915</td>\n",
       "      <td>0.425728</td>\n",
       "      <td>0.899097</td>\n",
       "      <td>-0.524894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17.194424</td>\n",
       "      <td>64.222584</td>\n",
       "      <td>559.939981</td>\n",
       "      <td>0.497790</td>\n",
       "      <td>0.934519</td>\n",
       "      <td>-0.525071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>16.389240</td>\n",
       "      <td>66.216210</td>\n",
       "      <td>357.642431</td>\n",
       "      <td>0.623331</td>\n",
       "      <td>0.934423</td>\n",
       "      <td>-0.525089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.471950</td>\n",
       "      <td>106.924120</td>\n",
       "      <td>397.842570</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.924038</td>\n",
       "      <td>-0.525457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.508117</td>\n",
       "      <td>182.304842</td>\n",
       "      <td>712.794090</td>\n",
       "      <td>0.420441</td>\n",
       "      <td>0.930807</td>\n",
       "      <td>-0.525571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.481535</td>\n",
       "      <td>62.365550</td>\n",
       "      <td>207.070979</td>\n",
       "      <td>0.471294</td>\n",
       "      <td>0.781278</td>\n",
       "      <td>-0.525590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.533528</td>\n",
       "      <td>117.166334</td>\n",
       "      <td>232.383120</td>\n",
       "      <td>0.450393</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>-0.525687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.207873</td>\n",
       "      <td>68.105397</td>\n",
       "      <td>536.009959</td>\n",
       "      <td>0.405839</td>\n",
       "      <td>0.954524</td>\n",
       "      <td>-0.525693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.765542</td>\n",
       "      <td>93.186696</td>\n",
       "      <td>597.508214</td>\n",
       "      <td>0.610208</td>\n",
       "      <td>0.809859</td>\n",
       "      <td>-0.525743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_leaves  min_child_samples     max_bin  colsample_bytree  subsample  \\\n",
       "24   22.382049          64.962952  376.640680          0.306975   0.947558   \n",
       "31   31.615417          66.021705  421.213915          0.425728   0.899097   \n",
       "15   17.194424          64.222584  559.939981          0.497790   0.934519   \n",
       "39   16.389240          66.216210  357.642431          0.623331   0.934423   \n",
       "4    10.471950         106.924120  397.842570          0.453457   0.924038   \n",
       "9    14.508117         182.304842  712.794090          0.420441   0.930807   \n",
       "3    19.481535          62.365550  207.070979          0.471294   0.781278   \n",
       "0    25.533528         117.166334  232.383120          0.450393   0.896000   \n",
       "21   13.207873          68.105397  536.009959          0.405839   0.954524   \n",
       "11   14.765542          93.186696  597.508214          0.610208   0.809859   \n",
       "\n",
       "       score  \n",
       "24 -0.524254  \n",
       "31 -0.524894  \n",
       "15 -0.525071  \n",
       "39 -0.525089  \n",
       "4  -0.525457  \n",
       "9  -0.525571  \n",
       "3  -0.525590  \n",
       "0  -0.525687  \n",
       "21 -0.525693  \n",
       "11 -0.525743  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_bo_scores = pd.DataFrame([[s[0]['num_leaves'],\n",
    "                               s[0]['min_child_samples'],\n",
    "                               s[0]['max_bin'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[1]] for s in zip(lgbm_BO.res['all']['params'],lgbm_BO.res['all']['values'])],\n",
    "                            columns = ['num_leaves',\n",
    "                                       'min_child_samples',\n",
    "                                       'max_bin',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'score'])\n",
    "gbm_bo_scores=gbm_bo_scores.sort_values('score',ascending=False)\n",
    "gbm_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=50):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "#     train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "#     test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "#     scores = np.zeros ((fold,N_params))\n",
    "#     best_rounds = np.zeros ((fold, N_params))\n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(learning_rate = 0.005)\n",
    "        est.set_params(subsample_freq = 1)\n",
    "        est.set_params(objective = 'multiclass')\n",
    "        est.set_params(n_estimators = 1000000)\n",
    "\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est)) \n",
    "\n",
    "        \n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "            est.fit(train_x_fold, train_y_fold,\n",
    "                   eval_set = [(val_x_fold,val_y_fold)],\n",
    "                   eval_metric = 'multi_logloss',\n",
    "                   early_stopping_rounds = early_stopping_rounds,\n",
    "                   verbose = False)\n",
    "            \n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,num_iteration = best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,num_iteration=best_round)\n",
    "            \n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "#         test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "#                 np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "#                           test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "#                           test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_loc_price_diff</th>\n",
       "      <th>num_price</th>\n",
       "      <th>num_loc_median_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>-49.5</td>\n",
       "      <td>2600</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>-500.0</td>\n",
       "      <td>2750</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>5700.0</td>\n",
       "      <td>8000</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>-1555.0</td>\n",
       "      <td>4195</td>\n",
       "      <td>5750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>4200</td>\n",
       "      <td>4400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>-3464.0</td>\n",
       "      <td>2300</td>\n",
       "      <td>5764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>4200.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>3275.0</td>\n",
       "      <td>6200</td>\n",
       "      <td>2925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>-1169.0</td>\n",
       "      <td>4595</td>\n",
       "      <td>5764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7145</th>\n",
       "      <td>-955.0</td>\n",
       "      <td>3995</td>\n",
       "      <td>4950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>-350.0</td>\n",
       "      <td>3650</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>-1237.5</td>\n",
       "      <td>2700</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>720.0</td>\n",
       "      <td>5895</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>-449.0</td>\n",
       "      <td>2250</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>-1250.0</td>\n",
       "      <td>2600</td>\n",
       "      <td>3850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10160</th>\n",
       "      <td>-198.0</td>\n",
       "      <td>4977</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10307</th>\n",
       "      <td>-250.0</td>\n",
       "      <td>3750</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11706</th>\n",
       "      <td>-1400.0</td>\n",
       "      <td>4325</td>\n",
       "      <td>5725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12982</th>\n",
       "      <td>-575.0</td>\n",
       "      <td>4600</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>950.0</td>\n",
       "      <td>3700</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>-49.5</td>\n",
       "      <td>2600</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>-455.0</td>\n",
       "      <td>1995</td>\n",
       "      <td>2450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14525</th>\n",
       "      <td>-250.0</td>\n",
       "      <td>3750</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15371</th>\n",
       "      <td>-837.5</td>\n",
       "      <td>3100</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15451</th>\n",
       "      <td>-2000.0</td>\n",
       "      <td>5200</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15634</th>\n",
       "      <td>-437.5</td>\n",
       "      <td>3500</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2850</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16250</th>\n",
       "      <td>795.0</td>\n",
       "      <td>2995</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45851</th>\n",
       "      <td>-255.0</td>\n",
       "      <td>2495</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46268</th>\n",
       "      <td>-154.5</td>\n",
       "      <td>2495</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47001</th>\n",
       "      <td>110.0</td>\n",
       "      <td>2410</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48562</th>\n",
       "      <td>-449.0</td>\n",
       "      <td>2250</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49191</th>\n",
       "      <td>1767.0</td>\n",
       "      <td>6942</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50883</th>\n",
       "      <td>-105.0</td>\n",
       "      <td>2095</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51757</th>\n",
       "      <td>-254.5</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52355</th>\n",
       "      <td>1152.5</td>\n",
       "      <td>4500</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53231</th>\n",
       "      <td>805.0</td>\n",
       "      <td>4400</td>\n",
       "      <td>3595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54106</th>\n",
       "      <td>-254.5</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55155</th>\n",
       "      <td>105.0</td>\n",
       "      <td>5800</td>\n",
       "      <td>5695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55392</th>\n",
       "      <td>-1037.5</td>\n",
       "      <td>2900</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55403</th>\n",
       "      <td>-901.0</td>\n",
       "      <td>3208</td>\n",
       "      <td>4109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55608</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56876</th>\n",
       "      <td>-6205.0</td>\n",
       "      <td>3795</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56881</th>\n",
       "      <td>270.0</td>\n",
       "      <td>2970</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57071</th>\n",
       "      <td>452.5</td>\n",
       "      <td>3800</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58290</th>\n",
       "      <td>-105.0</td>\n",
       "      <td>2095</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59732</th>\n",
       "      <td>3725.0</td>\n",
       "      <td>7000</td>\n",
       "      <td>3275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61119</th>\n",
       "      <td>5651.5</td>\n",
       "      <td>8999</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61426</th>\n",
       "      <td>300.0</td>\n",
       "      <td>3025</td>\n",
       "      <td>2725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65689</th>\n",
       "      <td>276.0</td>\n",
       "      <td>2950</td>\n",
       "      <td>2674.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66961</th>\n",
       "      <td>305.0</td>\n",
       "      <td>2300</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67319</th>\n",
       "      <td>-198.0</td>\n",
       "      <td>4977</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67678</th>\n",
       "      <td>-254.5</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67683</th>\n",
       "      <td>3720.0</td>\n",
       "      <td>6995</td>\n",
       "      <td>3275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68048</th>\n",
       "      <td>-608.0</td>\n",
       "      <td>2292</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70205</th>\n",
       "      <td>1650.0</td>\n",
       "      <td>7150</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70245</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71754</th>\n",
       "      <td>600.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>4400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_loc_price_diff  num_price  num_loc_median_price\n",
       "710                 -49.5       2600                2649.5\n",
       "779                -500.0       2750                3250.0\n",
       "988                5700.0       8000                2300.0\n",
       "1542               2800.0      10000                7200.0\n",
       "2099              -1555.0       4195                5750.0\n",
       "3447               -200.0       4200                4400.0\n",
       "3697              -3464.0       2300                5764.0\n",
       "4662               4200.0       6500                2300.0\n",
       "4669               3275.0       6200                2925.0\n",
       "4689              -1169.0       4595                5764.0\n",
       "7145               -955.0       3995                4950.0\n",
       "8096               4825.0      10000                5175.0\n",
       "8401               -350.0       3650                4000.0\n",
       "8585              -1237.5       2700                3937.5\n",
       "8746                720.0       5895                5175.0\n",
       "9409               -449.0       2250                2699.0\n",
       "10037             -1250.0       2600                3850.0\n",
       "10160              -198.0       4977                5175.0\n",
       "10307              -250.0       3750                4000.0\n",
       "11706             -1400.0       4325                5725.0\n",
       "12982              -575.0       4600                5175.0\n",
       "13114               950.0       3700                2750.0\n",
       "13144               -49.5       2600                2649.5\n",
       "14186              -455.0       1995                2450.0\n",
       "14525              -250.0       3750                4000.0\n",
       "15371              -837.5       3100                3937.5\n",
       "15451             -2000.0       5200                7200.0\n",
       "15634              -437.5       3500                3937.5\n",
       "16095               100.0       2850                2750.0\n",
       "16250               795.0       2995                2200.0\n",
       "...                   ...        ...                   ...\n",
       "45851              -255.0       2495                2750.0\n",
       "46268              -154.5       2495                2649.5\n",
       "47001               110.0       2410                2300.0\n",
       "48562              -449.0       2250                2699.0\n",
       "49191              1767.0       6942                5175.0\n",
       "50883              -105.0       2095                2200.0\n",
       "51757              -254.5       2395                2649.5\n",
       "52355              1152.5       4500                3347.5\n",
       "53231               805.0       4400                3595.0\n",
       "54106              -254.5       2395                2649.5\n",
       "55155               105.0       5800                5695.0\n",
       "55392             -1037.5       2900                3937.5\n",
       "55403              -901.0       3208                4109.0\n",
       "55608              2800.0      10000                7200.0\n",
       "56876             -6205.0       3795               10000.0\n",
       "56881               270.0       2970                2700.0\n",
       "57071               452.5       3800                3347.5\n",
       "58290              -105.0       2095                2200.0\n",
       "59732              3725.0       7000                3275.0\n",
       "61119              5651.5       8999                3347.5\n",
       "61426               300.0       3025                2725.0\n",
       "65689               276.0       2950                2674.0\n",
       "66961               305.0       2300                1995.0\n",
       "67319              -198.0       4977                5175.0\n",
       "67678              -254.5       2395                2649.5\n",
       "67683              3720.0       6995                3275.0\n",
       "68048              -608.0       2292                2900.0\n",
       "70205              1650.0       7150                5500.0\n",
       "70245              2800.0      10000                7200.0\n",
       "71754               600.0       5000                4400.0\n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 10 folds\n",
      "Model 1: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.317697, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.005, max_bin=572, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=101, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=17,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.947757, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 1 fold 1\n",
      "best round 14207\n",
      "('Score: ', 0.50722064493075603)\n",
      "Model 1 fold 1 fitting finished in 676.972s\n",
      "Model 1 fold 2\n",
      "best round 19337\n",
      "('Score: ', 0.49107678244137304)\n",
      "Model 1 fold 2 fitting finished in 852.580s\n",
      "Model 1 fold 3\n",
      "best round 14296\n",
      "('Score: ', 0.51806582706202187)\n",
      "Model 1 fold 3 fitting finished in 645.320s\n",
      "Model 1 fold 4\n",
      "best round 15855\n",
      "('Score: ', 0.49455983301319034)\n",
      "Model 1 fold 4 fitting finished in 710.216s\n",
      "Model 1 fold 5\n",
      "best round 14007\n",
      "('Score: ', 0.52704397695519112)\n",
      "Model 1 fold 5 fitting finished in 626.210s\n",
      "Model 1 fold 6\n",
      "best round 14281\n",
      "('Score: ', 0.5123891993844264)\n",
      "Model 1 fold 6 fitting finished in 645.883s\n",
      "Model 1 fold 7\n",
      "best round 14468\n",
      "('Score: ', 0.52703013252927455)\n",
      "Model 1 fold 7 fitting finished in 657.460s\n",
      "Model 1 fold 8\n",
      "best round 13953\n",
      "('Score: ', 0.54037834829851505)\n",
      "Model 1 fold 8 fitting finished in 628.894s\n",
      "Model 1 fold 9\n",
      "best round 14062\n",
      "('Score: ', 0.53529092668113376)\n",
      "Model 1 fold 9 fitting finished in 633.337s\n",
      "Model 1 fold 10\n",
      "best round 13256\n",
      "('Score: ', 0.52400217151588147)\n",
      "Model 1 fold 10 fitting finished in 611.655s\n",
      "Score for model 1 is 0.517706\n",
      "Model 2: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.306975, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.005, max_bin=376, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=64, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=22,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.947558, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 2 fold 1\n",
      "best round 11515\n",
      "('Score: ', 0.50666444331550731)\n",
      "Model 2 fold 1 fitting finished in 536.607s\n",
      "Model 2 fold 2\n",
      "best round 15327\n",
      "('Score: ', 0.48955914089577313)\n",
      "Model 2 fold 2 fitting finished in 704.094s\n",
      "Model 2 fold 3\n",
      "best round 12065\n",
      "('Score: ', 0.51882709847166664)\n",
      "Model 2 fold 3 fitting finished in 574.939s\n",
      "Model 2 fold 4\n",
      "best round 14229\n",
      "('Score: ', 0.49317140906830398)\n",
      "Model 2 fold 4 fitting finished in 650.733s\n",
      "Model 2 fold 5\n",
      "best round 11654\n",
      "('Score: ', 0.52698172741320481)\n",
      "Model 2 fold 5 fitting finished in 552.408s\n",
      "Model 2 fold 6\n",
      "best round 10528\n",
      "('Score: ', 0.5122892675387406)\n",
      "Model 2 fold 6 fitting finished in 500.476s\n",
      "Model 2 fold 7\n",
      "best round 11410\n",
      "('Score: ', 0.52771584782500225)\n",
      "Model 2 fold 7 fitting finished in 541.561s\n",
      "Model 2 fold 8\n",
      "best round 12172\n",
      "('Score: ', 0.53978316359591505)\n",
      "Model 2 fold 8 fitting finished in 572.782s\n",
      "Model 2 fold 9\n",
      "best round 12424\n",
      "('Score: ', 0.53529041288829382)\n",
      "Model 2 fold 9 fitting finished in 584.883s\n",
      "Model 2 fold 10\n",
      "best round 10920\n",
      "('Score: ', 0.52268554396885092)\n",
      "Model 2 fold 10 fitting finished in 525.532s\n",
      "Score for model 2 is 0.517297\n",
      "Model 3: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.425728, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.005, max_bin=421, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=66, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=31,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.899097, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 3 fold 1\n",
      "best round 7609\n",
      "('Score: ', 0.50633320197386222)\n",
      "Model 3 fold 1 fitting finished in 535.630s\n",
      "Model 3 fold 2\n",
      "best round 10684\n",
      "('Score: ', 0.48919844185771183)\n",
      "Model 3 fold 2 fitting finished in 698.351s\n",
      "Model 3 fold 3\n",
      "best round 8242\n",
      "('Score: ', 0.51749384923581332)\n",
      "Model 3 fold 3 fitting finished in 579.047s\n",
      "Model 3 fold 4\n",
      "best round 9513\n",
      "('Score: ', 0.49458151251069654)\n",
      "Model 3 fold 4 fitting finished in 632.339s\n",
      "Model 3 fold 5\n",
      "best round 7345\n",
      "('Score: ', 0.52799091732326386)\n",
      "Model 3 fold 5 fitting finished in 538.353s\n",
      "Model 3 fold 6\n",
      "best round 7743\n",
      "('Score: ', 0.51173817222065687)\n",
      "Model 3 fold 6 fitting finished in 605.909s\n",
      "Model 3 fold 7\n",
      "best round 8273\n",
      "('Score: ', 0.52778967171661229)\n",
      "Model 3 fold 7 fitting finished in 666.575s\n",
      "Model 3 fold 8\n",
      "best round 9825\n",
      "('Score: ', 0.53918986492556342)\n",
      "Model 3 fold 8 fitting finished in 667.631s\n",
      "Model 3 fold 9\n",
      "best round 7509\n",
      "('Score: ', 0.53585716478593481)\n",
      "Model 3 fold 9 fitting finished in 527.514s\n",
      "Model 3 fold 10\n",
      "best round 7100\n",
      "('Score: ', 0.523586748907652)\n",
      "Model 3 fold 10 fitting finished in 518.274s\n",
      "Score for model 3 is 0.517376\n",
      "Model 4: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.49779, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.005, max_bin=559, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=64, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=17,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.934519, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 4 fold 1\n",
      "best round 11971\n",
      "('Score: ', 0.5070654976070893)\n",
      "Model 4 fold 1 fitting finished in 727.009s\n",
      "Model 4 fold 2\n",
      "best round 16541\n",
      "('Score: ', 0.49129229424929438)\n",
      "Model 4 fold 2 fitting finished in 965.899s\n",
      "Model 4 fold 3\n",
      "best round 13876\n",
      "('Score: ', 0.51851166390285919)\n",
      "Model 4 fold 3 fitting finished in 836.532s\n",
      "Model 4 fold 4\n",
      "best round 15530\n",
      "('Score: ', 0.49461532491469168)\n",
      "Model 4 fold 4 fitting finished in 906.916s\n",
      "Model 4 fold 5\n",
      "best round 12434\n",
      "('Score: ', 0.52824546876330691)\n",
      "Model 4 fold 5 fitting finished in 762.874s\n",
      "Model 4 fold 6\n",
      "best round 11814\n",
      "('Score: ', 0.51310951377186109)\n",
      "Model 4 fold 6 fitting finished in 725.117s\n",
      "Model 4 fold 7\n",
      "best round 12152\n",
      "('Score: ', 0.52768830368542363)\n",
      "Model 4 fold 7 fitting finished in 744.376s\n",
      "Model 4 fold 8\n",
      "best round 15828\n",
      "('Score: ', 0.54011093246159525)\n",
      "Model 4 fold 8 fitting finished in 929.598s\n",
      "Model 4 fold 9\n",
      "best round 16190\n",
      "('Score: ', 0.53535761970118301)\n",
      "Model 4 fold 9 fitting finished in 961.056s\n",
      "Model 4 fold 10\n",
      "best round 13522\n",
      "('Score: ', 0.52385444017752858)\n",
      "Model 4 fold 10 fitting finished in 812.136s\n",
      "Score for model 4 is 0.517985\n",
      "Model 5: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.623331, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.005, max_bin=357, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=66, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=16,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.934423, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 5 fold 1\n",
      "best round 12194\n",
      "('Score: ', 0.50788471837670113)\n",
      "Model 5 fold 1 fitting finished in 804.323s\n",
      "Model 5 fold 2\n",
      "best round 19249\n",
      "('Score: ', 0.49105948451132886)\n",
      "Model 5 fold 2 fitting finished in 1201.511s\n",
      "Model 5 fold 3\n",
      "best round 16136\n",
      "('Score: ', 0.51970350447329239)\n",
      "Model 5 fold 3 fitting finished in 1045.091s\n",
      "Model 5 fold 4\n",
      "best round 15984\n",
      "('Score: ', 0.49562189686737473)\n",
      "Model 5 fold 4 fitting finished in 1032.389s\n",
      "Model 5 fold 5\n",
      "best round 12485\n",
      "('Score: ', 0.52845061172339147)\n",
      "Model 5 fold 5 fitting finished in 867.716s\n",
      "Model 5 fold 6\n",
      "best round 13465\n",
      "('Score: ', 0.51360207537179192)\n",
      "Model 5 fold 6 fitting finished in 890.802s\n",
      "Model 5 fold 7\n",
      "best round 13952\n",
      "('Score: ', 0.5278983367184199)\n",
      "Model 5 fold 7 fitting finished in 914.084s\n",
      "Model 5 fold 8\n",
      "best round 13740\n",
      "('Score: ', 0.54079766302892163)\n",
      "Model 5 fold 8 fitting finished in 908.672s\n",
      "Model 5 fold 9\n",
      "best round 14844\n",
      "('Score: ', 0.53649499520121102)\n",
      "Model 5 fold 9 fitting finished in 962.241s\n",
      "Model 5 fold 10\n",
      "best round 12264\n",
      "('Score: ', 0.52460671881482124)\n",
      "Model 5 fold 10 fitting finished in 823.418s\n",
      "Score for model 5 is 0.518612\n",
      "Score for blended models is 0.517795\n"
     ]
    }
   ],
   "source": [
    "est =       [lgb.LGBMClassifier(num_leaves = 17,\n",
    "                                min_child_samples = 101,\n",
    "                                colsample_bytree = 0.317697,\n",
    "                                subsample = 0.947757,\n",
    "                                max_bin = 572),\n",
    "             lgb.LGBMClassifier(num_leaves = 22,\n",
    "                                min_child_samples = 64,\n",
    "                                colsample_bytree = 0.306975,\n",
    "                                subsample = 0.947558,\n",
    "                                max_bin = 376),\n",
    "             lgb.LGBMClassifier(num_leaves = 31,\n",
    "                                min_child_samples = 66,\n",
    "                                colsample_bytree = 0.425728,\n",
    "                                subsample = 0.899097,\n",
    "                                max_bin = 421),\n",
    "             lgb.LGBMClassifier(num_leaves = 17,\n",
    "                                min_child_samples = 64,\n",
    "                                colsample_bytree = 0.497790,\n",
    "                                subsample = 0.934519,\n",
    "                                max_bin = 559),\n",
    "             lgb.LGBMClassifier(num_leaves = 16,\n",
    "                                min_child_samples = 66,\n",
    "                                colsample_bytree = 0.623331,\n",
    "                                subsample = 0.934423,\n",
    "                                max_bin = 357)]\n",
    "\n",
    "#  \t \tnum_leaves \tmin_child_samples \tmax_bin \tcolsample_bytree \tsubsample \tscore\n",
    "# 39 \t17.827540 \t101.915438 \t \t \t572.750117 \t0.317697 \t \t \t0.947757 \t-0.524479\n",
    "# 24 \t22.382049 \t64.962952 \t \t \t376.640680 \t0.306975 \t \t \t0.947558 \t-0.524254\n",
    "# 31 \t31.615417 \t66.021705 \t \t \t421.213915 \t0.425728 \t \t \t0.899097 \t-0.524894\n",
    "# 15 \t17.194424 \t64.222584 \t \t \t559.939981 \t0.497790 \t \t \t0.934519 \t-0.525071\n",
    "# 39 \t16.389240 \t66.216210 \t \t \t357.642431 \t0.623331 \t \t \t0.934423 \t-0.525089\n",
    "# 36 \t45.039551 \t103.986754 \t338.247631 \t0.369964 \t0.918298 \t-0.525114\n",
    "# 4 \t10.471950 \t106.924120 \t397.842570 \t0.453457 \t0.924038 \t-0.525457\n",
    "# 9 \t14.508117 \t182.304842 \t712.794090 \t0.420441 \t0.930807 \t-0.525571\n",
    "# 3 \t19.481535 \t62.365550 \t207.070979 \t0.471294 \t0.781278 \t-0.525590\n",
    "# 9 \t31.402499 \t301.654857 \t574.185729 \t0.672997 \t0.965712 \t-0.525661\n",
    "# 0 \t25.533528 \t117.166334 \t232.383120 \t0.450393 \t0.896000 \t-0.525687\n",
    "# 21 \t13.207873 \t68.105397 \t536.009959 \t0.405839 \t0.954524 \t-0.525693\n",
    "# 11 \t14.765542 \t93.186696 \t597.508214 \t0.610208 \t0.809859 \t-0.525743\n",
    "\n",
    "\n",
    "(train_blend_x_gbm,\n",
    " test_blend_x_gbm_mean,\n",
    " test_blend_x_gbm_gmean,\n",
    " blend_scores_gbm,\n",
    " best_rounds_gbm)= lgbm_blend(est, \n",
    "                               train_X, train_y, \n",
    "                               test_X,\n",
    "                               10,\n",
    "                               1000) #as the learning rate decreases the number of stopping rounds need to be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data 0322\n",
    "\n",
    "\n",
    "# Blend 5 estimators for 10 folds\n",
    "# Model 1: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.398779, drop_rate=0.1,\n",
    "#         is_unbalance=False, learning_rate=0.005, max_bin=357, max_depth=-1,\n",
    "#         max_drop=50, min_child_samples=168, min_child_weight=5,\n",
    "#         min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=16,\n",
    "#         objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
    "#         scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
    "#         skip_drop=0.5, subsample=0.94605, subsample_for_bin=50000,\n",
    "#         subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
    "# Model 1 fold 1\n",
    "# best round 13658\n",
    "# ('Score: ', 0.51511027734945769)\n",
    "# Model 1 fold 1 fitting finished in 666.358s\n",
    "# Model 1 fold 2\n",
    "# best round 18966\n",
    "# ('Score: ', 0.49748491939132805)\n",
    "# Model 1 fold 2 fitting finished in 933.243s\n",
    "# Model 1 fold 3\n",
    "# best round 16912\n",
    "# ('Score: ', 0.52418540090167209)\n",
    "# Model 1 fold 3 fitting finished in 744.659s\n",
    "# Model 1 fold 4\n",
    "# best round 17529\n",
    "# ('Score: ', 0.49909289305104237)\n",
    "# Model 1 fold 4 fitting finished in 671.642s\n",
    "# Model 1 fold 5\n",
    "# best round 12866\n",
    "# ('Score: ', 0.53343311318041697)\n",
    "# Model 1 fold 5 fitting finished in 568.086s\n",
    "# Model 1 fold 6\n",
    "# best round 13399\n",
    "# ('Score: ', 0.52049272741295138)\n",
    "# Model 1 fold 6 fitting finished in 505.250s\n",
    "# Model 1 fold 7\n",
    "# best round 13580\n",
    "# ('Score: ', 0.52872157155539778)\n",
    "# Model 1 fold 7 fitting finished in 472.583s\n",
    "# Model 1 fold 8\n",
    "# best round 14739\n",
    "# ('Score: ', 0.54319696370850756)\n",
    "# Model 1 fold 8 fitting finished in 526.231s\n",
    "# Model 1 fold 9\n",
    "# best round 15989\n",
    "# ('Score: ', 0.53855626201600892)\n",
    "# Model 1 fold 9 fitting finished in 591.713s\n",
    "# Model 1 fold 10\n",
    "# best round 15025\n",
    "# ('Score: ', 0.53069150503223939)\n",
    "# Model 1 fold 10 fitting finished in 553.884s\n",
    "# Score for model 1 is 0.523097\n",
    "# Model 2: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.7164, drop_rate=0.1,\n",
    "#         is_unbalance=False, learning_rate=0.005, max_bin=226, max_depth=-1,\n",
    "#         max_drop=50, min_child_samples=87, min_child_weight=5,\n",
    "#         min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=18,\n",
    "#         objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
    "#         scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
    "#         skip_drop=0.5, subsample=0.898679, subsample_for_bin=50000,\n",
    "#         subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
    "# Model 2 fold 1\n",
    "# best round 10248\n",
    "# ('Score: ', 0.51279527489887844)\n",
    "# Model 2 fold 1 fitting finished in 463.937s\n",
    "# Model 2 fold 2\n",
    "# best round 15443\n",
    "# ('Score: ', 0.4976452352207959)\n",
    "# Model 2 fold 2 fitting finished in 638.788s\n",
    "# Model 2 fold 3\n",
    "# best round 12373\n",
    "# ('Score: ', 0.52193792417675422)\n",
    "# Model 2 fold 3 fitting finished in 523.411s\n",
    "# Model 2 fold 4\n",
    "# best round 12944\n",
    "# ('Score: ', 0.4994116556253469)\n",
    "# Model 2 fold 4 fitting finished in 548.260s\n",
    "# Model 2 fold 5\n",
    "# best round 10491\n",
    "# ('Score: ', 0.53427000487297915)\n",
    "# Model 2 fold 5 fitting finished in 480.181s\n",
    "# Model 2 fold 6\n",
    "# best round 11402\n",
    "# ('Score: ', 0.52198833831101743)\n",
    "# Model 2 fold 6 fitting finished in 446.453s\n",
    "# Model 2 fold 7\n",
    "# best round 11700\n",
    "# ('Score: ', 0.52975063233231268)\n",
    "# Model 2 fold 7 fitting finished in 508.264s\n",
    "# Model 2 fold 8\n",
    "# best round 13942\n",
    "# ('Score: ', 0.54124823287843038)\n",
    "# Model 2 fold 8 fitting finished in 579.405s\n",
    "# Model 2 fold 9\n",
    "# best round 13851\n",
    "# ('Score: ', 0.53849899306996529)\n",
    "# Model 2 fold 9 fitting finished in 575.462s\n",
    "# Model 2 fold 10\n",
    "# best round 12086\n",
    "# ('Score: ', 0.52869565211129366)\n",
    "# Model 2 fold 10 fitting finished in 512.622s\n",
    "# Score for model 2 is 0.522624\n",
    "# Model 3: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.39677, drop_rate=0.1,\n",
    "#         is_unbalance=False, learning_rate=0.005, max_bin=351, max_depth=-1,\n",
    "#         max_drop=50, min_child_samples=171, min_child_weight=5,\n",
    "#         min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=15,\n",
    "#         objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
    "#         scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
    "#         skip_drop=0.5, subsample=0.970258, subsample_for_bin=50000,\n",
    "#         subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
    "# Model 3 fold 1\n",
    "# best round 12874\n",
    "# ('Score: ', 0.51579970828744059)\n",
    "# Model 3 fold 1 fitting finished in 460.611s\n",
    "# Model 3 fold 2\n",
    "# best round 18224\n",
    "# ('Score: ', 0.49875315482793847)\n",
    "# Model 3 fold 2 fitting finished in 617.800s\n",
    "# Model 3 fold 3\n",
    "# best round 16410\n",
    "# ('Score: ', 0.52445295623291099)\n",
    "# Model 3 fold 3 fitting finished in 556.717s\n",
    "# Model 3 fold 4\n",
    "# best round 15824\n",
    "# ('Score: ', 0.50021228553164887)\n",
    "# Model 3 fold 4 fitting finished in 564.615s\n",
    "# Model 3 fold 5\n",
    "# best round 15533\n",
    "# ('Score: ', 0.53396460910535426)\n",
    "# Model 3 fold 5 fitting finished in 532.299s\n",
    "# Model 3 fold 6\n",
    "# best round 12458\n",
    "# ('Score: ', 0.52040391903309924)\n",
    "# Model 3 fold 6 fitting finished in 421.335s\n",
    "# Model 3 fold 7\n",
    "# best round 15590\n",
    "# ('Score: ', 0.52977879904304848)\n",
    "# Model 3 fold 7 fitting finished in 538.673s\n",
    "# Model 3 fold 8\n",
    "# best round 15934\n",
    "# ('Score: ', 0.54355795947963481)\n",
    "# Model 3 fold 8 fitting finished in 552.762s\n",
    "# Model 3 fold 9\n",
    "# best round 16181\n",
    "# ('Score: ', 0.53803790113040595)\n",
    "# Model 3 fold 9 fitting finished in 553.873s\n",
    "# Model 3 fold 10\n",
    "# best round 15159\n",
    "# ('Score: ', 0.53069840648618571)\n",
    "# Model 3 fold 10 fitting finished in 531.805s\n",
    "# Score for model 3 is 0.523566\n",
    "# Model 4: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.402, drop_rate=0.1,\n",
    "#         is_unbalance=False, learning_rate=0.005, max_bin=338, max_depth=-1,\n",
    "#         max_drop=50, min_child_samples=120, min_child_weight=5,\n",
    "#         min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=36,\n",
    "#         objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
    "#         scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
    "#         skip_drop=0.5, subsample=0.9845, subsample_for_bin=50000,\n",
    "#         subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
    "# Model 4 fold 1\n",
    "# best round 6780\n",
    "# ('Score: ', 0.51297224026086863)\n",
    "# Model 4 fold 1 fitting finished in 388.870s\n",
    "# Model 4 fold 2\n",
    "# best round 9795\n",
    "# ('Score: ', 0.49493678490657994)\n",
    "# Model 4 fold 2 fitting finished in 456.681s\n",
    "# Model 4 fold 3\n",
    "# best round 8109\n",
    "# ('Score: ', 0.52283766823302491)\n",
    "# Model 4 fold 3 fitting finished in 446.561s\n",
    "# Model 4 fold 4\n",
    "# best round 8156\n",
    "# ('Score: ', 0.49875483248461955)\n",
    "# Model 4 fold 4 fitting finished in 438.891s\n",
    "# Model 4 fold 5\n",
    "# best round 6897\n",
    "# ('Score: ', 0.53322646425989406)\n",
    "# Model 4 fold 5 fitting finished in 356.814s\n",
    "# Model 4 fold 6\n",
    "# best round 6370\n",
    "# ('Score: ', 0.52000404633950559)\n",
    "# Model 4 fold 6 fitting finished in 469.238s\n",
    "# Model 4 fold 7\n",
    "# best round 7291\n",
    "# ('Score: ', 0.52866228253282754)\n",
    "# Model 4 fold 7 fitting finished in 406.877s\n",
    "# Model 4 fold 8\n",
    "# best round 8043\n",
    "# ('Score: ', 0.54111737889176825)\n",
    "# Model 4 fold 8 fitting finished in 426.052s\n",
    "# Model 4 fold 9\n",
    "# best round 7127\n",
    "# ('Score: ', 0.53782950085297831)\n",
    "# Model 4 fold 9 fitting finished in 370.410s\n",
    "# Model 4 fold 10\n",
    "# best round 8311\n",
    "# ('Score: ', 0.52849296178887739)\n",
    "# Model 4 fold 10 fitting finished in 467.649s\n",
    "# Score for model 4 is 0.521883\n",
    "# Model 5: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.562594, drop_rate=0.1,\n",
    "#         is_unbalance=False, learning_rate=0.005, max_bin=232, max_depth=-1,\n",
    "#         max_drop=50, min_child_samples=80, min_child_weight=5,\n",
    "#         min_split_gain=0, n_estimators=1000000, nthread=-1, num_leaves=21,\n",
    "#         objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
    "#         scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
    "#         skip_drop=0.5, subsample=0.886285, subsample_for_bin=50000,\n",
    "#         subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
    "# Model 5 fold 1\n",
    "# best round 11339\n",
    "# ('Score: ', 0.51246952947927349)\n",
    "# Model 5 fold 1 fitting finished in 473.511s\n",
    "# Model 5 fold 2\n",
    "# best round 13498\n",
    "# ('Score: ', 0.49665250024834595)\n",
    "# Model 5 fold 2 fitting finished in 544.382s\n",
    "# Model 5 fold 3\n",
    "# best round 12945\n",
    "# ('Score: ', 0.52162202839399208)\n",
    "# Model 5 fold 3 fitting finished in 522.065s\n",
    "# Model 5 fold 4\n",
    "# best round 12176\n",
    "# ('Score: ', 0.49930094273345088)\n",
    "# Model 5 fold 4 fitting finished in 478.193s\n",
    "# Model 5 fold 5\n",
    "# best round 10737\n",
    "# ('Score: ', 0.53347270474127595)\n",
    "# Model 5 fold 5 fitting finished in 464.415s\n",
    "# Model 5 fold 6\n",
    "# best round 9892\n",
    "# ('Score: ', 0.52071472372899585)\n",
    "# Model 5 fold 6 fitting finished in 436.333s\n",
    "# Model 5 fold 7\n",
    "# best round 10177\n",
    "# ('Score: ', 0.52835928323419556)\n",
    "# Model 5 fold 7 fitting finished in 411.271s\n",
    "# Model 5 fold 8\n",
    "# best round 11398\n",
    "# ('Score: ', 0.54099733268775996)\n",
    "# Model 5 fold 8 fitting finished in 449.035s\n",
    "# Model 5 fold 9\n",
    "# best round 11651\n",
    "# ('Score: ', 0.53796773146228249)\n",
    "# Model 5 fold 9 fitting finished in 503.545s\n",
    "# Model 5 fold 10\n",
    "# best round 12079\n",
    "# ('Score: ', 0.5282561706228871)\n",
    "# Model 5 fold 10 fitting finished in 498.074s\n",
    "# Score for model 5 is 0.521981\n",
    "# Score for blended models is 0.522630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51770578  0.51729681  0.51737595  0.51798511  0.518612  ]\n",
      "[ 14772.2  12224.4   8384.3  13985.8  14431.3]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_LightGBM_BM_0401_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_LightGBM_mean_BM_0401_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../blend/test_blend_LightGBM_gmean_BM_0401_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_gbm,axis=0))\n",
    "print (np.mean(best_rounds_gbm,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_gbm, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_gbm_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_gbm_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LightGBM_BM_0401_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_gbm_mean[:,6:9])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data 0322\n",
    "\n",
    "# [ 0.52309656  0.52262419  0.52356597  0.52188342  0.52198129]\n",
    "# [ 15266.3  12448.   15418.7   7687.9  11589.2]\n",
    "\n",
    "# data 0331\n",
    "# [ 0.51778446  0.51758745  0.51859108  0.51763268  0.51941101]\n",
    "# [ 15053.7   8042.4  16049.9  13754.3  28486.6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = (test_blend_x_gbm_mean[:,6:9] +test_blend_x_gbm_gmean[:,6:9])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LightGBM_BM_0331_total_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(temp)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_loc_price_diff</th>\n",
       "      <th>num_price</th>\n",
       "      <th>num_loc_median_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2600</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2750</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8000</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4195</td>\n",
       "      <td>5750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4200</td>\n",
       "      <td>4400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2300</td>\n",
       "      <td>5764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6500</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6200</td>\n",
       "      <td>2925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4595</td>\n",
       "      <td>5764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3995</td>\n",
       "      <td>4950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3650</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2700</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5895</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2250</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10037</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2600</td>\n",
       "      <td>3850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10160</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4977</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10307</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3750</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11706</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4325</td>\n",
       "      <td>5725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12982</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4600</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13114</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3700</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2600</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>2450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3750</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15371</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3100</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15451</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5200</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15634</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3500</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2850</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16250</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2995</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45851</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2495</td>\n",
       "      <td>2750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46268</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2495</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2410</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48562</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2250</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49191</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6942</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50883</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2095</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51757</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52355</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4500</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53231</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4400</td>\n",
       "      <td>3595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55155</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5800</td>\n",
       "      <td>5695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55392</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2900</td>\n",
       "      <td>3937.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55403</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3208</td>\n",
       "      <td>4109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56876</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3795</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2970</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57071</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3800</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2095</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59732</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7000</td>\n",
       "      <td>3275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61119</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8999</td>\n",
       "      <td>3347.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61426</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3025</td>\n",
       "      <td>2725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65689</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2950</td>\n",
       "      <td>2674.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66961</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2300</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67319</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4977</td>\n",
       "      <td>5175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67678</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2395</td>\n",
       "      <td>2649.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67683</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6995</td>\n",
       "      <td>3275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68048</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2292</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70205</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7150</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70245</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10000</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5000</td>\n",
       "      <td>4400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_loc_price_diff  num_price  num_loc_median_price\n",
       "710                   NaN       2600                2649.5\n",
       "779                   NaN       2750                3250.0\n",
       "988                   NaN       8000                2300.0\n",
       "1542                  NaN      10000                7200.0\n",
       "2099                  NaN       4195                5750.0\n",
       "3447                  NaN       4200                4400.0\n",
       "3697                  NaN       2300                5764.0\n",
       "4662                  NaN       6500                2300.0\n",
       "4669                  NaN       6200                2925.0\n",
       "4689                  NaN       4595                5764.0\n",
       "7145                  NaN       3995                4950.0\n",
       "8096                  NaN      10000                5175.0\n",
       "8401                  NaN       3650                4000.0\n",
       "8585                  NaN       2700                3937.5\n",
       "8746                  NaN       5895                5175.0\n",
       "9409                  NaN       2250                2699.0\n",
       "10037                 NaN       2600                3850.0\n",
       "10160                 NaN       4977                5175.0\n",
       "10307                 NaN       3750                4000.0\n",
       "11706                 NaN       4325                5725.0\n",
       "12982                 NaN       4600                5175.0\n",
       "13114                 NaN       3700                2750.0\n",
       "13144                 NaN       2600                2649.5\n",
       "14186                 NaN       1995                2450.0\n",
       "14525                 NaN       3750                4000.0\n",
       "15371                 NaN       3100                3937.5\n",
       "15451                 NaN       5200                7200.0\n",
       "15634                 NaN       3500                3937.5\n",
       "16095                 NaN       2850                2750.0\n",
       "16250                 NaN       2995                2200.0\n",
       "...                   ...        ...                   ...\n",
       "45851                 NaN       2495                2750.0\n",
       "46268                 NaN       2495                2649.5\n",
       "47001                 NaN       2410                2300.0\n",
       "48562                 NaN       2250                2699.0\n",
       "49191                 NaN       6942                5175.0\n",
       "50883                 NaN       2095                2200.0\n",
       "51757                 NaN       2395                2649.5\n",
       "52355                 NaN       4500                3347.5\n",
       "53231                 NaN       4400                3595.0\n",
       "54106                 NaN       2395                2649.5\n",
       "55155                 NaN       5800                5695.0\n",
       "55392                 NaN       2900                3937.5\n",
       "55403                 NaN       3208                4109.0\n",
       "55608                 NaN      10000                7200.0\n",
       "56876                 NaN       3795               10000.0\n",
       "56881                 NaN       2970                2700.0\n",
       "57071                 NaN       3800                3347.5\n",
       "58290                 NaN       2095                2200.0\n",
       "59732                 NaN       7000                3275.0\n",
       "61119                 NaN       8999                3347.5\n",
       "61426                 NaN       3025                2725.0\n",
       "65689                 NaN       2950                2674.0\n",
       "66961                 NaN       2300                1995.0\n",
       "67319                 NaN       4977                5175.0\n",
       "67678                 NaN       2395                2649.5\n",
       "67683                 NaN       6995                3275.0\n",
       "68048                 NaN       2292                2900.0\n",
       "70205                 NaN       7150                5500.0\n",
       "70245                 NaN      10000                7200.0\n",
       "71754                 NaN       5000                4400.0\n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
