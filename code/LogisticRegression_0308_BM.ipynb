{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 285) (74659, 285) (49352L,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_csv(data_path + 'train_BrandenMurray.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_BrandenMurray.csv')\n",
    "train_y = np.ravel(pd.read_csv(data_path + 'labels_BrandenMurray.csv'))\n",
    "ntrain = train_X.shape[0]\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_X,test_X])\n",
    "features_to_use = train_X.columns.values\n",
    "\n",
    "skewed_cols = full_data[features_to_use].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] - full_data[skewed_col].min() + 1)\n",
    "#     print skewed_col, '\\t', lam\n",
    "for col in features_to_use:\n",
    "    full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1))\n",
    "    train_X[col] = full_data.iloc[:ntrain][col]\n",
    "    test_X[col] = full_data.iloc[ntrain:][col]\n",
    "\n",
    "    \n",
    "del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 285)\n",
      "(9871, 285)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.607339530294\tC: 0.010\tTime: 20.939s\n",
      "score: 0.606411472469\tC: 0.030\tTime: 28.818s\n",
      "score: 0.606160340445\tC: 0.100\tTime: 49.535s\n",
      "C:0.100\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'lbfgs',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_lbfgs = C\n",
    "        best_score = score\n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, (time.time()-start))\n",
    "print 'C:{0:.3f}'.format(C_ovr_lbfgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.606421331593\tC: 0.030\tTime: 45.458s\n",
      "score: 0.606149658324\tC: 0.100\tTime: 94.441s\n",
      "C:0.100\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'sag',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [3e-2,1e-1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_sag = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, (time.time()-start))\n",
    "print 'C:{0:.3f}'.format(C_ovr_sag)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.607339756681\tC: 0.010\tTime: 19.949s\n",
      "score: 0.606409001558\tC: 0.030\tTime: 23.836s\n",
      "score: 0.606157874057\tC: 0.100\tTime: 33.457s\n",
      "C:0.100\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'newton-cg',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_newton = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, time.time()-start)\n",
    "print 'C:{0:.3f}'.format(C_ovr_newton)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.609210424517\tC: 0.010\tTime: 11.418s\n",
      "score: 0.606736376324\tC: 0.030\tTime: 14.851s\n",
      "score: 0.606199866825\tC: 0.100\tTime: 23.248s\n",
      "score: 0.606955690143\tC: 0.300\tTime: 35.063s\n",
      "score: 0.608300467155\tC: 1.000\tTime: 51.217s\n",
      "C:0.100\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'liblinear',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_liblinear = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, time.time()-start)\n",
    "print 'C:{0:.3f}'.format(C_ovr_liblinear)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.603533947713\tC: 0.010\tTime: 41.598s\n",
      "score: 0.603452939182\tC: 0.030\tTime: 62.785s\n",
      "score: 0.604198477269\tC: 0.100\tTime: 92.314s\n",
      "score: 0.605567583798\tC: 0.300\tTime: 151.256s\n",
      "score: 0.607752640094\tC: 1.000\tTime: 227.016s\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'lbfgs',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_lbfgs = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, time.time()-start)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_lbfgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.6035345059\tC: 0.010\tTime: 19.847s\n",
      "score: 0.603457205755\tC: 0.030\tTime: 44.085s\n",
      "score: 0.604154703692\tC: 0.100\tTime: 102.722s\n",
      "score: 0.605499331947\tC: 0.300\tTime: 251.266s\n",
      "score: 0.607534921899\tC: 1.000\tTime: 698.357s\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'sag',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_sag = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, time.time()-start)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_sag)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.603527422588\tC: 0.010\tTime: 34.088s\n",
      "score: 0.603450329015\tC: 0.030\tTime: 44.021s\n",
      "score: 0.604181498409\tC: 0.100\tTime: 76.718s\n",
      "score: 0.60556107096\tC: 0.300\tTime: 116.000s\n",
      "score: 0.607652222914\tC: 1.000\tTime: 155.443s\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'newton-cg',\n",
    "                            n_jobs = -1, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_newton = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.3f}s'.format(score, C, time.time()-start)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_newton)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LR_blend(est, train_x, train_y, test_x, fold):\n",
    "    N_params = len(est)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "    \n",
    "    for j, ester in enumerate(est):\n",
    "        print (\"Model %d:\" %(j+1))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "\n",
    "            \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]            \n",
    "            \n",
    "\n",
    "            ester.fit(train_x_fold,train_y_fold)\n",
    "            \n",
    "            val_y_predict_fold = ester.predict_proba(val_x_fold)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = ester.predict_proba(test_x)\n",
    "            \n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "\n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 7 estimators for 10 folds\n",
      "Model 1:\n",
      "Model 1 fold 1\n",
      "('Score: ', 0.58716933314206787)\n",
      "Model 1 fold 1 fitting finished in 48.204s\n",
      "Model 1 fold 2\n",
      "('Score: ', 0.57548652721090132)\n",
      "Model 1 fold 2 fitting finished in 60.988s\n",
      "Model 1 fold 3\n",
      "('Score: ', 0.6009083935438827)\n",
      "Model 1 fold 3 fitting finished in 59.097s\n",
      "Model 1 fold 4\n",
      "('Score: ', 0.58383693096066502)\n",
      "Model 1 fold 4 fitting finished in 55.266s\n",
      "Model 1 fold 5\n",
      "('Score: ', 0.61293538607663278)\n",
      "Model 1 fold 5 fitting finished in 60.075s\n",
      "Model 1 fold 6\n",
      "('Score: ', 0.59124403454253738)\n",
      "Model 1 fold 6 fitting finished in 57.650s\n",
      "Model 1 fold 7\n",
      "('Score: ', 0.59654576854108199)\n",
      "Model 1 fold 7 fitting finished in 54.433s\n",
      "Model 1 fold 8\n",
      "('Score: ', 0.62344515463159278)\n",
      "Model 1 fold 8 fitting finished in 57.143s\n",
      "Model 1 fold 9\n",
      "('Score: ', 0.61162546325132261)\n",
      "Model 1 fold 9 fitting finished in 58.078s\n",
      "Model 1 fold 10\n",
      "('Score: ', 0.6149339841947159)\n",
      "Model 1 fold 10 fitting finished in 55.708s\n",
      "Score for model 1 is 0.599813\n",
      "Model 2:\n",
      "Model 2 fold 1\n",
      "('Score: ', 0.58717099209452939)\n",
      "Model 2 fold 1 fitting finished in 202.456s\n",
      "Model 2 fold 2\n",
      "('Score: ', 0.57548484128370647)\n",
      "Model 2 fold 2 fitting finished in 205.153s\n",
      "Model 2 fold 3\n",
      "('Score: ', 0.60091224791316389)\n",
      "Model 2 fold 3 fitting finished in 154.558s\n",
      "Model 2 fold 4\n",
      "('Score: ', 0.58381223848962427)\n",
      "Model 2 fold 4 fitting finished in 190.744s\n",
      "Model 2 fold 5\n",
      "('Score: ', 0.61293572966783927)\n",
      "Model 2 fold 5 fitting finished in 195.349s\n",
      "Model 2 fold 6\n",
      "('Score: ', 0.59124618320506495)\n",
      "Model 2 fold 6 fitting finished in 195.054s\n",
      "Model 2 fold 7\n",
      "('Score: ', 0.59655308927713468)\n",
      "Model 2 fold 7 fitting finished in 199.279s\n",
      "Model 2 fold 8\n",
      "('Score: ', 0.62345647847693286)\n",
      "Model 2 fold 8 fitting finished in 191.640s\n",
      "Model 2 fold 9\n",
      "('Score: ', 0.61162075576260599)\n",
      "Model 2 fold 9 fitting finished in 195.389s\n",
      "Model 2 fold 10\n",
      "('Score: ', 0.61493390897182654)\n",
      "Model 2 fold 10 fitting finished in 184.049s\n",
      "Score for model 2 is 0.599813\n",
      "Model 3:\n",
      "Model 3 fold 1\n",
      "('Score: ', 0.58717206542641576)\n",
      "Model 3 fold 1 fitting finished in 40.750s\n",
      "Model 3 fold 2\n",
      "('Score: ', 0.57548319827453742)\n",
      "Model 3 fold 2 fitting finished in 45.946s\n",
      "Model 3 fold 3\n",
      "('Score: ', 0.60091445415498512)\n",
      "Model 3 fold 3 fitting finished in 42.490s\n",
      "Model 3 fold 4\n",
      "('Score: ', 0.58381244272249166)\n",
      "Model 3 fold 4 fitting finished in 44.133s\n",
      "Model 3 fold 5\n",
      "('Score: ', 0.6129385564016383)\n",
      "Model 3 fold 5 fitting finished in 44.813s\n",
      "Model 3 fold 6\n",
      "('Score: ', 0.59124769680827616)\n",
      "Model 3 fold 6 fitting finished in 41.522s\n",
      "Model 3 fold 7\n",
      "('Score: ', 0.59655311427620628)\n",
      "Model 3 fold 7 fitting finished in 43.677s\n",
      "Model 3 fold 8\n",
      "('Score: ', 0.62345588657436724)\n",
      "Model 3 fold 8 fitting finished in 41.525s\n",
      "Model 3 fold 9\n",
      "('Score: ', 0.61162224426057865)\n",
      "Model 3 fold 9 fitting finished in 43.143s\n",
      "Model 3 fold 10\n",
      "('Score: ', 0.61493494168384044)\n",
      "Model 3 fold 10 fitting finished in 42.962s\n",
      "Score for model 3 is 0.599813\n",
      "Model 4:\n",
      "Model 4 fold 1\n",
      "('Score: ', 0.58729942649292877)\n",
      "Model 4 fold 1 fitting finished in 32.458s\n",
      "Model 4 fold 2\n",
      "('Score: ', 0.57565278079641613)\n",
      "Model 4 fold 2 fitting finished in 32.614s\n",
      "Model 4 fold 3\n",
      "('Score: ', 0.60102161670515686)\n",
      "Model 4 fold 3 fitting finished in 32.460s\n",
      "Model 4 fold 4\n",
      "('Score: ', 0.58394906559017867)\n",
      "Model 4 fold 4 fitting finished in 33.348s\n",
      "Model 4 fold 5\n",
      "('Score: ', 0.61284609059839457)\n",
      "Model 4 fold 5 fitting finished in 36.284s\n",
      "Model 4 fold 6\n",
      "('Score: ', 0.59141495825229495)\n",
      "Model 4 fold 6 fitting finished in 36.661s\n",
      "Model 4 fold 7\n",
      "('Score: ', 0.59677590986961326)\n",
      "Model 4 fold 7 fitting finished in 36.075s\n",
      "Model 4 fold 8\n",
      "('Score: ', 0.6235740540036433)\n",
      "Model 4 fold 8 fitting finished in 32.940s\n",
      "Model 4 fold 9\n",
      "('Score: ', 0.61167487375532914)\n",
      "Model 4 fold 9 fitting finished in 35.269s\n",
      "Model 4 fold 10\n",
      "('Score: ', 0.61511961106891322)\n",
      "Model 4 fold 10 fitting finished in 32.565s\n",
      "Score for model 4 is 0.599933\n",
      "Model 5:\n",
      "Model 5 fold 1\n",
      "('Score: ', 0.58336217819051328)\n",
      "Model 5 fold 1 fitting finished in 73.170s\n",
      "Model 5 fold 2\n",
      "('Score: ', 0.57036883022270701)\n",
      "Model 5 fold 2 fitting finished in 73.294s\n",
      "Model 5 fold 3\n",
      "('Score: ', 0.59447512253471435)\n",
      "Model 5 fold 3 fitting finished in 78.261s\n",
      "Model 5 fold 4\n",
      "('Score: ', 0.58016460466381048)\n",
      "Model 5 fold 4 fitting finished in 75.647s\n",
      "Model 5 fold 5\n",
      "('Score: ', 0.61022480855595962)\n",
      "Model 5 fold 5 fitting finished in 77.096s\n",
      "Model 5 fold 6\n",
      "('Score: ', 0.58831445907476687)\n",
      "Model 5 fold 6 fitting finished in 77.500s\n",
      "Model 5 fold 7\n",
      "('Score: ', 0.59267240899307427)\n",
      "Model 5 fold 7 fitting finished in 75.323s\n",
      "Model 5 fold 8\n",
      "('Score: ', 0.62011707604226651)\n",
      "Model 5 fold 8 fitting finished in 78.273s\n",
      "Model 5 fold 9\n",
      "('Score: ', 0.60817444600731685)\n",
      "Model 5 fold 9 fitting finished in 76.034s\n",
      "Model 5 fold 10\n",
      "('Score: ', 0.61251346928437345)\n",
      "Model 5 fold 10 fitting finished in 73.755s\n",
      "Score for model 5 is 0.596039\n",
      "Model 6:\n",
      "Model 6 fold 1\n",
      "('Score: ', 0.58335342104178356)\n",
      "Model 6 fold 1 fitting finished in 95.355s\n",
      "Model 6 fold 2\n",
      "('Score: ', 0.5703547955271826)\n",
      "Model 6 fold 2 fitting finished in 96.512s\n",
      "Model 6 fold 3\n",
      "('Score: ', 0.59447554770560895)\n",
      "Model 6 fold 3 fitting finished in 79.722s\n",
      "Model 6 fold 4\n",
      "('Score: ', 0.58017528150665865)\n",
      "Model 6 fold 4 fitting finished in 98.518s\n",
      "Model 6 fold 5\n",
      "('Score: ', 0.61023424051623421)\n",
      "Model 6 fold 5 fitting finished in 99.516s\n",
      "Model 6 fold 6\n",
      "('Score: ', 0.58831026352206917)\n",
      "Model 6 fold 6 fitting finished in 99.173s\n",
      "Model 6 fold 7\n",
      "('Score: ', 0.59267146206216015)\n",
      "Model 6 fold 7 fitting finished in 96.517s\n",
      "Model 6 fold 8\n",
      "('Score: ', 0.62012693855151857)\n",
      "Model 6 fold 8 fitting finished in 100.971s\n",
      "Model 6 fold 9\n",
      "('Score: ', 0.60817455483035443)\n",
      "Model 6 fold 9 fitting finished in 101.432s\n",
      "Model 6 fold 10\n",
      "('Score: ', 0.61250678790941393)\n",
      "Model 6 fold 10 fitting finished in 95.937s\n",
      "Score for model 6 is 0.596038\n",
      "Model 7:\n",
      "Model 7 fold 1\n",
      "('Score: ', 0.58335409996380727)\n",
      "Model 7 fold 1 fitting finished in 76.527s\n",
      "Model 7 fold 2\n",
      "('Score: ', 0.57035363152046636)\n",
      "Model 7 fold 2 fitting finished in 67.998s\n",
      "Model 7 fold 3\n",
      "('Score: ', 0.59447594828705475)\n",
      "Model 7 fold 3 fitting finished in 70.322s\n",
      "Model 7 fold 4\n",
      "('Score: ', 0.58017468749481005)\n",
      "Model 7 fold 4 fitting finished in 83.918s\n",
      "Model 7 fold 5\n",
      "('Score: ', 0.61023518377003816)\n",
      "Model 7 fold 5 fitting finished in 74.456s\n",
      "Model 7 fold 6\n",
      "('Score: ', 0.58831173486779897)\n",
      "Model 7 fold 6 fitting finished in 77.232s\n",
      "Model 7 fold 7\n",
      "('Score: ', 0.59267098676447072)\n",
      "Model 7 fold 7 fitting finished in 78.540s\n",
      "Model 7 fold 8\n",
      "('Score: ', 0.6201268747710631)\n",
      "Model 7 fold 8 fitting finished in 69.689s\n",
      "Model 7 fold 9\n",
      "('Score: ', 0.60817481879454838)\n",
      "Model 7 fold 9 fitting finished in 81.267s\n",
      "Model 7 fold 10\n",
      "('Score: ', 0.61250743516958328)\n",
      "Model 7 fold 10 fitting finished in 102.387s\n",
      "Score for model 7 is 0.596039\n",
      "Score for blended models is 0.598213\n"
     ]
    }
   ],
   "source": [
    "est = [LogisticRegression(C = C_ovr_lbfgs,\n",
    "                          solver = 'lbfgs',\n",
    "                          multi_class = 'ovr',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_ovr_sag,\n",
    "                          solver = 'sag',\n",
    "                          multi_class = 'ovr',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_ovr_newton,\n",
    "                          solver = 'newton-cg',\n",
    "                          multi_class = 'ovr',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_ovr_liblinear,\n",
    "                          solver = 'liblinear',\n",
    "                          multi_class = 'ovr',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_multinomial_lbfgs,\n",
    "                          solver = 'lbfgs',\n",
    "                          multi_class = 'multinomial',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_multinomial_sag,\n",
    "                          solver = 'sag',\n",
    "                          multi_class = 'multinomial',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-5,\n",
    "                          random_state = seed),\n",
    "      LogisticRegression(C = C_multinomial_newton,\n",
    "                          solver = 'newton-cg',\n",
    "                          multi_class = 'multinomial',\n",
    "                          n_jobs = -1, max_iter=10000000,tol = 1e-7,\n",
    "                          random_state = seed)]\n",
    "\n",
    "(train_blend_x_LR,\n",
    " test_blend_x_LR_mean,\n",
    " test_blend_x_LR_gmean,\n",
    " blend_scores_LR,\n",
    " best_rounds_LR) = LR_blend(est, \n",
    "                             train_X, train_y, \n",
    "                             test_X,\n",
    "                             10) #as the learning rate decreases the number of stopping rounds need to be increased\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5998131   0.59981265  0.59981346  0.59993284  0.59603874  0.59603833\n",
      "  0.59603854]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_LR_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_LR_mean_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_LR_gmean_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_LR,axis=0))\n",
    "print (np.mean(best_rounds_LR,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_LR, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_LR_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_LR_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LR_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_LR_mean[:,-3:])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.99825948e-01,   4.14264109e-01,   8.59099439e-02],\n",
       "       [  9.61754449e-01,   3.54476789e-02,   2.79787173e-03],\n",
       "       [  8.95388169e-01,   8.92753393e-02,   1.53364921e-02],\n",
       "       ..., \n",
       "       [  8.98833551e-01,   8.57164167e-02,   1.54500320e-02],\n",
       "       [  9.87695048e-01,   1.21304643e-02,   1.74487682e-04],\n",
       "       [  6.25250605e-01,   3.37126019e-01,   3.76233760e-02]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_LR_mean[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52045174,  0.39842677,  0.08112149,  0.52045357,  0.39843122,\n",
       "        0.0811152 ,  0.52045527,  0.39843378,  0.08111095,  0.51907442,\n",
       "        0.3980157 ,  0.08290988,  0.49980675,  0.41427613,  0.08591712,\n",
       "        0.49982491,  0.41426329,  0.08591179,  0.49982595,  0.41426411,\n",
       "        0.08590994])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_LR_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
