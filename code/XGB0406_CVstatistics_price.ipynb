{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn import model_selection,ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "import random\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, LabelBinarizer, MultiLabelBinarizer,LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_train(train,y):\n",
    "    xgtrain = xgb.DMatrix(train, label=y)\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=0\n",
    "    params['eta'] = 0.3\n",
    "    params['verbose_eval'] = False\n",
    "    params['max_depth'] = 6\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=0,callbacks=[xgb.callback.early_stop(20)]\n",
    "    )\n",
    "\n",
    "    return cv_result['test-mlogloss-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CV_st(train,test,feature1,feature2):\n",
    "    index=list(range(train.shape[0]))\n",
    "    random.shuffle(index)\n",
    "    kf = KFold(n_splits=5,shuffle=True, random_state=0)\n",
    "    \n",
    "    # median feature names\n",
    "    features_tmp = []\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_median')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].median().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[0]] = f_low\n",
    "    train[features_tmp[1]] = f_medium\n",
    "    train[features_tmp[2]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].median().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[0]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[1]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[2]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "\n",
    "    # mean feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_mean')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[3]] = f_low\n",
    "    train[features_tmp[4]] = f_medium\n",
    "    train[features_tmp[5]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[3]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[4]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[5]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # max feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_max')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].max().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[6]] = f_low\n",
    "    train[features_tmp[7]] = f_medium\n",
    "    train[features_tmp[8]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].max().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[6]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[7]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[8]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # min feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_min')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].min().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[9]] = f_low\n",
    "    train[features_tmp[10]] = f_medium\n",
    "    train[features_tmp[11]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].min().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[9]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[10]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[11]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "\n",
    "    # std feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_std') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_std') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_std')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].std().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[12]] = f_low\n",
    "    train[features_tmp[13]] = f_medium\n",
    "    train[features_tmp[14]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].std().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[12]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[13]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[14]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "#     # var feature names\n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_low_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_medium_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_high_var')\n",
    "    \n",
    "#     # train data \n",
    "#     f_low=pd.Series([np.nan]*len(train))\n",
    "#     f_medium=pd.Series([np.nan]*len(train))\n",
    "#     f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "#     for train_index, test_index in kf.split(index):\n",
    "#         tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#                 reset_index().rename(columns={feature2:'new'})\n",
    "#         f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                                             on=feature1,how='left')['new'].values   \n",
    "#     train[features_tmp[15]] = f_low\n",
    "#     train[features_tmp[16]] = f_medium\n",
    "#     train[features_tmp[17]] = f_high\n",
    "\n",
    "#     # test data\n",
    "#     tmp = train.groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#             reset_index().rename(columns={feature2:'new'})    \n",
    "#     test[features_tmp[15]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[16]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[17]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                        on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # ratio/diff feature\n",
    "    cols = features_tmp[:]\n",
    "#     features_tmp = []\n",
    "    for col in cols:\n",
    "        new_feature = col+'_ratio'\n",
    "        train[new_feature] = train[col] / train[feature2]\n",
    "        test[new_feature] = test[col] / test[feature2]\n",
    "        features_tmp.append(new_feature)\n",
    "        \n",
    "    print feature1,' vs ', feature2,'Done!'\n",
    "    return train,test,features_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#input data\n",
    "train_df=pd.read_json('../input/train.json').reset_index(drop = True)\n",
    "test_df=pd.read_json('../input/test.json').reset_index(drop = True)\n",
    "test_df.loc[test_df.bathrooms == 112.0,'bathrooms'] = 1.5    \n",
    "test_df.loc[test_df.bathrooms == 20.0,'bathrooms'] = 2.0\n",
    "test_df.loc[test_df.listing_id == 7220763,'bedrooms'] = 3\n",
    "test_df.loc[test_df.listing_id == 7047074,'bedrooms'] = 6\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    df[\"num_photo_count\"] = df[\"photos\"].apply(len)\n",
    "    df[\"street_address\"] = df['street_address'].apply(fmt)\n",
    "    df[\"display_address\"] = df[\"display_address\"].apply(fmt)\n",
    "    df[\"num_desc_wordcount\"] = df[\"description\"].apply(len)\n",
    "    df[\"num_pricePerBed\"] = df['price'] / df['bedrooms']\n",
    "    df[\"num_pricePerBath\"] = df['price'] / df['bathrooms']\n",
    "    df[\"num_pricePerRoom\"] = df['price'] / (df['bedrooms'] + df['bathrooms'])\n",
    "    df[\"num_bedPerBath\"] = df['bedrooms'] / df['bathrooms']\n",
    "    df[\"num_bedBathDiff\"] = df['bedrooms'] - df['bathrooms']\n",
    "    df[\"num_bedBathSum\"] = df[\"bedrooms\"] + df['bathrooms']\n",
    "    df[\"num_bedsPerc\"] = df[\"bedrooms\"] / (df['bedrooms'] + df['bathrooms'])\n",
    "\n",
    "    df = df.fillna(-1).replace(np.inf, -1)\n",
    "    return df\n",
    "\n",
    "# Add common features\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df) \n",
    "\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df['num_desc_length_null'] = (train_df.description.str.len()==0).astype(float)\n",
    "test_df['num_desc_length_null'] = (test_df.description.str.len()==0).astype(float)\n",
    "    \n",
    "features_to_use=[\n",
    "    \"latitude\", \"longitude\",\"num_pricePerBed\",\n",
    "    'num_bedBathSum','num_pricePerBath','num_pricePerRoom','num_bedPerBath',\n",
    "    'num_bedBathDiff','num_bedsPerc',\n",
    "    \"num_photo_count\", \"num_features\", \"num_desc_wordcount\",'num_desc_length_null',\n",
    "    \"listing_id\"]\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Location features: Latitude, longitude\n",
    "precision = 3\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2)\n",
    "train_df['num_dist_from_center'] = x.values\n",
    "x = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2)\n",
    "test_df['num_dist_from_center'] = x.values\n",
    "train_df['position'] = train_df.longitude.round(precision).astype(str) + '_' + train_df.latitude.round(precision).astype(str)\n",
    "test_df['position'] = test_df.longitude.round(precision).astype(str) + '_' + test_df.latitude.round(precision).astype(str)\n",
    "\n",
    "new_feature = ['num_dist_from_center']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Degree of \"outlierness\"\n",
    "OutlierAggregated = (train_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated2 = (test_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms < 1).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms < 1).astype(float)\n",
    "x = np.abs((train_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.abs((test_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.log1p(train_df.price/(train_df.bedrooms.clip(1,3) + train_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.log1p(test_df.price/(test_df.bedrooms.clip(1,3) + test_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "train_df['num_OutlierAggregated'] = OutlierAggregated.values\n",
    "test_df['num_OutlierAggregated'] = OutlierAggregated2.values\n",
    "\n",
    "\n",
    "new_feature = ['num_OutlierAggregated']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Density in unique locations at given precision\n",
    "vals = train_df['position'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df['num_pos_density'] = train_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df['num_pos_density'] = test_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "# Building null\n",
    "train_df['num_building_null'] = (train_df.building_id=='0').astype(float)\n",
    "test_df['num_building_null'] = (test_df.building_id=='0').astype(float)\n",
    "\n",
    "\n",
    "new_feature = ['num_pos_density','num_building_null']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Creation time features\n",
    "train_df['created'] = pd.to_datetime(train_df.created)\n",
    "train_df['num_created_weekday'] = train_df.created.dt.dayofweek.astype(float)\n",
    "train_df['num_created_weekofyear'] = train_df.created.dt.weekofyear\n",
    "train_df['num_created_day'] = train_df.created.dt.day\n",
    "train_df['num_created_month'] = train_df.created.dt.month\n",
    "train_df['num_created_hour'] = train_df.created.dt.hour\n",
    "  \n",
    "test_df['created'] = pd.to_datetime(test_df.created)\n",
    "test_df['num_created_weekday'] = test_df.created.dt.dayofweek\n",
    "test_df['num_created_weekofyear'] = test_df.created.dt.weekofyear\n",
    "test_df['num_created_day'] = test_df.created.dt.day\n",
    "test_df['num_created_month'] = test_df.created.dt.month\n",
    "test_df['num_created_hour'] = test_df.created.dt.hour\n",
    "\n",
    "\n",
    "new_feature = ['num_created_weekday','num_created_weekofyear','num_created_day','num_created_month','num_created_hour']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Bedrooms/Bathrooms/Price\n",
    "train_df['num_bathrooms'] = train_df.bathrooms.clip_upper(4)\n",
    "test_df['num_bathrooms'] = test_df.bathrooms.clip_upper(4)\n",
    "\n",
    "train_df['num_bedrooms'] = train_df.bedrooms.clip_upper(5)\n",
    "test_df['num_bedrooms'] = test_df.bedrooms.clip_upper(5)\n",
    "\n",
    "train_df['num_price'] = train_df.price.clip_upper(10000)\n",
    "test_df['num_price'] = test_df.price.clip_upper(10000)\n",
    "\n",
    "bins = train_df.price.quantile(np.arange(0.05, 1, 0.05))\n",
    "train_df['num_price_q'] = np.digitize(train_df.price, bins)\n",
    "test_df['num_price_q'] = np.digitize(test_df.price, bins)\n",
    "\n",
    "\n",
    "new_feature = ['num_bathrooms','num_bedrooms','num_price','num_price_q']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Composite features based on: \n",
    "# https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths\n",
    "train_df['num_priceXroom'] = (train_df.price / (1 + train_df.bedrooms.clip(1, 4) + 0.5*train_df.bathrooms.clip(0, 2))).values\n",
    "test_df['num_priceXroom'] = (test_df.price / (1 + test_df.bedrooms.clip(1, 4) + 0.5*test_df.bathrooms.clip(0, 2))).values\n",
    "\n",
    "train_df['num_even_bathrooms'] = ((np.round(train_df.bathrooms) - train_df.bathrooms)==0).astype(float)\n",
    "test_df['num_even_bathrooms'] = ((np.round(test_df.bathrooms) - test_df.bathrooms)==0).astype(float)\n",
    "\n",
    "new_feature = ['num_priceXroom','num_even_bathrooms']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\",'position']\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            if f not in features_to_use:\n",
    "                features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dftemp = train_df.copy()\n",
    "for i in ['latitude', 'longitude']:\n",
    "    while(1):\n",
    "        x = dftemp[i].median()\n",
    "        ix = abs(dftemp[i] - x) > 3*dftemp[i].std()\n",
    "        if ix.sum()==0:\n",
    "            break\n",
    "        dftemp.loc[ix, i] = np.nan\n",
    "dftemp = dftemp.loc[dftemp[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n",
    "\n",
    "dfm = DataFrameMapper([(['latitude'], [StandardScaler()]), (['longitude'], [StandardScaler()])])\n",
    "\n",
    "for i in [6]:\n",
    "    pipe_location = make_pipeline(dfm, KMeans(n_clusters=i, random_state=1))\n",
    "    pipe_location.fit(dftemp);\n",
    "    train_df['location_'+str(i)] = pipe_location.predict(train_df).astype(str)\n",
    "    test_df['location_'+str(i)] = pipe_location.predict(test_df).astype(str)\n",
    "for i in train_df.location_6.unique():\n",
    "    f = 'num_location_6_'+str(i)\n",
    "    train_df[f] = (train_df.location_6==i).astype(float)\n",
    "    test_df[f] = (test_df.location_6==i).astype(float)\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "    \n",
    "    \n",
    "train_df['tmp_bathrooms'] = train_df.bathrooms.clip_upper(2)\n",
    "test_df['tmp_bathrooms'] = test_df.bathrooms.clip_upper(2)\n",
    "train_df['tmp_bedrooms'] = train_df.bedrooms.clip_upper(4)\n",
    "test_df['tmp_bedrooms'] = test_df.bedrooms.clip_upper(4)\n",
    "train_df['roomcal'] = train_df.tmp_bedrooms.astype(str) + '_' + train_df.tmp_bathrooms.astype(str)    \n",
    "test_df['roomcal'] = test_df.tmp_bedrooms.astype(str) + '_' + test_df.tmp_bathrooms.astype(str)    \n",
    "\n",
    "room_lb = LabelBinarizer()\n",
    "room_lb.fit(train_df['roomcal'])\n",
    "room_col = ['num_room_type_' + str(x) for x in range(len(train_df['roomcal'].unique()))]\n",
    "for f in room_col:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "train_df = train_df.join(pd.DataFrame(room_lb.transform(train_df['roomcal']),columns=room_col,index=train_df.index))\n",
    "test_df = test_df.join(pd.DataFrame(room_lb.transform(test_df['roomcal']),columns=room_col,index=test_df.index))\n",
    "\n",
    "tmp = train_df.groupby(['roomcal','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "\n",
    "test_df.loc[27462,'num_6_median_price'] =  7200.0\n",
    "\n",
    "train_df['num_6_price_ratio'] = train_df['num_price'] / train_df['num_6_median_price']\n",
    "train_df['num_6_price_diff'] = train_df['num_price'] - train_df['num_6_median_price']\n",
    "test_df['num_6_price_ratio'] = test_df['num_price'] / test_df['num_6_median_price']\n",
    "test_df['num_6_price_diff'] = test_df['num_price'] - test_df['num_6_median_price']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price','num_6_price_ratio','num_6_price_diff']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = train_df.groupby(['num_bedrooms','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price_bedroom'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "\n",
    "train_df['num_6_price_ratio_bedroom'] = train_df['num_price'] / train_df['num_6_median_price_bedroom']\n",
    "train_df['num_6_price_diff_bedroom'] = train_df['num_price'] - train_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_ratio_bedroom'] = test_df['num_price'] / test_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_diff_bedroom'] = test_df['num_price'] - test_df['num_6_median_price_bedroom']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price_bedroom','num_6_price_ratio_bedroom','num_6_price_diff_bedroom']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322)\n",
      "(74659, 322)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "print train_X_0322.shape\n",
    "print test_X_0322.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment = [\n",
    "       'feature_1_month_free', 'feature_24/7_concierge',\n",
    "       'feature_24/7_doorman', 'feature_24/7_doorman_concierge',\n",
    "       'feature_actual_apt._photos', 'feature_air_conditioning',\n",
    "       'feature_all_pets_ok', 'feature_all_utilities_included',\n",
    "       'feature_assigned-parking-space', 'feature_attended_lobby',\n",
    "       'feature_backyard', 'feature_balcony', 'feature_basement_storage',\n",
    "       'feature_basketball_court', 'feature_bike_room',\n",
    "       'feature_bike_storage', 'feature_billiards_room',\n",
    "       'feature_billiards_table_and_wet_bar', 'feature_brand_new',\n",
    "       'feature_breakfast_bar', 'feature_bright', 'feature_brownstone',\n",
    "       'feature_building-common-outdoor-space', 'feature_business_center',\n",
    "       'feature_cable/satellite_tv', 'feature_cable_ready',\n",
    "       'feature_call/text_abraham_caro_@_917-373-0862',\n",
    "       'feature_cats_allowed', 'feature_central_a/c', 'feature_central_ac',\n",
    "       'feature_central_air', 'feature_chefs_kitchen',\n",
    "       \"feature_children's_playroom\", 'feature_childrens_playroom',\n",
    "       'feature_cinema_room', 'feature_city_view',\n",
    "       'feature_close_to_subway', 'feature_closets_galore!',\n",
    "       'feature_club_sun_deck_has_spectacular_city_and_river_views',\n",
    "       'feature_cold_storage', 'feature_common_backyard',\n",
    "       'feature_common_garden', 'feature_common_outdoor_space',\n",
    "       'feature_common_parking/garage', 'feature_common_roof_deck',\n",
    "       'feature_common_storage', 'feature_common_terrace',\n",
    "       'feature_community_recreation_facilities',\n",
    "       'feature_complimentary_sunday_brunch', 'feature_concierge',\n",
    "       'feature_concierge_service', 'feature_condo_finishes',\n",
    "       'feature_courtyard', 'feature_crown_moldings', 'feature_deck',\n",
    "       'feature_deco_brick_wall', 'feature_decorative_fireplace',\n",
    "       'feature_dining_room', 'feature_dishwasher', 'feature_dogs_allowed',\n",
    "       'feature_doorman', 'feature_dry_cleaning_service',\n",
    "       'feature_dryer_in_unit', 'feature_duplex', 'feature_duplex_lounge',\n",
    "       'feature_eat-in_kitchen', 'feature_eat_in_kitchen',\n",
    "       'feature_elegant_glass-enclosed_private_lounge_with_magnificent_river_views',\n",
    "       'feature_elevator', 'feature_exclusive',\n",
    "       'feature_exercise/yoga_studio', 'feature_exposed_brick',\n",
    "       'feature_extra_room', 'feature_fireplace', 'feature_fireplaces',\n",
    "       'feature_fitness_center', 'feature_fitness_room', 'feature_flex-2',\n",
    "       'feature_flex-3', 'feature_free_wifi_in_club_lounge',\n",
    "       'feature_ft_doorman', 'feature_full-time_doorman',\n",
    "       'feature_full_service_garage',\n",
    "       'feature_fully-equipped_club_fitness_center',\n",
    "       'feature_fully__equipped', 'feature_furnished', 'feature_game_room',\n",
    "       'feature_garage', 'feature_garbage_disposal', 'feature_garden',\n",
    "       'feature_garden/patio', 'feature_granite_countertops',\n",
    "       'feature_granite_kitchen', 'feature_green_building',\n",
    "       'feature_guarantors_accepted', 'feature_gut_renovated',\n",
    "       'feature_gym', 'feature_gym/fitness', 'feature_gym_in_building',\n",
    "       'feature_hardwood', 'feature_hardwood_floors',\n",
    "       'feature_health_club', 'feature_hi_rise',\n",
    "       'feature_high-speed_internet', 'feature_high_ceiling',\n",
    "       'feature_high_ceilings', 'feature_high_speed_internet',\n",
    "       'feature_highrise', 'feature_housekeeping_service',\n",
    "       'feature_in-unit_washer/dryer', 'feature_indoor_pool',\n",
    "       'feature_intercom', 'feature_jacuzzi', 'feature_large_living_room',\n",
    "       'feature_laundry', 'feature_laundry_&_housekeeping',\n",
    "       'feature_laundry_in_building', 'feature_laundry_in_unit',\n",
    "       'feature_laundry_on_every_floor', 'feature_laundry_on_floor',\n",
    "       'feature_laundry_room', 'feature_light', 'feature_live-in_super',\n",
    "       'feature_live-in_superintendent', 'feature_live/work',\n",
    "       'feature_live_in_super', 'feature_loft', 'feature_lounge',\n",
    "       'feature_lounge_room', 'feature_lowrise', 'feature_luxury_building',\n",
    "       'feature_magnificent_venetian-style', 'feature_mail_room',\n",
    "       'feature_marble_bath', 'feature_marble_bathroom',\n",
    "       'feature_media_room', 'feature_media_screening_room',\n",
    "       'feature_microwave', 'feature_midrise', 'feature_multi-level',\n",
    "       'feature_new_construction', 'feature_newly_renovated',\n",
    "       'feature_no_fee', 'feature_no_pets', 'feature_on-site_atm_machine',\n",
    "       'feature_on-site_attended_garage', 'feature_on-site_garage',\n",
    "       'feature_on-site_laundry',\n",
    "#        'feature_on-site_lifestyle_concierge_by_luxury_attach\\xc3\\xa9',\n",
    "       'feature_on-site_parking', 'feature_on-site_parking_available',\n",
    "       'feature_on-site_parking_lot', 'feature_on-site_super',\n",
    "       'feature_one_month_free', 'feature_outdoor_areas',\n",
    "       'feature_outdoor_entertainment_space', 'feature_outdoor_pool',\n",
    "       'feature_outdoor_roof_deck_overlooking_new_york_harbor_and_battery_park',\n",
    "       'feature_outdoor_space', 'feature_package_room', 'feature_parking',\n",
    "       'feature_parking_available', 'feature_parking_space',\n",
    "       'feature_part-time_doorman', 'feature_party_room', 'feature_patio',\n",
    "       'feature_penthouse', 'feature_pet_friendly', 'feature_pets',\n",
    "       'feature_pets_allowed', 'feature_pets_on_approval',\n",
    "       'feature_playroom', 'feature_playroom/nursery', 'feature_pool',\n",
    "       'feature_post-war', 'feature_post_war', 'feature_pre-war',\n",
    "       'feature_pre_war', 'feature_prewar', 'feature_private-balcony',\n",
    "       'feature_private-outdoor-space', 'feature_private_backyard',\n",
    "       'feature_private_balcony', 'feature_private_deck',\n",
    "       'feature_private_garden',\n",
    "       'feature_private_laundry_room_on_every_floor',\n",
    "       'feature_private_outdoor_space', 'feature_private_parking',\n",
    "       'feature_private_roof_deck', 'feature_private_roofdeck',\n",
    "       'feature_private_terrace', 'feature_publicoutdoor',\n",
    "       'feature_queen_size_bedrooms', 'feature_queen_sized_rooms',\n",
    "       'feature_reduced_fee', 'feature_renovated',\n",
    "       'feature_renovated_kitchen', 'feature_residents_garden',\n",
    "       'feature_residents_lounge', 'feature_roof-deck',\n",
    "       'feature_roof_access', 'feature_roof_deck',\n",
    "       'feature_roof_deck_with_grills', 'feature_roofdeck',\n",
    "       'feature_rooftop_deck', 'feature_rooftop_terrace',\n",
    "       'feature_s/s_appliances', 'feature_sauna', 'feature_screening_room',\n",
    "       'feature_separate_kitchen', 'feature_shared_backyard',\n",
    "       'feature_shared_garden', 'feature_shares_ok',\n",
    "       'feature_short_term_allowed', 'feature_simplex', 'feature_skylight',\n",
    "       'feature_skylight_atrium', 'feature_southern_exposure',\n",
    "       'feature_spa_services', 'feature_ss_appliances',\n",
    "       'feature_stainless_steel', 'feature_stainless_steel_appliances',\n",
    "       'feature_state-of-the-art_fitness_center', 'feature_storage',\n",
    "       'feature_storage_available', 'feature_storage_facilities_available',\n",
    "       'feature_storage_room', 'feature_sublet', 'feature_subway',\n",
    "       'feature_sundeck', 'feature_swimming_pool', 'feature_tenant_lounge',\n",
    "       'feature_terrace', 'feature_terraces_/_balconies',\n",
    "       'feature_tons_of_natural_light', 'feature_valet',\n",
    "       'feature_valet_parking', 'feature_valet_service',\n",
    "       'feature_valet_services',\n",
    "       'feature_valet_services_including_dry_cleaning',\n",
    "       'feature_video_intercom', 'feature_view', 'feature_virtual_doorman',\n",
    "       'feature_virtual_tour', 'feature_walk-in_closet', 'feature_walk-up',\n",
    "       'feature_walk_in_closet', 'feature_walk_in_closet(s)',\n",
    "       'feature_washer/dryer', 'feature_washer/dryer_hookup',\n",
    "       'feature_washer/dryer_in-unit', 'feature_washer/dryer_in_building',\n",
    "       'feature_washer/dryer_in_unit', 'feature_washer_&_dryer',\n",
    "       'feature_washer_in_unit', 'feature_wheelchair_access',\n",
    "       'feature_wheelchair_ramp', 'feature_wifi', 'feature_wifi_access',\n",
    "       'feature_wood-burning_fireplace', 'feature_yard',\n",
    "       'feature_yoga_classes','building_id_mean_med',\n",
    "       'building_id_mean_high', 'manager_id_mean_med',\n",
    "       'manager_id_mean_high','median_price_bed', 'ratio_bed',\n",
    "       'compound', 'neg', 'neu', 'pos', 'street',\n",
    "       'avenue', 'east', 'west', 'north', 'south', 'other_address',\n",
    "       'top_10_manager', 'top_25_manager', 'top_5_manager',\n",
    "       'top_50_manager', 'top_1_manager', 'top_2_manager',\n",
    "       'top_15_manager', 'top_20_manager', 'top_30_manager',\n",
    "       'Zero_building_id', 'top_10_building', 'top_25_building',\n",
    "       'top_5_building', 'top_50_building', 'top_1_building',\n",
    "       'top_2_building', 'top_15_building', 'top_20_building',\n",
    "       'top_30_building','listing_id'\n",
    "]\n",
    "\n",
    "train_df = train_df.merge(train_X_0322[sentiment],on='listing_id', how='left')\n",
    "test_df = test_df.merge(test_X_0322[sentiment],on='listing_id', how='left')\n",
    "\n",
    "for f in sentiment:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','price')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_ratio_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_ratio_bedroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_priceXroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff_bedroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  ratio_bed Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','ratio_bed')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_ratio Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_ratio')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_created_hour Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_created_hour')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_photo_count Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_photo_count')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_dist_from_center Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_dist_from_center')\n",
    "\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  latitude Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','latitude')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerBed Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerBed')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerBath Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerBath')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_features Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_features')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_desc_wordcount Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_desc_wordcount')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerRoom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerRoom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  longitude Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','longitude')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pos_density Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pos_density')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 394) (74659, 394)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n",
    "\n",
    "train_X.replace(np.inf, np.nan)\n",
    "test_X.replace(np.inf, np.nan)\n",
    "\n",
    "train_X.loc[:,'num_nan'] = train_X.isnull().sum(axis=1)\n",
    "test_X.loc[:,'num_nan'] = test_X.isnull().sum(axis=1)\n",
    "\n",
    "target_num_map = {'high':2, 'medium':1, 'low':0}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print train_X.shape, test_X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>num_pricePerBed</th>\n",
       "      <th>num_bedBathSum</th>\n",
       "      <th>num_pricePerBath</th>\n",
       "      <th>num_pricePerRoom</th>\n",
       "      <th>num_bedPerBath</th>\n",
       "      <th>num_bedBathDiff</th>\n",
       "      <th>num_bedsPerc</th>\n",
       "      <th>num_photo_count</th>\n",
       "      <th>...</th>\n",
       "      <th>manager_id_price_low_max</th>\n",
       "      <th>manager_id_price_medium_max</th>\n",
       "      <th>manager_id_price_high_max</th>\n",
       "      <th>manager_id_price_low_min</th>\n",
       "      <th>manager_id_price_medium_min</th>\n",
       "      <th>manager_id_price_high_min</th>\n",
       "      <th>manager_id_price_low_std</th>\n",
       "      <th>manager_id_price_medium_std</th>\n",
       "      <th>manager_id_price_high_std</th>\n",
       "      <th>num_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.7145</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>666.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>523.428881</td>\n",
       "      <td>758.927692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.7947</td>\n",
       "      <td>-73.9667</td>\n",
       "      <td>2732.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5465.0</td>\n",
       "      <td>1821.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3005.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1583.222479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.7388</td>\n",
       "      <td>-74.0018</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>1425.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>8795.0</td>\n",
       "      <td>7995.0</td>\n",
       "      <td>3895.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>1524.253239</td>\n",
       "      <td>1413.697943</td>\n",
       "      <td>1062.802898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.7539</td>\n",
       "      <td>-73.9677</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>1637.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11100.0</td>\n",
       "      <td>7495.0</td>\n",
       "      <td>6350.0</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>1407.452620</td>\n",
       "      <td>1636.311315</td>\n",
       "      <td>1622.957508</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.8241</td>\n",
       "      <td>-73.9493</td>\n",
       "      <td>837.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1495.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1102.296157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude  num_pricePerBed  num_bedBathSum  num_pricePerBath  \\\n",
       "0   40.7145   -73.9425           1000.0             4.5            2000.0   \n",
       "1   40.7947   -73.9667           2732.5             3.0            5465.0   \n",
       "2   40.7388   -74.0018           2850.0             2.0            2850.0   \n",
       "3   40.7539   -73.9677           3275.0             2.0            3275.0   \n",
       "4   40.8241   -73.9493            837.5             5.0            3350.0   \n",
       "\n",
       "   num_pricePerRoom  num_bedPerBath  num_bedBathDiff  num_bedsPerc  \\\n",
       "0        666.666667             2.0              1.5      0.666667   \n",
       "1       1821.666667             2.0              1.0      0.666667   \n",
       "2       1425.000000             1.0              0.0      0.500000   \n",
       "3       1637.500000             1.0              0.0      0.500000   \n",
       "4        670.000000             4.0              3.0      0.800000   \n",
       "\n",
       "   num_photo_count   ...     manager_id_price_low_max  \\\n",
       "0                5   ...                       4100.0   \n",
       "1               11   ...                       9800.0   \n",
       "2                8   ...                       8795.0   \n",
       "3                3   ...                      11100.0   \n",
       "4                3   ...                       5000.0   \n",
       "\n",
       "   manager_id_price_medium_max  manager_id_price_high_max  \\\n",
       "0                       4400.0                        NaN   \n",
       "1                       1995.0                        NaN   \n",
       "2                       7995.0                     3895.0   \n",
       "3                       7495.0                     6350.0   \n",
       "4                          NaN                        NaN   \n",
       "\n",
       "   manager_id_price_low_min  manager_id_price_medium_min  \\\n",
       "0                    1700.0                       1800.0   \n",
       "1                    3005.0                       1995.0   \n",
       "2                    2100.0                       1850.0   \n",
       "3                    1775.0                       1995.0   \n",
       "4                    1495.0                          NaN   \n",
       "\n",
       "   manager_id_price_high_min  manager_id_price_low_std  \\\n",
       "0                        NaN                523.428881   \n",
       "1                        NaN               1583.222479   \n",
       "2                     1650.0               1524.253239   \n",
       "3                     1695.0               1407.452620   \n",
       "4                        NaN               1102.296157   \n",
       "\n",
       "   manager_id_price_medium_std  manager_id_price_high_std  num_nan  \n",
       "0                   758.927692                        NaN       10  \n",
       "1                          NaN                        NaN       12  \n",
       "2                  1413.697943                1062.802898        0  \n",
       "3                  1636.311315                1622.957508        0  \n",
       "4                          NaN                        NaN       20  \n",
       "\n",
       "[5 rows x 394 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 394)\n",
      "(9871, 394)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.530492 695\n",
      "4 \t0.528765 525\n",
      "5 \t0.529225 350\n",
      "6 \t0.529538 213\n",
      "7 \t0.529026 179\n",
      "8 \t0.532885 144\n",
      "9 \t0.536976 113\n",
      "10 \t0.541965 74\n",
      "11 \t0.546624 62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-523b73c9314c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[1;31m# check evaluation result.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst_eval_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36meval_set\u001b[0;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[1;32m    877\u001b[0m         _check_call(_LIB.XGBoosterEvalOneIter(self.handle, iteration,\n\u001b[1;32m    878\u001b[0m                                               \u001b[0mdmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                                               ctypes.byref(msg)))\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# max_depth = train_param\n",
    "max_depth = 9\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_score = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t0.536106 109\n",
      "4 \t0.533341 130\n",
      "8 \t0.530499 134\n",
      "12 \t0.528106 140\n",
      "16 \t0.527847 165\n",
      "20 \t0.529206 146\n",
      "24 \t0.527782 138\n",
      "28 \t0.528749 146\n",
      "32 \t0.530082 138\n",
      "40 \t0.529713 208\n",
      "48 \t0.528488 212\n",
      "64 \t0.529307 220\n",
      "80 \t0.529193 230\n",
      "128 \t0.532131 221\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20,24,28,32,40,48,64,80,128]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = 48\n",
    "print min_child_weight\n",
    "best_score = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 \t0.535266 355\n",
      "0.1 \t0.530002 293\n",
      "0.2 \t0.526755 229\n",
      "0.3 \t0.527294 166\n",
      "0.4 \t0.528033 218\n",
      "0.5 \t0.526212 217\n",
      "0.6 \t0.528171 215\n",
      "0.7 \t0.528578 151\n",
      "0.8 \t0.530005 163\n",
      "0.9 \t0.529102 160\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.534286 143\n",
      "0.6 \t0.532799 180\n",
      "0.7 \t0.530517 180\n",
      "0.8 \t0.530897 204\n",
      "0.9 \t0.529085 194\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.527212 180\n",
      "0.6 \t0.52709 234\n",
      "0.9 \t0.526593 222\n",
      "1.2 \t0.528878 238\n",
      "1.5 \t0.526974 193\n",
      "1.8 \t0.527208 188\n",
      "2.1 \t0.526333 193\n",
      "2.4 \t0.52557 207\n",
      "2.7 \t0.526084 240\n",
      "3.0 \t0.52666 224\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[428]\ttrain-mlogloss:0.363966+0.00300174\ttest-mlogloss:0.527457+0.00659426\n",
      "\n",
      "    1 | 27m24s | \u001b[35m  -0.52746\u001b[0m | \u001b[32m            0.5300\u001b[0m | \u001b[32m   2.5164\u001b[0m | \u001b[32m     6.7811\u001b[0m | \u001b[32m           36.9457\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-mlogloss:0.322439+0.00229235\ttest-mlogloss:0.528592+0.00693657\n",
      "\n",
      "    2 | 27m15s |   -0.52859 |             0.6644 |    2.0657 |     11.1918 |            52.9145 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-mlogloss:0.360994+0.00153016\ttest-mlogloss:0.528472+0.00710817\n",
      "\n",
      "    3 | 17m09s |   -0.52847 |             0.2023 |    1.6117 |      8.3270 |            78.7071 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[176]\ttrain-mlogloss:0.304+0.0033053\ttest-mlogloss:0.528335+0.00658829\n",
      "\n",
      "    4 | 22m13s |   -0.52834 |             0.4802 |    1.0852 |     11.8751 |            43.5901 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[360]\ttrain-mlogloss:0.364315+0.00119632\ttest-mlogloss:0.529083+0.00690435\n",
      "\n",
      "    5 | 54m12s |   -0.52908 |             0.7922 |    3.6335 |     11.1882 |            89.3461 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[301]\ttrain-mlogloss:0.385243+0.00226347\ttest-mlogloss:0.52922+0.00642015\n",
      "\n",
      "    6 | 26m00s |   -0.52922 |             0.6477 |    0.1119 |      7.4093 |            96.2057 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[519]\ttrain-mlogloss:0.369016+0.00197565\ttest-mlogloss:0.527703+0.00731201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0003838]), 'nit': 4, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7 | 16m01s |   -0.52770 |             0.1049 |    2.4241 |      7.3114 |            37.9278 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-mlogloss:0.348944+0.00172091\ttest-mlogloss:0.525854+0.00681073\n",
      "\n",
      "    8 | 28m49s | \u001b[35m  -0.52585\u001b[0m | \u001b[32m            0.4091\u001b[0m | \u001b[32m   3.8706\u001b[0m | \u001b[32m    12.0160\u001b[0m | \u001b[32m           32.5874\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-mlogloss:0.348944+0.00172091\ttest-mlogloss:0.525854+0.00681073\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00090961]), 'nit': 0, 'funcalls': 21}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 34m46s |   -0.52585 |             0.4091 |    3.8706 |     12.0160 |            32.5874 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\ttrain-mlogloss:0.323776+0.00206345\ttest-mlogloss:0.529308+0.00653546\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00553392]), 'nit': 5, 'funcalls': 59}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00571516]), 'nit': 11, 'funcalls': 66}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 | 37m54s |   -0.52931 |             0.7969 |    3.1037 |     13.4687 |            34.6022 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[508]\ttrain-mlogloss:0.349179+0.00116203\ttest-mlogloss:0.530599+0.00694128\n",
      "\n",
      "   11 | 27m02s |   -0.53060 |             0.1374 |    2.5442 |     13.4305 |            99.1244 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1439]\ttrain-mlogloss:0.429442+0.00211071\ttest-mlogloss:0.528189+0.00660848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.28920826e-05]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00017931]), 'nit': 19, 'funcalls': 70}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00015384]), 'nit': 8, 'funcalls': 66}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 68m05s |   -0.52819 |             0.3700 |    3.9238 |      5.3241 |            96.5526 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[484]\ttrain-mlogloss:0.397501+0.00177919\ttest-mlogloss:0.527865+0.0061275\n",
      "\n",
      "   13 | 16m02s |   -0.52787 |             0.1820 |    0.7637 |      5.2381 |            49.4190 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[789]\ttrain-mlogloss:0.392444+0.00281413\ttest-mlogloss:0.526668+0.00630868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00025387]), 'nit': 6, 'funcalls': 67}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 58m32s |   -0.52667 |             0.5682 |    3.9033 |      6.1013 |            42.5387 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[449]\ttrain-mlogloss:0.348903+0.00124365\ttest-mlogloss:0.526126+0.00618817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -5.64888451e-05]), 'nit': 6, 'funcalls': 60}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 28m27s |   -0.52613 |             0.3221 |    2.7118 |      7.4455 |            31.1321 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[546]\ttrain-mlogloss:0.405224+0.00253266\ttest-mlogloss:0.527397+0.00644255\n",
      "\n",
      "   16 | 30m37s |   -0.52740 |             0.4752 |    2.7278 |      5.8785 |            59.6967 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[449]\ttrain-mlogloss:0.348903+0.00124365\ttest-mlogloss:0.526126+0.00618817\n",
      "\n",
      "   17 | 27m18s |   -0.52613 |             0.3221 |    2.7118 |      7.4455 |            31.1321 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c11fd7636467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mxgb_BO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\bayes_opt\\bayesian_optimization.pyc\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[1;31m# Append most recently generated values to X and Y arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[1;31m# Updating the GP.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-c11fd7636467>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(min_child_weight, colsample_bytree, max_depth, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    395\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[1;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, gamma): #, subsample\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = 0.99# max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=1234,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (5,14),\n",
    "        'min_child_weight': (30,100),\n",
    "        'colsample_bytree': (0.1,0.8),\n",
    "#         'subsample': (0.7,1),\n",
    "        'gamma': (0,4)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.016003</td>\n",
       "      <td>32.587412</td>\n",
       "      <td>0.409076</td>\n",
       "      <td>3.870580</td>\n",
       "      <td>-0.525854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.016004</td>\n",
       "      <td>32.587412</td>\n",
       "      <td>0.409076</td>\n",
       "      <td>3.870580</td>\n",
       "      <td>-0.525854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.445485</td>\n",
       "      <td>31.132142</td>\n",
       "      <td>0.322098</td>\n",
       "      <td>2.711777</td>\n",
       "      <td>-0.526126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.445485</td>\n",
       "      <td>31.132142</td>\n",
       "      <td>0.322098</td>\n",
       "      <td>2.711777</td>\n",
       "      <td>-0.526126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.101341</td>\n",
       "      <td>42.538652</td>\n",
       "      <td>0.568188</td>\n",
       "      <td>3.903256</td>\n",
       "      <td>-0.526668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.878451</td>\n",
       "      <td>59.696664</td>\n",
       "      <td>0.475184</td>\n",
       "      <td>2.727763</td>\n",
       "      <td>-0.527397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.311439</td>\n",
       "      <td>37.927790</td>\n",
       "      <td>0.104916</td>\n",
       "      <td>2.424076</td>\n",
       "      <td>-0.527703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.238122</td>\n",
       "      <td>49.419011</td>\n",
       "      <td>0.182047</td>\n",
       "      <td>0.763688</td>\n",
       "      <td>-0.527865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.324136</td>\n",
       "      <td>96.552623</td>\n",
       "      <td>0.370024</td>\n",
       "      <td>3.923847</td>\n",
       "      <td>-0.528189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.409311</td>\n",
       "      <td>96.205662</td>\n",
       "      <td>0.647738</td>\n",
       "      <td>0.111881</td>\n",
       "      <td>-0.529220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree     gamma     score\n",
       "2   12.016003         32.587412          0.409076  3.870580 -0.525854\n",
       "3   12.016004         32.587412          0.409076  3.870580 -0.525854\n",
       "9    7.445485         31.132142          0.322098  2.711777 -0.526126\n",
       "11   7.445485         31.132142          0.322098  2.711777 -0.526126\n",
       "8    6.101341         42.538652          0.568188  3.903256 -0.526668\n",
       "10   5.878451         59.696664          0.475184  2.727763 -0.527397\n",
       "1    7.311439         37.927790          0.104916  2.424076 -0.527703\n",
       "7    5.238122         49.419011          0.182047  0.763688 -0.527865\n",
       "6    5.324136         96.552623          0.370024  3.923847 -0.528189\n",
       "0    7.409311         96.205662          0.647738  0.111881 -0.529220"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "#                                s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "#                                        'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=1234)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.02)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]      \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 5 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4091,\n",
      "       gamma=3.8706, learning_rate=0.02, max_delta_step=0, max_depth=12,\n",
      "       min_child_weight=32, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 1 fold 1\n",
      "best round 2232\n",
      "('Score: ', 0.53155544781621866)\n",
      "Model 1 fold 1 fitting finished in 2715.331s\n",
      "Model 1 fold 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6e3face9fbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                               \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                               \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                               300)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-e3465f52b445>\u001b[0m in \u001b[0;36mxgb_blend\u001b[0;34m(estimators, train_x, train_y, test_x, fold, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0meval_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     verbose=False)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mbest_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbest_rounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "#     xgb.XGBClassifier(max_depth = 7,\n",
    "#                               min_child_weight = 24,\n",
    "#                               colsample_bytree = 0.309861 ,\n",
    "#                               subsample = 0.998132 ,\n",
    "#                               gamma = 2.211859),\n",
    "#              xgb.XGBClassifier(max_depth = 6,\n",
    "#                               min_child_weight = 19,\n",
    "#                               colsample_bytree = 0.432358,\n",
    "#                               subsample = 0.949350,\n",
    "#                               gamma = 2.976848),\n",
    "#              xgb.XGBClassifier(max_depth = 7,\n",
    "#                               min_child_weight = 23,\n",
    "#                               colsample_bytree = 0.214791,\n",
    "#                               subsample = 0.997197,\n",
    "#                               gamma = 2.163581),         \n",
    "#              xgb.XGBClassifier(max_depth = 8,\n",
    "#                               min_child_weight = 23,\n",
    "#                               colsample_bytree = 0.5,\n",
    "#                               subsample = 0.988002,\n",
    "#                               gamma = 3.0),  \n",
    "             xgb.XGBClassifier(max_depth = 12,\n",
    "                              min_child_weight = 32,\n",
    "                              colsample_bytree = 0.4091,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 3.8706)              \n",
    "             ]\n",
    "\n",
    "#  \tmax_depth \tmin_child_weight \tcolsample_bytree \tgamma \tscore\n",
    "# 2 \t12.016003 \t32.587412 \t0.409076 \t3.870580 \t-0.525854\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb_mean,\n",
    " test_blend_x_xgb_gmean,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              5,\n",
    "                              300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x_xgb = pd.DataFrame(train_blend_x_xgb)\n",
    "train_blend_x_xgb.columns = [\"low\", \"medium\", \"high\"]\n",
    "train_blend_x_xgb[\"listing_id\"] = train_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_mean = pd.DataFrame(test_blend_x_xgb_mean)\n",
    "test_blend_x_xgb_mean.columns = [\"low\", \"medium\", \"high\"]\n",
    "test_blend_x_xgb_mean[\"listing_id\"] = test_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_gmean = pd.DataFrame(test_blend_x_xgb_gmean)\n",
    "test_blend_x_xgb_gmean.columns = [\"low\", \"medium\", \"high\"]\n",
    "test_blend_x_xgb_gmean[\"listing_id\"] = test_X.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train = train_X_0322[['listing_id']].merge(train_blend_x_xgb,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values\n",
    "tmp_test_mean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_mean,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values\n",
    "tmp_test_gmean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_gmean,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_mean_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_gmean_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,tmp_train, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,tmp_test_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,tmp_test_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
