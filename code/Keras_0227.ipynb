{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,ParametricSoftplus,ThresholdedReLU,SReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 16)\n",
      "(74659, 15)\n",
      "49352\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file).reset_index()\n",
    "test_df = pd.read_json(test_file).reset_index()\n",
    "ntrain = train_df.shape[0]\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sc_price\n",
    "tmp = pd.concat([train_df['price'],test_df['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "llimit = np.percentile(tmp.values, 1)\n",
    "\n",
    "train_df.loc[:,'sc_price'] = train_df['price'].values.reshape(-1, 1)\n",
    "test_df.loc[:,'sc_price'] = test_df['price'].values.reshape(-1, 1)\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "train_df.loc[train_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "test_df.loc[test_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "\n",
    "\n",
    "\n",
    "# sc_ba_price\n",
    "inx_train = train_df['bathrooms'] == 0\n",
    "inx_test = test_df['bathrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_ba_price'] = train_df.loc[non0_inx_train,'sc_price']\\\n",
    "                                                /train_df.loc[non0_inx_train,'bathrooms']\n",
    "test_df.loc[non0_inx_test,'sc_ba_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bathrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_ba_price'] = 0\n",
    "test_df.loc[inx_test,'sc_ba_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bathrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bathrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bathrooms0'] = 0\n",
    "test_df.loc[inx_test,'bathrooms0'] = 0\n",
    "\n",
    "# price per bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_be_price'] = train_df.loc[non0_inx_train,'sc_price'] \\\n",
    "                                                /train_df.loc[non0_inx_train,'bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_be_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_be_price'] = 0\n",
    "test_df.loc[inx_test,'sc_be_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bedrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bedrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bedrooms0'] = 0\n",
    "test_df.loc[inx_test,'bedrooms0'] = 0\n",
    "# bathrooms\n",
    "\n",
    "ulimit = 5\n",
    "\n",
    "train_df['sc_bathrooms']=train_df['bathrooms']\n",
    "test_df['sc_bathrooms']=test_df['bathrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "\n",
    "# bedrooms\n",
    "\n",
    "ulimit = 8\n",
    "\n",
    "train_df['sc_bedrooms']=train_df['bedrooms']\n",
    "test_df['sc_bedrooms']=test_df['bedrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['sc_bedrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit,['sc_bedrooms']] = ulimit\n",
    "\n",
    "# longitude\n",
    "\n",
    "tmp = pd.concat([train_df['longitude'],test_df['longitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_longitude']=train_df['longitude']\n",
    "test_df['sc_longitude']=test_df['longitude']\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "\n",
    "# latitude\n",
    "\n",
    "tmp = pd.concat([train_df['latitude'],test_df['latitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_latitude']=train_df['latitude']\n",
    "test_df['sc_latitude']=test_df['latitude']\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "\n",
    "\n",
    "features_to_use  = [\"sc_bathrooms\", \"sc_bedrooms\", \"sc_latitude\", \"sc_longitude\",\n",
    "                    \"sc_price\", \"sc_ba_price\", \"sc_be_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \n",
    "                        \"created_day\", \"created_hour\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full_data=pd.concat([train_df,test_df])\n",
    "\n",
    "# # SSL = preprocessing.StandardScaler()\n",
    "# # for col in features_to_use:\n",
    "# #     full_data[col], lam = boxcox(full_data[col] - full_data[col].min() + 1)\n",
    "# #     full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1)) \n",
    "# skewed_cols = full_data[features_to_use].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "# SSL = preprocessing.StandardScaler()\n",
    "# skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "# for skewed_col in skewed_cols:\n",
    "#     full_data[skewed_col], lam = boxcox(full_data[skewed_col] - full_data[skewed_col].min() + 1)\n",
    "#     print skewed_col, '\\t', lam\n",
    "# for col in features_to_use:\n",
    "#     full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1))\n",
    "#     train_df[col] = full_data.iloc[:ntrain][col]\n",
    "#     test_df[col] = full_data.iloc[ntrain:][col]\n",
    "\n",
    "    \n",
    "# del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend([\"listing_id\",\"bedrooms0\",'bathrooms0'])\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    doorman elevator fitness_center cats_allowed d...\n",
      "2    laundry_in_building dishwasher hardwood_floors...\n",
      "3                               hardwood_floors no_fee\n",
      "4                                              pre-war\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "test_df['features'] = test_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "sparse_features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 20)\n",
      "(74659, 20)\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.concat([train_df[features_to_use],test_df[features_to_use]])\n",
    "full_data = preprocessing.StandardScaler().fit_transform(full_data)\n",
    "train_df_nn = full_data[:ntrain]\n",
    "test_df_nn = full_data[ntrain:]\n",
    "\n",
    "print train_df_nn.shape\n",
    "print test_df_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 220) (74659, 220)\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df_nn, tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df_nn, te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "weight_num_map = {'high':1, 'medium':1, 'low':1}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "train_y = to_categorical(train_y)\n",
    "W_train = np.array(train_df['interest_level'].apply(lambda x: weight_num_map[x]))\n",
    "\n",
    "all_features = features_to_use + sparse_features\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 1.0909Epoch 00000: val_loss improved from inf to 0.68854, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 1.0903 - val_loss: 0.6885\n",
      "Epoch 2/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.7534Epoch 00001: val_loss improved from 0.68854 to 0.66213, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.7532 - val_loss: 0.6621\n",
      "Epoch 3/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.7027Epoch 00002: val_loss improved from 0.66213 to 0.65928, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.7029 - val_loss: 0.6593\n",
      "Epoch 4/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6854Epoch 00003: val_loss improved from 0.65928 to 0.65413, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6854 - val_loss: 0.6541\n",
      "Epoch 5/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6783Epoch 00004: val_loss improved from 0.65413 to 0.64905, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6781 - val_loss: 0.6491\n",
      "Epoch 6/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6747Epoch 00005: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6746 - val_loss: 0.6491\n",
      "Epoch 7/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6673Epoch 00006: val_loss improved from 0.64905 to 0.64714, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6673 - val_loss: 0.6471\n",
      "Epoch 8/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6691Epoch 00007: val_loss improved from 0.64714 to 0.64531, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6690 - val_loss: 0.6453\n",
      "Epoch 9/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6669Epoch 00008: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6671 - val_loss: 0.6454\n",
      "Epoch 10/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6614Epoch 00009: val_loss improved from 0.64531 to 0.63739, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6613 - val_loss: 0.6374\n",
      "Epoch 11/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6633Epoch 00010: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6633 - val_loss: 0.6383\n",
      "Epoch 12/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6571Epoch 00011: val_loss improved from 0.63739 to 0.63124, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6570 - val_loss: 0.6312\n",
      "Epoch 13/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6538Epoch 00012: val_loss improved from 0.63124 to 0.62965, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6537 - val_loss: 0.6297\n",
      "Epoch 14/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6533Epoch 00013: val_loss improved from 0.62965 to 0.62220, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6535 - val_loss: 0.6222\n",
      "Epoch 15/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6492Epoch 00014: val_loss improved from 0.62220 to 0.61745, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6493 - val_loss: 0.6175\n",
      "Epoch 16/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6472Epoch 00015: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6473 - val_loss: 0.6178\n",
      "Epoch 17/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6449Epoch 00016: val_loss improved from 0.61745 to 0.61484, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6450 - val_loss: 0.6148\n",
      "Epoch 18/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6435Epoch 00017: val_loss improved from 0.61484 to 0.61387, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6435 - val_loss: 0.6139\n",
      "Epoch 19/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6404Epoch 00018: val_loss improved from 0.61387 to 0.61140, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6405 - val_loss: 0.6114\n",
      "Epoch 20/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6390Epoch 00019: val_loss improved from 0.61140 to 0.60691, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6387 - val_loss: 0.6069\n",
      "Epoch 21/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6384Epoch 00020: val_loss improved from 0.60691 to 0.60518, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6384 - val_loss: 0.6052\n",
      "Epoch 22/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6321Epoch 00021: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6321 - val_loss: 0.6067\n",
      "Epoch 23/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6353Epoch 00022: val_loss improved from 0.60518 to 0.60384, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6354 - val_loss: 0.6038\n",
      "Epoch 24/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6330Epoch 00023: val_loss improved from 0.60384 to 0.60166, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6327 - val_loss: 0.6017\n",
      "Epoch 25/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6330Epoch 00024: val_loss improved from 0.60166 to 0.60079, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6329 - val_loss: 0.6008\n",
      "Epoch 26/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6288Epoch 00025: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6289 - val_loss: 0.6021\n",
      "Epoch 27/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6274Epoch 00026: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6274 - val_loss: 0.6022\n",
      "Epoch 28/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6276Epoch 00027: val_loss improved from 0.60079 to 0.59924, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6276 - val_loss: 0.5992\n",
      "Epoch 29/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6271Epoch 00028: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6270 - val_loss: 0.6000\n",
      "Epoch 30/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6244Epoch 00029: val_loss improved from 0.59924 to 0.59887, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6243 - val_loss: 0.5989\n",
      "Epoch 31/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6245Epoch 00030: val_loss improved from 0.59887 to 0.59474, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6245 - val_loss: 0.5947\n",
      "Epoch 32/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6218Epoch 00031: val_loss did not improve\n",
      "49408/49352 [==============================] - 7s - loss: 0.6218 - val_loss: 0.5961\n",
      "Epoch 33/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6244Epoch 00032: val_loss improved from 0.59474 to 0.59462, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6242 - val_loss: 0.5946\n",
      "Epoch 34/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6238Epoch 00033: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6237 - val_loss: 0.5951\n",
      "Epoch 35/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6164Epoch 00034: val_loss improved from 0.59462 to 0.59395, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6166 - val_loss: 0.5939\n",
      "Epoch 36/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6192Epoch 00035: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6191 - val_loss: 0.5944\n",
      "Epoch 37/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6213Epoch 00036: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6212 - val_loss: 0.5941\n",
      "Epoch 38/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6128Epoch 00037: val_loss improved from 0.59395 to 0.59170, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6130 - val_loss: 0.5917\n",
      "Epoch 39/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6170Epoch 00038: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6172 - val_loss: 0.5936\n",
      "Epoch 40/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6154Epoch 00039: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6153 - val_loss: 0.5919\n",
      "Epoch 41/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6163Epoch 00040: val_loss improved from 0.59170 to 0.59016, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6164 - val_loss: 0.5902\n",
      "Epoch 42/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6139Epoch 00041: val_loss improved from 0.59016 to 0.59002, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6138 - val_loss: 0.5900\n",
      "Epoch 43/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6161Epoch 00042: val_loss improved from 0.59002 to 0.58987, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6162 - val_loss: 0.5899\n",
      "Epoch 44/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6175Epoch 00043: val_loss improved from 0.58987 to 0.58759, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 7s - loss: 0.6173 - val_loss: 0.5876\n",
      "Epoch 45/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6152Epoch 00044: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6151 - val_loss: 0.5906\n",
      "Epoch 46/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6120Epoch 00045: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6119 - val_loss: 0.5899\n",
      "Epoch 47/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6123Epoch 00046: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6123 - val_loss: 0.5911\n",
      "Epoch 48/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6082Epoch 00047: val_loss improved from 0.58759 to 0.58675, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6083 - val_loss: 0.5868\n",
      "Epoch 49/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6116Epoch 00048: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6116 - val_loss: 0.5873\n",
      "Epoch 50/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6143Epoch 00049: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6145 - val_loss: 0.5885\n",
      "Epoch 51/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6082Epoch 00050: val_loss improved from 0.58675 to 0.58604, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6083 - val_loss: 0.5860\n",
      "Epoch 52/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6094Epoch 00051: val_loss improved from 0.58604 to 0.58535, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6098 - val_loss: 0.5853\n",
      "Epoch 53/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6095Epoch 00052: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6095 - val_loss: 0.5868\n",
      "Epoch 54/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6089Epoch 00053: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6091 - val_loss: 0.5867\n",
      "Epoch 55/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6077Epoch 00054: val_loss improved from 0.58535 to 0.58534, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6077 - val_loss: 0.5853\n",
      "Epoch 56/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6043Epoch 00055: val_loss improved from 0.58534 to 0.58406, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6045 - val_loss: 0.5841\n",
      "Epoch 57/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6076Epoch 00056: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6074 - val_loss: 0.5858\n",
      "Epoch 58/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6063Epoch 00057: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6068 - val_loss: 0.5845\n",
      "Epoch 59/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6038Epoch 00058: val_loss improved from 0.58406 to 0.58330, saving model to weights.hdf5\n",
      "49408/49352 [==============================] - 6s - loss: 0.6037 - val_loss: 0.5833\n",
      "Epoch 60/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6047Epoch 00059: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6048 - val_loss: 0.5849\n",
      "Epoch 61/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6079Epoch 00060: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6080 - val_loss: 0.5839\n",
      "Epoch 62/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6021Epoch 00061: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6022 - val_loss: 0.5834\n",
      "Epoch 63/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6048Epoch 00062: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6049 - val_loss: 0.5844\n",
      "Epoch 64/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6066Epoch 00063: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6066 - val_loss: 0.5840\n",
      "Epoch 65/1000\n",
      "49280/49352 [============================>.] - ETA: 0s - loss: 0.6045Epoch 00064: val_loss did not improve\n",
      "49408/49352 [==============================] - 6s - loss: 0.6047 - val_loss: 0.5840\n",
      "0.5832976706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    init = 'he_normal'\n",
    "    \n",
    "    \n",
    "    model.add(Dense(150, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim, # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(50,init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                   init = init,\n",
    "                   activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "#                   metrics=[mae_log],\n",
    "                  optimizer = 'adam' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=ntrain,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )\n",
    "\n",
    "print min(fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.hdf5\")\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict_proba(x=test_X.toarray(),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_Keras_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(pred_y)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    init = 'he_normal'\n",
    "    \n",
    "    model.add(Dense(params['input_size'], # number of input units: needs to be tuned\n",
    "                    input_dim = params['input_dim'], # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(params['input_drop_out'])) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(params['hidden_size'],\n",
    "                    init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out'])) #dropout rate. needs to be tuned\n",
    "    \n",
    "#     model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(BatchNormalization())    \n",
    "#     model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                    init = init,\n",
    "                    activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128):\n",
    "    N_params = len(parameters)\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = train_y.shape[1]\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "\n",
    "    \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "        print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "\n",
    "            model = nn_model(nn_params)\n",
    "#             print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, 128, True),\n",
    "                                     nb_epoch=60,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     verbose = 0,\n",
    "                                     callbacks=[ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                                                                monitor='val_loss', \n",
    "                                                                verbose=0, save_best_only=True)]\n",
    "                                    )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'adam' )\n",
    "            \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_proba(x=val_x_fold.toarray(),verbose=0)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'adam' )            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = model.predict_proba(x=test_X.toarray(),verbose=0)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 2 estimators for 4 folds\n",
      "Model 1: {'input_size': 300, 'input_drop_out': 0.4, 'hidden_drop_out': 0.2, 'hidden_size': 100, 'input_dim': 220}\n",
      "Model 1 fold 1\n",
      "best round 54\n",
      "('Score: ', 0.58930159188570641)\n",
      "Model 1 fold 1 fitting finished in 457.649s\n",
      "Model 1 fold 2\n",
      "best round 54\n",
      "('Score: ', 0.58212658337893708)\n",
      "Model 1 fold 2 fitting finished in 458.268s\n",
      "Model 1 fold 3\n",
      "best round 54\n",
      "('Score: ', 0.58625029173067966)\n",
      "Model 1 fold 3 fitting finished in 458.267s\n",
      "Model 1 fold 4\n",
      "best round 54\n",
      "('Score: ', 0.58740975337936085)\n",
      "Model 1 fold 4 fitting finished in 462.457s\n",
      "Score for model 1 is 0.586272\n",
      "Model 2: {'input_size': 200, 'input_drop_out': 0.4, 'hidden_drop_out': 0.2, 'hidden_size': 50, 'input_dim': 220}\n",
      "Model 2 fold 1\n",
      "best round 54\n",
      "('Score: ', 0.5915979231118349)\n",
      "Model 2 fold 1 fitting finished in 313.168s\n",
      "Model 2 fold 2\n",
      "best round 54\n",
      "('Score: ', 0.58284341458860967)\n",
      "Model 2 fold 2 fitting finished in 312.474s\n",
      "Model 2 fold 3\n",
      "best round 54\n",
      "('Score: ', 0.58856996830310526)\n",
      "Model 2 fold 3 fitting finished in 312.810s\n",
      "Model 2 fold 4\n",
      "best round 54\n",
      "('Score: ', 0.58927660583302155)\n",
      "Model 2 fold 4 fitting finished in 313.689s\n",
      "Score for model 2 is 0.588072\n",
      "Score for blended models is 0.587172\n"
     ]
    }
   ],
   "source": [
    "nn_parameters = [\n",
    "    { 'input_size' :300 ,\n",
    "     'input_dim' : train_X.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 100 ,\n",
    "     'hidden_drop_out' :0.2},\n",
    "    { 'input_size' :200 ,\n",
    "     'input_dim' : train_X.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 50 ,\n",
    "     'hidden_drop_out' :0.2}\n",
    "\n",
    "]\n",
    "\n",
    "(train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_X, train_y, test_X,\n",
    "                                                         4,\n",
    "                                                         5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03418285,  0.29842315,  0.66739401],\n",
       "       [ 0.08886517,  0.2184982 ,  0.69263662],\n",
       "       [ 0.01659429,  0.15124959,  0.83215612],\n",
       "       ..., \n",
       "       [ 0.02705079,  0.2883161 ,  0.68463309],\n",
       "       [ 0.58800882,  0.35606831,  0.05592286],\n",
       "       [ 0.00831845,  0.11601497,  0.87566657]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x[:,3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x[:,3:6])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54336879  0.54314207  0.54391966  0.54451063  0.54393703]\n",
      "4863.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_gbm,axis=0))\n",
    "print (np.mean(best_rounds_gbm,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_gbm, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_gbm, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "labels2idx ={'high': 0, 'low': 2, 'medium': 1}\n",
    "sub_name = '../output/sub_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub[\"listing_id\"] = df[\"listing_id\"]\n",
    "\n",
    "y_test = np.zeros((df.shape[0], 3))\n",
    "\n",
    "for N in range(3):\n",
    "    y_test[:,N] = pd.DataFrame(test_blend_x_gbm).iloc[:,[x for x in range(test_blend_x_gbm.shape[1]) if x%3 == N]].mean(axis=1)\n",
    "    \n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    sub[label] = y_test[:, labels2idx[label]]\n",
    "sub.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "sub_name = '../output/sub_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "tmp2[\"listing_id\"] = df[\"listing_id\"].values\n",
    "# tmp1.columns = ['0','1','2']\n",
    "# for label in [\"high\", \"medium\", \"low\"]:\n",
    "#     sub[label] = tmp2.iloc[:,label].values\n",
    "tmp2.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
