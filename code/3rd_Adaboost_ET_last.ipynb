{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from scipy import sparse\n",
    "from scipy.stats.mstats import gmean\n",
    "from datetime import datetime\n",
    "# from sklearn import preprocessing\n",
    "# from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = np.ravel(pd.read_csv('../input/' + 'labels_BrandenMurray.csv'))\n",
    "\n",
    "names = ['low_0','medium_0','high_0',\n",
    "        'low_1','medium_1','high_1',\n",
    "        'low_2','medium_2','high_2',\n",
    "        'low_3','medium_3','high_3',\n",
    "        'low_4','medium_4','high_4',\n",
    "        'low_5','medium_5','high_5',\n",
    "        'low_6','medium_6','high_6',\n",
    "        'low_7','medium_7','high_7',\n",
    "        'low_8','medium_8','high_8',\n",
    "        'low_9','medium_9','high_9']\n",
    "\n",
    "data_path = \"../2ndlast/\"\n",
    "total_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.355361      0.544070    0.100569\n",
      "1   0.508303      0.446720    0.044978\n",
      "2   0.603091      0.349880    0.047029\n",
      "3   0.616221      0.328639    0.055140\n",
      "4   0.947230      0.049863    0.002907\n",
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.288217      0.532268    0.179515\n",
      "1   0.970891      0.025801    0.003308\n",
      "2   0.908912      0.078535    0.012553\n",
      "3   0.400539      0.476918    0.122542\n",
      "4   0.700470      0.269945    0.029586\n"
     ]
    }
   ],
   "source": [
    "# RFC 1st level \n",
    "file_train      = 'train_blend_RFC_entropy_last_2017-04-21-11-06' + '.csv'\n",
    "file_test_mean  = 'test_blend_RFC_entropy_mean_last_2017-04-21-11-06' + '.csv'\n",
    "\n",
    "train_rfc = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_rfc  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_rfc.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_rfc.columns = ['rfc_' + x for x in names[:n_column]]\n",
    "test_rfc.columns  = ['rfc_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_rfc.iloc[:5,:3]\n",
    "\n",
    "print test_rfc.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.259359     0.652577   0.088065\n",
      "1  0.738646     0.234238   0.027115\n",
      "2  0.396965     0.512991   0.090045\n",
      "3  0.647052     0.312537   0.040412\n",
      "4  0.923203     0.072255   0.004543\n",
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.282172     0.549423   0.168405\n",
      "1  0.955532     0.040091   0.004377\n",
      "2  0.925692     0.065197   0.009112\n",
      "3  0.631038     0.309690   0.059272\n",
      "4  0.803117     0.187725   0.009158\n"
     ]
    }
   ],
   "source": [
    "# LR 1st level\n",
    "file_train      = 'train_blend_LR_last_2017-04-21-11-16' + '.csv'\n",
    "file_test_mean  = 'test_blend_LR_mean_last_2017-04-21-11-16' + '.csv'\n",
    "\n",
    "train_LR = pd.read_csv(data_path + file_train, header = None)\n",
    "test_LR  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_LR.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_LR.columns = ['LR_' + x for x in names[:n_column]]\n",
    "test_LR.columns  = ['LR_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_LR.iloc[:5,:3]\n",
    "print test_LR.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.332903     0.538085   0.129012\n",
      "1  0.471780     0.454812   0.073408\n",
      "2  0.582223     0.383893   0.033884\n",
      "3  0.622462     0.328557   0.048981\n",
      "4  0.926402     0.064996   0.008602\n",
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.309822     0.527181   0.162997\n",
      "1  0.984336     0.014059   0.001605\n",
      "2  0.956579     0.038080   0.005341\n",
      "3  0.518490     0.384524   0.096986\n",
      "4  0.759357     0.210049   0.030594\n"
     ]
    }
   ],
   "source": [
    "# ET 1st level\n",
    "file_train      = 'train_blend_ET_entropy_last_2017-04-21-11-48' + '.csv'\n",
    "file_test_mean  = 'test_blend_ET_entropy_mean_last_2017-04-21-11-48' + '.csv'\n",
    "\n",
    "train_ET = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_ET  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_ET.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ET.columns = ['ET_' + x for x in names[:n_column]]\n",
    "test_ET.columns  = ['ET_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ET.iloc[:5,:3]\n",
    "print test_ET.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.507812              0.390625            0.101562\n",
      "1           0.531250              0.359375            0.109375\n",
      "2           0.671875              0.273438            0.054688\n",
      "3           0.609375              0.250000            0.140625\n",
      "4           0.843750              0.140625            0.015625\n",
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.381250              0.457813            0.160938\n",
      "1           0.968750              0.031250            0.000000\n",
      "2           0.970313              0.029687            0.000000\n",
      "3           0.693750              0.259375            0.046875\n",
      "4           0.612500              0.321875            0.065625\n"
     ]
    }
   ],
   "source": [
    "# KNN 1st level\n",
    "file_train      = 'train_blend_KNN_uniform_last_2017-04-21-13-53' + '.csv'\n",
    "file_test_mean  = 'test_blend_KNN_uniform_mean_last_2017-04-21-13-53' + '.csv'\n",
    "\n",
    "\n",
    "train_KNN = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_KNN  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_KNN.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_KNN.columns      = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "test_KNN.columns  = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_KNN.iloc[:5,:3]\n",
    "print test_KNN.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.460187          0.436036        0.103776\n",
      "1       0.268598          0.571916        0.159486\n",
      "2       0.724799          0.239351        0.035851\n",
      "3       0.669683          0.286716        0.043600\n",
      "4       0.917878          0.073469        0.008653\n",
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.449136          0.411890        0.138974\n",
      "1       0.971615          0.020319        0.008066\n",
      "2       0.909864          0.074040        0.016096\n",
      "3       0.661175          0.269026        0.069799\n",
      "4       0.705851          0.263069        0.031080\n"
     ]
    }
   ],
   "source": [
    "# TFFM 1st level 0322\n",
    "file_train      = 'train_blend_FM_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "file_test_mean  = 'test_blend_FM_mean_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "\n",
    "train_FM_0322      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_FM_mean_0322  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_FM_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_FM_0322.columns      = ['FM_0322_' + x for x in names[:n_column]]\n",
    "test_FM_mean_0322.columns  = ['FM_0322_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_FM_0322.iloc[:5,:3]\n",
    "print test_FM_mean_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.216985      0.579031    0.203985\n",
      "1   0.560375      0.382293    0.057331\n",
      "2   0.621019      0.332482    0.046499\n",
      "3   0.312441      0.345115    0.342444\n",
      "4   0.901678      0.090035    0.008287\n",
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.218456      0.591575    0.189969\n",
      "1   0.992103      0.007102    0.000795\n",
      "2   0.971220      0.025168    0.003612\n",
      "3   0.501988      0.411198    0.086813\n",
      "4   0.754496      0.213158    0.032347\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes 1st level\n",
    "file_train      = 'train_blend_MNB_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "file_test_mean  = 'test_blend_MNB_mean_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "\n",
    "\n",
    "train_MNB      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_MNB_mean  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_MNB.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_MNB.columns      = ['MNB_' + x for x in names[:n_column]]\n",
    "test_MNB_mean.columns  = ['MNB_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_MNB.iloc[:5,:3]\n",
    "print test_MNB_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tsne_0     tsne_1    tsne_2\n",
      "0  -8.398991  -2.415894 -3.602143\n",
      "1   0.698237   0.335786  8.884257\n",
      "2  -5.811380 -16.669975  7.145837\n",
      "3  -0.371861 -25.894747 -2.076309\n",
      "4 -15.371799   9.656209  5.813590\n",
      "      tsne_0     tsne_1     tsne_2\n",
      "0  -5.176846  -0.768422  -2.339259\n",
      "1   9.003089  13.250301  -0.707032\n",
      "2   4.188036  14.397186   4.573307\n",
      "3  10.890132 -12.660774 -13.414140\n",
      "4   6.011381   5.177731  15.669250\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level\n",
    "\n",
    "file_train = 'X_train_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "\n",
    "train_tsne = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "\n",
    "n_column = train_tsne.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne.columns = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "test_tsne.columns  = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "\n",
    "\n",
    "print train_tsne.iloc[:5,:3]\n",
    "print test_tsne.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -6.649132    13.028168     8.329733\n",
      "1     7.615566     0.067456   -14.932181\n",
      "2     8.333528     8.561174   -13.536297\n",
      "3    12.819587   -20.027314     0.661660\n",
      "4    -5.513088    -5.609218    17.130673\n",
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -5.721674     7.011411    -6.499047\n",
      "1     8.238390    -8.589710    13.771045\n",
      "2   -11.383577   -16.071395    15.083511\n",
      "3    -6.111491     6.348311   -10.222012\n",
      "4     4.426022    15.553415    11.315777\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level 0322\n",
    "\n",
    "file_train = 'X_train_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "\n",
    "train_tsne_0322 = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne_0322  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "n_column = train_tsne_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne_0322.columns = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "test_tsne_0322.columns  = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "\n",
    "print train_tsne_0322.iloc[:5,:3]\n",
    "print test_tsne_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.338520      0.631375    0.030105\n",
      "1   0.563156      0.390819    0.046025\n",
      "2   0.462242      0.498429    0.039328\n",
      "3   0.930174      0.067369    0.002458\n",
      "4   0.892201      0.106567    0.001232\n",
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.178073      0.621011    0.200916\n",
      "1   0.978270      0.012231    0.009499\n",
      "2   0.939539      0.055440    0.005021\n",
      "3   0.151284      0.616193    0.232523\n",
      "4   0.698756      0.292996    0.008247\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb*2\n",
    "tmp_test  = test_xgb_mean*2\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb      = (tmp_train + train_xgb*20) / 22.0\n",
    "test_xgb_mean  = (tmp_test + test_xgb_mean*20) / 22.0\n",
    "\n",
    "n_column = train_xgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "test_xgb_mean.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb.iloc[:5,:3]\n",
    "print test_xgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.309433             0.660416           0.030151\n",
      "1          0.565861             0.382300           0.051838\n",
      "2          0.401785             0.566253           0.031961\n",
      "3          0.931191             0.066339           0.002470\n",
      "4          0.881444             0.117002           0.001555\n",
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.174706             0.640575           0.184719\n",
      "1          0.977405             0.013037           0.009558\n",
      "2          0.944000             0.051268           0.004732\n",
      "3          0.145425             0.592400           0.262175\n",
      "4          0.670348             0.322061           0.007591\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "file_test_mean  = 'test_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_30fold      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_30fold.columns      = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_30fold.columns  = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_30fold.iloc[:5,:3]\n",
    "print test_xgb_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.313141          0.658802        0.028057\n",
      "1       0.491897          0.450891        0.057212\n",
      "2       0.486618          0.486574        0.026809\n",
      "3       0.925839          0.071295        0.002866\n",
      "4       0.863436          0.134349        0.002214\n",
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.167358          0.646104        0.186538\n",
      "1       0.981203          0.010042        0.008756\n",
      "2       0.906588          0.088855        0.004558\n",
      "3       0.167107          0.602472        0.230421\n",
      "4       0.716008          0.274903        0.009089\n"
     ]
    }
   ],
   "source": [
    "# XGB one vs rest 1st level\n",
    "\n",
    "file_train      = 'train_blend_xgb_ovr_last_2017-04-21-10-09' + '.csv'\n",
    "file_test_mean  = 'test_blend_xgb_ovr_mean_last_2017-04-21-10-09' + '.csv'\n",
    "\n",
    "train_xgb_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_ovr.columns      = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_ovr.columns  = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_xgb_ovr,axis=1)\n",
    "sum_test  = np.sum(test_xgb_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_xgb_ovr.columns.values:\n",
    "    train_xgb_ovr[col] = train_xgb_ovr[col] / sum_train\n",
    "    test_xgb_mean_ovr[col] = test_xgb_mean_ovr[col] / sum_test\n",
    "\n",
    "\n",
    "print train_xgb_ovr.iloc[:5,:3]\n",
    "print test_xgb_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.317617            0.650112          0.032271\n",
      "1         0.597390            0.386899          0.015711\n",
      "2         0.450143            0.517132          0.032726\n",
      "3         0.898037            0.100391          0.001572\n",
      "4         0.880435            0.118283          0.001282\n",
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.186506            0.567313          0.246180\n",
      "1         0.964578            0.025110          0.010312\n",
      "2         0.913688            0.080543          0.005769\n",
      "3         0.114846            0.657136          0.228019\n",
      "4         0.650263            0.344835          0.004902\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 1st level\n",
    "\n",
    "file_train      = 'train_blend_LightGBM_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_LightGBM_mean_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "\n",
    "\n",
    "train_lgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_lgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_lgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_lgb.columns      = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "test_lgb_mean.columns  = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_lgb.iloc[:5,:3]\n",
    "print test_lgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.335217     0.604643   0.060140\n",
      "1  0.772956     0.207974   0.019070\n",
      "2  0.529658     0.446699   0.023643\n",
      "3  0.956680     0.042453   0.000867\n",
      "4  0.938238     0.060709   0.001053\n",
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.276664     0.582024   0.141312\n",
      "1  0.995422     0.004223   0.000356\n",
      "2  0.979827     0.019278   0.000895\n",
      "3  0.362736     0.446379   0.190885\n",
      "4  0.729782     0.259918   0.010300\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level No.1\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-21-23' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-21-23' + '.csv'\n",
    "\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "test_nn_mean = tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-22-05' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-22-05' + '.csv'\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = (train_nn + tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "test_nn_mean = (test_nn_mean + tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn.columns      = ['nn_' + x for x in names[:n_column]]\n",
    "test_nn_mean.columns  = ['nn_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn.iloc[:5,:3]\n",
    "print test_nn_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.378595            0.564656          0.056749\n",
      "1         0.763784            0.215613          0.020603\n",
      "2         0.577165            0.398481          0.024354\n",
      "3         0.949192            0.049937          0.000871\n",
      "4         0.958236            0.040963          0.000801\n",
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.286080            0.567262          0.146658\n",
      "1         0.996091            0.003613          0.000296\n",
      "2         0.986770            0.012566          0.000664\n",
      "3         0.370175            0.443003          0.186822\n",
      "4         0.738394            0.250685          0.010921\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "\n",
    "train_nn_30fold     = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_30fold.columns      = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "test_nn_mean_30fold.columns  = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_30fold.iloc[:5,:3]\n",
    "print test_nn_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.414059         0.533141       0.052800\n",
      "1      0.752638         0.210038       0.037324\n",
      "2      0.515976         0.440243       0.043781\n",
      "3      0.780622         0.219167       0.000211\n",
      "4      0.965501         0.032244       0.002255\n",
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.227279         0.679449       0.093272\n",
      "1      0.976299         0.014297       0.009403\n",
      "2      0.919964         0.066669       0.013367\n",
      "3      0.266264         0.389498       0.344238\n",
      "4      0.874583         0.119637       0.005780\n"
     ]
    }
   ],
   "source": [
    "# Keras one vs rest 1st level\n",
    "file_train      = 'train_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "\n",
    "train_nn_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_ovr.columns      = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "test_nn_mean_ovr.columns  = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_nn_ovr,axis=1)\n",
    "sum_test  = np.sum(test_nn_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_nn_ovr.columns.values:\n",
    "    train_nn_ovr[col] = train_nn_ovr[col] / sum_train\n",
    "    test_nn_mean_ovr[col] = test_nn_mean_ovr[col] / sum_test \n",
    "\n",
    "print train_nn_ovr.iloc[:5,:3]\n",
    "print test_nn_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.445882            0.488624          0.065494\n",
      "1         0.761473            0.222332          0.016195\n",
      "2         0.542404            0.415798          0.041798\n",
      "3         0.956819            0.041689          0.001492\n",
      "4         0.950208            0.048009          0.001783\n",
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.300305            0.540791          0.158904\n",
      "1         0.995423            0.004364          0.000213\n",
      "2         0.984619            0.014709          0.000672\n",
      "3         0.376123            0.446293          0.177584\n",
      "4         0.713087            0.267086          0.019827\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 3layer 20 bagging\n",
    "file_train      = 'train_blend_Keras_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "\n",
    "train_nn_3layer      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_3layer  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_3layer.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_3layer.columns      = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "test_nn_mean_3layer.columns  = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_nn_3layer.iloc[:5,:3]\n",
    "print test_nn_mean_3layer.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../3rdlast/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_2nd_low_0  nn_2nd_medium_0  nn_2nd_high_0\n",
      "0      0.357091         0.585324       0.057586\n",
      "1      0.554172         0.404744       0.041084\n",
      "2      0.410131         0.546008       0.043861\n",
      "3      0.890533         0.106213       0.003254\n",
      "4      0.951064         0.047675       0.001261\n",
      "   nn_2nd_low_0  nn_2nd_medium_0  nn_2nd_high_0\n",
      "0      0.162490         0.584023       0.253486\n",
      "1      0.995381         0.004230       0.000389\n",
      "2      0.972846         0.025891       0.001264\n",
      "3      0.108858         0.501719       0.389423\n",
      "4      0.733884         0.257278       0.008839\n"
     ]
    }
   ],
   "source": [
    "# Keras 2nd level \n",
    "\n",
    "file_train      = 'train_blend_2ndKeras_100bagging_2017-04-22-18-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_2ndKeras_100bagging_2017-04-22-18-54' + '.csv'\n",
    "\n",
    "\n",
    "train_nn_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_2nd.columns      = ['nn_2nd_' + x for x in names[:n_column]]\n",
    "test_nn_mean_2nd.columns  = ['nn_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_2nd.iloc[:5,:3]\n",
    "print test_nn_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_2nd_low_0  xgb_2nd_medium_0  xgb_2nd_high_0\n",
      "0       0.315073          0.651911        0.033015\n",
      "1       0.583858          0.368919        0.047223\n",
      "2       0.446948          0.505974        0.047079\n",
      "3       0.907728          0.087888        0.004384\n",
      "4       0.912005          0.085264        0.002731\n",
      "   xgb_2nd_low_0  xgb_2nd_medium_0  xgb_2nd_high_0\n",
      "0       0.171173          0.618564        0.210263\n",
      "1       0.994381          0.004820        0.000799\n",
      "2       0.944171          0.053315        0.002514\n",
      "3       0.133564          0.550379        0.316057\n",
      "4       0.750419          0.240804        0.008776\n"
     ]
    }
   ],
   "source": [
    "# XGB 2nd level\n",
    "\n",
    "file_train     = 'train_blend_2ndXGB_BM_100bagging_2017-04-22-07-18' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndXGB_BM_100bagging_2017-04-22-07-18' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb_2nd\n",
    "tmp_test  = test_xgb_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndXGB_BM_100bagging_1_2017-04-22-14-37' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndXGB_BM_100bagging_1_2017-04-22-14-37' + '.csv'\n",
    "\n",
    "train_xgb_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb_2nd      = (tmp_train + train_xgb_2nd) / 2.0\n",
    "test_xgb_mean_2nd  = (tmp_test + test_xgb_mean_2nd) / 2.0\n",
    "\n",
    "n_column = train_xgb_2nd.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_2nd.columns = ['xgb_2nd_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_2nd.columns = ['xgb_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_2nd.iloc[:5,:3]\n",
    "print test_xgb_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ADET_2nd_low_0  ADET_2nd_medium_0  ADET_2nd_high_0\n",
      "0        0.350422           0.600100         0.049478\n",
      "1        0.457400           0.494753         0.047847\n",
      "2        0.428550           0.521861         0.049589\n",
      "3        0.840500           0.157169         0.002331\n",
      "4        0.932883           0.057958         0.009158\n",
      "   ADET_2nd_low_0  ADET_2nd_medium_0  ADET_2nd_high_0\n",
      "0        0.132605           0.644076         0.223319\n",
      "1        0.982162           0.017716         0.000123\n",
      "2        0.910824           0.088860         0.000316\n",
      "3        0.163267           0.489916         0.346817\n",
      "4        0.723374           0.258492         0.018134\n"
     ]
    }
   ],
   "source": [
    "# ADET 2nd level\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_2017-04-22-04-30' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_2017-04-22-04-30' + '.csv'\n",
    "\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_ADET_2nd\n",
    "tmp_test  = test_ADET_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_1_2017-04-22-10-08' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_1_2017-04-22-10-08' + '.csv'\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = tmp_train + train_ADET_2nd\n",
    "tmp_test  = tmp_test + test_ADET_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_2_2017-04-22-17-43' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_2_2017-04-22-17-43' + '.csv'\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_ADET_2nd      = (tmp_train + train_ADET_2nd) / 3.0\n",
    "test_ADET_mean_2nd  = (tmp_test + test_ADET_mean_2nd) / 3.0\n",
    "\n",
    "n_column = train_ADET_2nd.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ADET_2nd.columns = ['ADET_2nd_' + x for x in names[:n_column]]\n",
    "test_ADET_mean_2nd.columns = ['ADET_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ADET_2nd.iloc[:5,:3]\n",
    "print test_ADET_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "print total_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2nd: (49352, 57)\t test_2nd_mean:(74659, 57)\n"
     ]
    }
   ],
   "source": [
    "train_2nd      = pd.concat([train_rfc, train_LR, train_ET, train_KNN, train_FM_0322,    train_MNB,     train_tsne,\n",
    "                            train_tsne_0322, train_xgb,     train_xgb_30fold,     train_xgb_ovr, \n",
    "                            train_nn,     train_nn_30fold,     train_nn_ovr,     train_nn_3layer,\n",
    "                            train_lgb,\n",
    "                            train_nn_2nd, train_xgb_2nd, train_ADET_2nd\n",
    "                           ], axis = 1)\n",
    "\n",
    "test_2nd_mean  = pd.concat([test_rfc,  test_LR,  test_ET,  test_KNN, test_FM_mean_0322, test_MNB_mean, test_tsne, \n",
    "                            test_tsne_0322,  test_xgb_mean, test_xgb_mean_30fold, test_xgb_mean_ovr,\n",
    "                            test_nn_mean, test_nn_mean_30fold, test_nn_mean_ovr, test_nn_mean_3layer,\n",
    "                            test_lgb_mean,\n",
    "                            test_nn_mean_2nd, test_xgb_mean_2nd, test_ADET_mean_2nd\n",
    "                           ], axis = 1)\n",
    "\n",
    "print 'train_2nd: {}\\t test_2nd_mean:{}'.\\\n",
    "            format(train_2nd.shape,test_2nd_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.48198894 -0.47647544 -0.49132319 -0.50804814 -0.53112909]\n",
      "-0.49779295946\n",
      "Spend:  173.851999998\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "\n",
    "rgr = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=1,\n",
    "                         n_estimators=800)\n",
    "score = cross_val_score(rgr,train_2nd,train_y, scoring = 'neg_log_loss', cv = 5)\n",
    "print score\n",
    "print score.mean()\n",
    "\n",
    "print \"Spend: \", time.time() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1200 0.8 800\n",
    "[-0.48213797 -0.47289378 -0.48640647 -0.50762276 -0.52545789]\n",
    "-0.494903775628\n",
    "Spend:  170.286999941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ET_blend(est, train_x, train_y, test_x, fold,randomseed):\n",
    "    N_params = len(est)\n",
    "#     print \"Blend %d estimators for %d folds\" % (N_params, fold)\n",
    "    skf = KFold(n_splits=fold,shuffle=True,random_state=randomseed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "    fold_start = time.time() \n",
    "    \n",
    "    for j, ester in enumerate(est):\n",
    "#         print \"Model %d:\" %(j+1)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "\n",
    "            \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "#             print \"Model %d fold %d\" %(j+1,i+1)\n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]            \n",
    "            \n",
    "\n",
    "            ester.fit(train_x_fold,train_y_fold)\n",
    "            \n",
    "            val_y_predict_fold = ester.predict_proba(val_x_fold)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "#             print \"Score: \", score\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = ester.predict_proba(test_x)\n",
    "            \n",
    "#             print \"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start)            \n",
    "\n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "\n",
    "            \n",
    "#         print \"Score for model %d is %f\" % (j+1,np.mean(scores[:,j]))\n",
    "    print \"Score for blended models is %f in %0.3fm\" % (np.mean(scores), (time.time() - fold_start)/60)\n",
    "    return (train_blend_x, test_blend_x_mean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Score for blended models is 0.496791 in 3.877m\n",
      "1\n",
      "Score for blended models is 0.497281 in 3.721m\n",
      "2\n",
      "Score for blended models is 0.493352 in 3.705m\n",
      "3\n",
      "Score for blended models is 0.495751 in 3.703m\n",
      "4\n",
      "Score for blended models is 0.496461 in 3.656m\n",
      "5\n",
      "Score for blended models is 0.495192 in 3.720m\n",
      "6\n",
      "Score for blended models is 0.494493 in 3.644m\n",
      "7\n",
      "Score for blended models is 0.496762 in 3.617m\n",
      "8\n",
      "Score for blended models is 0.497342 in 3.717m\n",
      "9\n",
      "Score for blended models is 0.500149 in 3.621m\n",
      "10\n",
      "Score for blended models is 0.493725 in 3.615m\n",
      "11\n",
      "Score for blended models is 0.495960 in 3.708m\n",
      "12\n",
      "Score for blended models is 0.495387 in 3.614m\n",
      "13\n",
      "Score for blended models is 0.495785 in 3.638m\n",
      "14\n",
      "Score for blended models is 0.495555 in 3.677m\n",
      "15\n",
      "Score for blended models is 0.496480 in 3.619m\n",
      "16\n",
      "Score for blended models is 0.494908 in 3.713m\n",
      "17\n",
      "Score for blended models is 0.495324 in 3.606m\n",
      "18\n",
      "Score for blended models is 0.493638 in 3.599m\n",
      "19\n",
      "Score for blended models is 0.495135 in 3.722m\n",
      "20\n",
      "Score for blended models is 0.494266 in 3.582m\n",
      "21\n",
      "Score for blended models is 0.497436 in 3.588m\n",
      "22\n",
      "Score for blended models is 0.495985 in 3.741m\n",
      "23\n",
      "Score for blended models is 0.493823 in 3.588m\n",
      "24\n",
      "Score for blended models is 0.496854 in 3.641m\n",
      "25\n",
      "Score for blended models is 0.494433 in 3.660m\n",
      "26\n",
      "Score for blended models is 0.493579 in 3.589m\n",
      "27\n",
      "Score for blended models is 0.494748 in 3.707m\n",
      "28\n",
      "Score for blended models is 0.496496 in 3.590m\n",
      "29\n",
      "Score for blended models is 0.493788 in 3.579m\n",
      "30\n",
      "Score for blended models is 0.494942 in 3.698m\n",
      "31\n",
      "Score for blended models is 0.497091 in 3.585m\n",
      "32\n",
      "Score for blended models is 0.491111 in 3.570m\n",
      "33\n",
      "Score for blended models is 0.493369 in 3.570m\n",
      "34\n",
      "Score for blended models is 0.493726 in 3.590m\n",
      "35\n",
      "Score for blended models is 0.493675 in 3.630m\n",
      "36\n",
      "Score for blended models is 0.495069 in 3.683m\n",
      "37\n",
      "Score for blended models is 0.495352 in 3.574m\n",
      "38\n",
      "Score for blended models is 0.495243 in 3.695m\n",
      "39\n",
      "Score for blended models is 0.498894 in 3.596m\n",
      "40\n",
      "Score for blended models is 0.494925 in 3.565m\n",
      "41\n",
      "Score for blended models is 0.497563 in 3.700m\n",
      "42\n",
      "Score for blended models is 0.497379 in 3.573m\n",
      "43\n",
      "Score for blended models is 0.495844 in 3.585m\n",
      "44\n",
      "Score for blended models is 0.493871 in 3.709m\n",
      "45\n",
      "Score for blended models is 0.494732 in 3.594m\n",
      "46\n",
      "Score for blended models is 0.494578 in 3.577m\n",
      "47\n",
      "Score for blended models is 0.496832 in 3.576m\n",
      "48\n",
      "Score for blended models is 0.495947 in 3.577m\n",
      "49\n",
      "Score for blended models is 0.494041 in 3.684m\n",
      "50\n",
      "Score for blended models is 0.494237 in 3.606m\n",
      "51\n",
      "Score for blended models is 0.495951 in 3.576m\n",
      "52\n",
      "Score for blended models is 0.494980 in 3.722m\n",
      "53\n",
      "Score for blended models is 0.496138 in 3.575m\n",
      "54\n",
      "Score for blended models is 0.494962 in 3.588m\n",
      "55\n",
      "Score for blended models is 0.494582 in 3.709m\n",
      "56\n",
      "Score for blended models is 0.495677 in 3.568m\n",
      "57\n",
      "Score for blended models is 0.495536 in 3.606m\n",
      "58\n",
      "Score for blended models is 0.496787 in 3.657m\n",
      "59\n",
      "Score for blended models is 0.493801 in 3.574m\n",
      "60\n",
      "Score for blended models is 0.496777 in 3.688m\n",
      "61\n",
      "Score for blended models is 0.495161 in 3.593m\n",
      "62\n",
      "Score for blended models is 0.496994 in 3.579m\n",
      "63\n",
      "Score for blended models is 0.497704 in 3.682m\n",
      "64\n",
      "Score for blended models is 0.495372 in 3.564m\n",
      "65\n",
      "Score for blended models is 0.497380 in 3.587m\n",
      "66\n",
      "Score for blended models is 0.494627 in 3.690m\n",
      "67\n",
      "Score for blended models is 0.495675 in 3.594m\n",
      "68\n",
      "Score for blended models is 0.496618 in 3.624m\n",
      "69\n",
      "Score for blended models is 0.494645 in 3.674m\n",
      "70\n",
      "Score for blended models is 0.497304 in 3.574m\n",
      "71\n",
      "Score for blended models is 0.496062 in 3.695m\n",
      "72\n",
      "Score for blended models is 0.495232 in 3.578m\n",
      "73\n",
      "Score for blended models is 0.494904 in 3.575m\n",
      "74\n",
      "Score for blended models is 0.494887 in 3.720m\n",
      "75\n",
      "Score for blended models is 0.495530 in 3.573m\n",
      "76\n",
      "Score for blended models is 0.496543 in 3.576m\n",
      "77\n",
      "Score for blended models is 0.496756 in 3.697m\n",
      "78\n",
      "Score for blended models is 0.495625 in 3.579m\n",
      "79\n",
      "Score for blended models is 0.494108 in 3.626m\n",
      "80\n",
      "Score for blended models is 0.497866 in 3.634m\n",
      "81\n",
      "Score for blended models is 0.495046 in 3.564m\n",
      "82\n",
      "Score for blended models is 0.494752 in 3.715m\n",
      "83\n",
      "Score for blended models is 0.494397 in 3.579m\n",
      "84\n",
      "Score for blended models is 0.493800 in 3.576m\n",
      "85\n",
      "Score for blended models is 0.495515 in 3.696m\n",
      "86\n",
      "Score for blended models is 0.495074 in 3.577m\n",
      "87\n",
      "Score for blended models is 0.495046 in 3.575m\n",
      "88\n",
      "Score for blended models is 0.496178 in 3.686m\n",
      "89\n",
      "Score for blended models is 0.496765 in 3.575m\n",
      "90\n",
      "Score for blended models is 0.496486 in 3.622m\n",
      "91\n",
      "Score for blended models is 0.495704 in 3.650m\n",
      "92\n",
      "Score for blended models is 0.494452 in 3.579m\n",
      "93\n",
      "Score for blended models is 0.493642 in 3.701m\n",
      "94\n",
      "Score for blended models is 0.495925 in 3.614m\n",
      "95\n",
      "Score for blended models is 0.495745 in 3.571m\n",
      "96\n",
      "Score for blended models is 0.492766 in 3.690m\n",
      "97\n",
      "Score for blended models is 0.494747 in 3.578m\n",
      "98\n",
      "Score for blended models is 0.495906 in 3.566m\n",
      "99\n",
      "Score for blended models is 0.495998 in 3.580m\n",
      "100\n",
      "Score for blended models is 0.494889 in 3.579m\n",
      "101\n",
      "Score for blended models is 0.496224 in 3.578m\n",
      "102\n",
      "Score for blended models is 0.496158 in 3.564m\n",
      "103\n",
      "Score for blended models is 0.495259 in 3.579m\n",
      "104\n",
      "Score for blended models is 0.495709 in 3.679m\n",
      "105\n",
      "Score for blended models is 0.495729 in 3.622m\n",
      "106\n",
      "Score for blended models is 0.496708 in 3.583m\n",
      "107\n",
      "Score for blended models is 0.494422 in 3.700m\n",
      "108\n",
      "Score for blended models is 0.496639 in 3.559m\n",
      "109\n",
      "Score for blended models is 0.495895 in 3.578m\n",
      "110\n",
      "Score for blended models is 0.494005 in 3.689m\n",
      "111\n",
      "Score for blended models is 0.494672 in 3.554m\n",
      "112\n",
      "Score for blended models is 0.494957 in 3.603m\n",
      "113\n",
      "Score for blended models is 0.497886 in 3.645m\n",
      "114\n",
      "Score for blended models is 0.496443 in 3.574m\n",
      "115\n",
      "Score for blended models is 0.496264 in 3.655m\n",
      "116\n",
      "Score for blended models is 0.496572 in 3.601m\n",
      "117\n",
      "Score for blended models is 0.496443 in 3.572m\n",
      "118\n",
      "Score for blended models is 0.494621 in 3.697m\n",
      "119\n",
      "Score for blended models is 0.495969 in 3.591m\n",
      "120\n",
      "Score for blended models is 0.495265 in 3.567m\n",
      "121\n",
      "Score for blended models is 0.494031 in 3.683m\n",
      "122\n",
      "Score for blended models is 0.494613 in 3.606m\n",
      "123\n",
      "Score for blended models is 0.496897 in 3.627m\n",
      "124\n",
      "Score for blended models is 0.497036 in 3.661m\n",
      "125\n",
      "Score for blended models is 0.494760 in 3.603m\n",
      "126\n",
      "Score for blended models is 0.495045 in 3.691m\n",
      "127\n",
      "Score for blended models is 0.493985 in 3.622m\n",
      "128\n",
      "Score for blended models is 0.494452 in 3.620m\n",
      "129\n",
      "Score for blended models is 0.497132 in 3.707m\n",
      "130\n",
      "Score for blended models is 0.496385 in 3.599m\n",
      "131\n",
      "Score for blended models is 0.497493 in 3.596m\n",
      "132\n",
      "Score for blended models is 0.495472 in 3.686m\n",
      "133\n",
      "Score for blended models is 0.497513 in 3.576m\n",
      "134\n",
      "Score for blended models is 0.495707 in 3.621m\n",
      "135\n",
      "Score for blended models is 0.496183 in 3.648m\n",
      "136\n",
      "Score for blended models is 0.497725 in 3.577m\n",
      "137\n",
      "Score for blended models is 0.497989 in 3.686m\n",
      "138\n",
      "Score for blended models is 0.496330 in 3.580m\n",
      "139\n",
      "Score for blended models is 0.495661 in 3.582m\n",
      "140\n",
      "Score for blended models is 0.495574 in 3.687m\n",
      "141\n",
      "Score for blended models is 0.495651 in 3.588m\n",
      "142\n",
      "Score for blended models is 0.494264 in 3.574m\n",
      "143\n",
      "Score for blended models is 0.495374 in 3.689m\n",
      "144\n",
      "Score for blended models is 0.497427 in 3.589m\n",
      "145\n",
      "Score for blended models is 0.496292 in 3.634m\n",
      "146\n",
      "Score for blended models is 0.493764 in 3.636m\n",
      "147\n",
      "Score for blended models is 0.495915 in 3.577m\n",
      "148\n",
      "Score for blended models is 0.494278 in 3.691m\n",
      "149\n",
      "Score for blended models is 0.494932 in 3.587m\n",
      "150\n",
      "Score for blended models is 0.497458 in 3.573m\n",
      "151\n",
      "Score for blended models is 0.496195 in 3.697m\n",
      "152\n",
      "Score for blended models is 0.496182 in 3.577m\n",
      "153\n",
      "Score for blended models is 0.496228 in 3.571m\n",
      "154\n",
      "Score for blended models is 0.495146 in 3.715m\n",
      "155\n",
      "Score for blended models is 0.494920 in 3.580m\n",
      "156\n",
      "Score for blended models is 0.494769 in 3.611m\n",
      "157\n",
      "Score for blended models is 0.495618 in 3.626m\n",
      "158\n",
      "Score for blended models is 0.493334 in 3.568m\n",
      "159\n",
      "Score for blended models is 0.498533 in 3.681m\n",
      "160\n",
      "Score for blended models is 0.495327 in 3.568m\n",
      "161\n",
      "Score for blended models is 0.494682 in 3.556m\n",
      "162\n",
      "Score for blended models is 0.495741 in 3.670m\n",
      "163\n",
      "Score for blended models is 0.493099 in 3.560m\n",
      "164\n",
      "Score for blended models is 0.494260 in 3.552m\n",
      "165\n",
      "Score for blended models is 0.496912 in 3.694m\n",
      "166\n",
      "Score for blended models is 0.495882 in 3.561m\n",
      "167\n",
      "Score for blended models is 0.498076 in 3.587m\n",
      "168\n",
      "Score for blended models is 0.496030 in 3.674m\n",
      "169\n",
      "Score for blended models is 0.496886 in 3.561m\n",
      "170\n",
      "Score for blended models is 0.495291 in 3.662m\n",
      "171\n",
      "Score for blended models is 0.495630 in 3.551m\n",
      "172\n",
      "Score for blended models is 0.492614 in 3.556m\n",
      "173\n",
      "Score for blended models is 0.496470 in 3.565m\n",
      "174\n",
      "Score for blended models is 0.495672 in 3.541m\n",
      "175\n",
      "Score for blended models is 0.494917 in 3.614m\n",
      "176\n",
      "Score for blended models is 0.494530 in 3.695m\n",
      "177\n",
      "Score for blended models is 0.495128 in 3.568m\n",
      "178\n",
      "Score for blended models is 0.494718 in 3.576m\n",
      "179\n",
      "Score for blended models is 0.495667 in 3.676m\n",
      "180\n",
      "Score for blended models is 0.495152 in 3.572m\n",
      "181\n",
      "Score for blended models is 0.495545 in 3.646m\n",
      "182\n",
      "Score for blended models is 0.495591 in 3.617m\n",
      "183\n",
      "Score for blended models is 0.496133 in 3.572m\n",
      "184\n",
      "Score for blended models is 0.497954 in 3.573m\n",
      "185\n",
      "Score for blended models is 0.496925 in 3.555m\n",
      "186\n",
      "Score for blended models is 0.493715 in 3.577m\n",
      "187\n",
      "Score for blended models is 0.494820 in 3.671m\n",
      "188\n",
      "Score for blended models is 0.495871 in 3.570m\n",
      "189\n",
      "Score for blended models is 0.493806 in 3.573m\n",
      "190\n",
      "Score for blended models is 0.494315 in 3.569m\n",
      "191\n",
      "Score for blended models is 0.496155 in 3.563m\n",
      "192\n",
      "Score for blended models is 0.495920 in 3.615m\n",
      "193\n",
      "Score for blended models is 0.494107 in 3.627m\n",
      "194\n",
      "Score for blended models is 0.497366 in 3.556m\n",
      "195\n",
      "Score for blended models is 0.494586 in 3.693m\n",
      "196\n",
      "Score for blended models is 0.494129 in 3.570m\n",
      "197\n",
      "Score for blended models is 0.495255 in 3.573m\n",
      "198\n",
      "Score for blended models is 0.494680 in 3.693m\n",
      "199\n",
      "Score for blended models is 0.494299 in 3.561m\n",
      "200\n",
      "Score for blended models is 0.495633 in 3.563m\n",
      "201\n",
      "Score for blended models is 0.497844 in 3.569m\n",
      "202\n",
      "Score for blended models is 0.496511 in 3.557m\n",
      "203\n",
      "Score for blended models is 0.496524 in 3.561m\n",
      "204\n",
      "Score for blended models is 0.494974 in 3.586m\n",
      "205\n",
      "Score for blended models is 0.496136 in 3.577m\n",
      "206\n",
      "Score for blended models is 0.493878 in 3.565m\n",
      "207\n",
      "Score for blended models is 0.495377 in 3.561m\n",
      "208\n",
      "Score for blended models is 0.494739 in 3.557m\n",
      "209\n",
      "Score for blended models is 0.494941 in 3.711m\n",
      "210\n",
      "Score for blended models is 0.496042 in 3.572m\n",
      "211\n",
      "Score for blended models is 0.495026 in 3.574m\n",
      "212\n",
      "Score for blended models is 0.494807 in 3.699m\n",
      "213\n",
      "Score for blended models is 0.494813 in 3.576m\n",
      "214\n",
      "Score for blended models is 0.493931 in 3.592m\n",
      "215\n",
      "Score for blended models is 0.495954 in 3.680m\n",
      "216\n",
      "Score for blended models is 0.494235 in 3.577m\n",
      "217\n",
      "Score for blended models is 0.496427 in 3.678m\n",
      "218\n",
      "Score for blended models is 0.494977 in 3.613m\n",
      "219\n",
      "Score for blended models is 0.495670 in 3.573m\n",
      "220\n",
      "Score for blended models is 0.492820 in 3.700m\n",
      "221\n",
      "Score for blended models is 0.495218 in 3.560m\n",
      "222\n",
      "Score for blended models is 0.495242 in 3.568m\n",
      "223\n",
      "Score for blended models is 0.496029 in 3.686m\n",
      "224\n",
      "Score for blended models is 0.497266 in 3.564m\n",
      "225\n",
      "Score for blended models is 0.493651 in 3.563m\n",
      "226\n",
      "Score for blended models is 0.497059 in 3.701m\n",
      "227\n",
      "Score for blended models is 0.495676 in 3.591m\n",
      "228\n",
      "Score for blended models is 0.497357 in 3.650m\n",
      "229\n",
      "Score for blended models is 0.496075 in 3.734m\n",
      "230\n",
      "Score for blended models is 0.495101 in 3.595m\n",
      "231\n",
      "Score for blended models is 0.495658 in 3.618m\n",
      "232\n",
      "Score for blended models is 0.495363 in 3.607m\n",
      "233\n",
      "Score for blended models is 0.495650 in 3.709m\n",
      "234\n",
      "Score for blended models is 0.495304 in 3.852m\n",
      "235\n",
      "Score for blended models is 0.492763 in 3.817m\n",
      "236\n",
      "Score for blended models is 0.496128 in 3.795m\n",
      "237\n",
      "Score for blended models is 0.493140 in 3.775m\n",
      "238\n",
      "Score for blended models is 0.493157 in 3.759m\n",
      "239\n",
      "Score for blended models is 0.496682 in 3.879m\n",
      "240\n",
      "Score for blended models is 0.497073 in 3.771m\n",
      "241\n",
      "Score for blended models is 0.497710 in 3.694m\n",
      "242\n",
      "Score for blended models is 0.496284 in 3.799m\n",
      "243\n",
      "Score for blended models is 0.495980 in 3.703m\n",
      "244\n",
      "Score for blended models is 0.494626 in 3.767m\n",
      "245\n",
      "Score for blended models is 0.496383 in 3.791m\n",
      "246\n",
      "Score for blended models is 0.495860 in 3.750m\n",
      "247\n",
      "Score for blended models is 0.495850 in 3.920m\n",
      "248\n",
      "Score for blended models is 0.496344 in 3.812m\n",
      "249\n",
      "Score for blended models is 0.492868 in 3.868m\n",
      "250\n",
      "Score for blended models is 0.496341 in 3.922m\n",
      "251\n",
      "Score for blended models is 0.495464 in 3.859m\n",
      "252\n",
      "Score for blended models is 0.497041 in 3.977m\n",
      "253\n",
      "Score for blended models is 0.496397 in 3.839m\n",
      "254\n",
      "Score for blended models is 0.494406 in 3.980m\n",
      "255\n",
      "Score for blended models is 0.494337 in 4.077m\n",
      "256\n",
      "Score for blended models is 0.497862 in 3.758m\n",
      "257\n",
      "Score for blended models is 0.496800 in 3.903m\n",
      "258\n",
      "Score for blended models is 0.495937 in 3.788m\n",
      "259\n",
      "Score for blended models is 0.495136 in 3.765m\n",
      "260\n",
      "Score for blended models is 0.495673 in 3.898m\n",
      "261\n",
      "Score for blended models is 0.496166 in 3.802m\n",
      "262\n",
      "Score for blended models is 0.496499 in 3.843m\n",
      "263\n",
      "Score for blended models is 0.493631 in 3.862m\n",
      "264\n",
      "Score for blended models is 0.494078 in 3.790m\n",
      "265\n",
      "Score for blended models is 0.494960 in 3.924m\n",
      "266\n",
      "Score for blended models is 0.495420 in 3.774m\n",
      "267\n",
      "Score for blended models is 0.493924 in 3.815m\n",
      "268\n",
      "Score for blended models is 0.497729 in 3.868m\n",
      "269\n",
      "Score for blended models is 0.493927 in 3.799m\n",
      "270\n",
      "Score for blended models is 0.494861 in 3.921m\n",
      "271\n",
      "Score for blended models is 0.496098 in 3.802m\n",
      "272\n",
      "Score for blended models is 0.496867 in 3.793m\n",
      "273\n",
      "Score for blended models is 0.493229 in 3.941m\n",
      "274\n",
      "Score for blended models is 0.494681 in 3.805m\n",
      "275\n",
      "Score for blended models is 0.495635 in 3.800m\n",
      "276\n",
      "Score for blended models is 0.494150 in 3.813m\n",
      "277\n",
      "Score for blended models is 0.497192 in 3.802m\n",
      "278\n",
      "Score for blended models is 0.496776 in 3.922m\n",
      "279\n",
      "Score for blended models is 0.494272 in 3.821m\n",
      "280\n",
      "Score for blended models is 0.494510 in 3.808m\n",
      "281\n",
      "Score for blended models is 0.496295 in 3.818m\n",
      "282\n",
      "Score for blended models is 0.495415 in 3.820m\n",
      "283\n",
      "Score for blended models is 0.495819 in 3.817m\n",
      "284\n",
      "Score for blended models is 0.496779 in 3.828m\n",
      "285\n",
      "Score for blended models is 0.496207 in 3.828m\n",
      "286\n",
      "Score for blended models is 0.494673 in 3.811m\n",
      "287\n",
      "Score for blended models is 0.495994 in 3.805m\n",
      "288\n",
      "Score for blended models is 0.494894 in 3.908m\n",
      "289\n",
      "Score for blended models is 0.495215 in 3.831m\n",
      "290\n",
      "Score for blended models is 0.495213 in 3.824m\n",
      "291\n",
      "Score for blended models is 0.496347 in 3.810m\n",
      "292\n",
      "Score for blended models is 0.495427 in 3.825m\n",
      "293\n",
      "Score for blended models is 0.497052 in 3.823m\n",
      "294\n",
      "Score for blended models is 0.496281 in 3.809m\n",
      "295\n",
      "Score for blended models is 0.494663 in 3.842m\n",
      "296\n",
      "Score for blended models is 0.493444 in 3.839m\n",
      "297\n",
      "Score for blended models is 0.495082 in 3.833m\n",
      "298\n",
      "Score for blended models is 0.497506 in 3.818m\n",
      "299\n",
      "Score for blended models is 0.495441 in 3.831m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_2nd.shape[0], 3))\n",
    "test_total = np.zeros((test_2nd_mean.shape[0], 3))\n",
    "name_train_blend = '../tmp/train_adet_3rd.csv'\n",
    "name_test_blend = '../tmp/test_adet_3rd.csv'\n",
    "score_total = 0\n",
    "count = 300\n",
    "for n in range(count):\n",
    "    print n\n",
    "    randomseed = n + 54453\n",
    "    est = [AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=0.8, n_estimators=800)]\n",
    "    (train_blend_2nd_ADET,\n",
    "     test_blend_2nd_ADET,\n",
    "     blend_scores_2nd_ADET,\n",
    "     best_rounds_2nd_ADET) = ET_blend(est,\n",
    "                                 train_2nd,train_y,\n",
    "                                 test_2nd_mean,\n",
    "                                 5,randomseed)\n",
    "    train_total += train_blend_2nd_ADET\n",
    "    test_total += test_blend_2nd_ADET\n",
    "    score_total += np.mean(blend_scores_2nd_ADET)\n",
    "    \n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49552927936\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_3rdADET_300bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_3rdADET_300bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print score_total\n",
    "# print (np.mean(best_rounds_RFC,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(\"../input/\" + 'test_BM_MB_add03052240.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_2ndADET_100bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_total)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.01042222e-01,   6.56203333e-01,   2.42754444e-01],\n",
       "       [  9.87466111e-01,   1.21961111e-02,   3.37777778e-04],\n",
       "       [  9.33456667e-01,   6.61800000e-02,   3.63333333e-04],\n",
       "       ..., \n",
       "       [  9.88676111e-01,   1.10300000e-02,   2.93888889e-04],\n",
       "       [  9.44523333e-01,   5.38172222e-02,   1.65944444e-03],\n",
       "       [  5.49079444e-01,   4.14411111e-01,   3.65094444e-02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
