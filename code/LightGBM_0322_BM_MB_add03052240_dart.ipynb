{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322) (74659, 322) (49352L,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "train_y = np.ravel(pd.read_csv(data_path + 'labels_BrandenMurray.csv'))\n",
    "ntrain = train_X.shape[0]\n",
    "sub_id = test_X.listing_id.values\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 322)\n",
      "(9871, 322)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "\n",
    "import sys  \n",
    "stdi,stdo,stde=sys.stdin,sys.stdout,sys.stderr\n",
    "reload(sys)  \n",
    "sys.stdin,sys.stdout,sys.stderr=stdi,stdo,stde\n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = lgb.LGBMClassifier()\n",
    "# clf.set_params(learning_rate = 0.1)\n",
    "# clf.set_params(subsample_freq = 1)\n",
    "# clf.set_params(objective = 'multiclass')\n",
    "# clf.set_params(n_estimators = 100000)\n",
    "        \n",
    "# clf = clf.fit(X_train, y_train,\n",
    "#               eval_set = [(X_val,y_val)],\n",
    "#               eval_metric = 'multi_logloss',\n",
    "#               early_stopping_rounds = 50,\n",
    "#               verbose = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred_y = clf.predict_proba(test_X, num_iteration = clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# sub_name = '../output/sub_LightGBM_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "# out_df = pd.DataFrame(pred_y[:,:3])\n",
    "# out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "# out_df[\"listing_id\"] = sub_id\n",
    "# out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(learning_rate = 0.1)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "clf.set_params(boosting_type = 'dart')\n",
    "clf.set_params(seed = seed)\n",
    "tmp  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  \t0.541015795481 1394\n",
      "15  \t0.535520643335 1116\n",
      "31  \t0.532809905024 778\n",
      "63  \t0.533565449038 625\n",
      "127  \t0.536832255243 325\n",
      "255  \t0.541986381983 245\n"
     ]
    }
   ],
   "source": [
    "for x in [8,15,31,63,127,255]:\n",
    "    clf.set_params(num_leaves = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        num_leaves = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(num_leaves = num_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20  \t0.539272604499 407\n",
      "30  \t0.543087827387 285\n",
      "50  \t0.533818564892 750\n",
      "70  \t0.533334507269 810\n",
      "80  \t0.533467385285 668\n",
      "90  \t0.534999595688 645\n",
      "100  \t0.535044396376 549\n",
      "110  \t0.530928410377 880\n",
      "120  \t0.532047199131 881\n",
      "150  \t0.532855052432 807\n",
      "170  \t0.533691244069 781\n",
      "200  \t0.53325817989 759\n",
      "230  \t0.532611706884 890\n",
      "260  \t0.533057418827 915\n"
     ]
    }
   ],
   "source": [
    "min_child_samples = 10\n",
    "\n",
    "for x in [20, 30, 50, 70, 80,90,100,110,120,150,170,200,230,260]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  \t0.532911772246 954\n",
      "350  \t0.534287413942 967\n",
      "400  \t0.534026193565 911\n",
      "450  \t0.534118220251 809\n",
      "500  \t0.533101353311 938\n"
     ]
    }
   ],
   "source": [
    "for x in [300,350,400,450,500]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=500, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=110, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print min_child_samples\n",
    "clf.set_params(min_child_samples = min_child_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2  \t0.530523614238 1319\n",
      "0.3  \t0.528740074153 1151\n",
      "0.4  \t0.52847335583 1200\n",
      "0.5  \t0.529425519615 1170\n",
      "0.6  \t0.531773805656 801\n",
      "0.7  \t0.537678359227 407\n",
      "0.8  \t0.532697221123 771\n",
      "0.9  \t0.533260395329 806\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = 1\n",
    "for x in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    clf.set_params(colsample_bytree = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        colsample_bytree = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=110, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print colsample_bytree\n",
    "\n",
    "clf.set_params(colsample_bytree = colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5  \t0.534293883688 809\n",
      "0.6  \t0.534663564215 806\n",
      "0.7  \t0.529391220803 1323\n",
      "0.8  \t0.532074542962 810\n",
      "0.9  \t0.532550988969 755\n"
     ]
    }
   ],
   "source": [
    "subsample = 1.0\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    clf.set_params(subsample = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        subsample = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=110, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1.0, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print subsample\n",
    "clf.set_params(subsample = subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  \t0.536688857754 1049\n",
      "31  \t0.534468185301 935\n",
      "63  \t0.530532367141 1155\n",
      "127  \t0.540523135824 407\n",
      "511  \t0.53000503041 952\n",
      "1023  \t0.539521507992 407\n",
      "2047  \t0.53093937159 957\n"
     ]
    }
   ],
   "source": [
    "max_bin = 255\n",
    "\n",
    "for x in [15,31,63, 127, 511, 1023, 2047]: #[200,300,400]:#\n",
    "    clf.set_params(max_bin = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        max_bin = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210  \t0.539239337795 407\n",
      "240  \t0.529590411308 898\n",
      "270  \t0.528406563042 1230\n",
      "300  \t0.529405419661 1051\n",
      "350  \t0.532100747004 810\n",
      "400  \t0.531729001523 771\n",
      "450  \t0.539675397699 410\n"
     ]
    }
   ],
   "source": [
    "for x in [210,240,270,300,350,400,450]:\n",
    "    clf.set_params(max_bin = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        max_bin = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=270, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=110, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1.0, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print max_bin\n",
    "clf.set_params(max_bin = max_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05  \t0.530407385003 910\n",
      "0.15  \t0.530345892393 1114\n",
      "0.2  \t0.530500969161 957\n",
      "0.25  \t0.530247231038 1051\n",
      "0.3  \t0.52912889179 1116\n"
     ]
    }
   ],
   "source": [
    "drop_rate = 0.1\n",
    "# clf.set_params(max_drop = 50)\n",
    "\n",
    "for x in [0.05,0.15,0.2, 0.25, 0.3]: \n",
    "    clf.set_params(drop_rate = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        drop_rate = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3  \t0.582516152841 221\n",
      "0.4  \t0.549728883413 372\n",
      "0.6  \t0.528374802311 864\n",
      "0.7  \t0.529720291491 629\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(max_drop = 50)\n",
    "clf.set_params(drop_rate = 0.1)\n",
    "skip_drop = 0.5\n",
    "for x in [0.3,0.4,0.6,0.7]: \n",
    "    clf.set_params(skip_drop = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        skip_drop = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8  \t0.528899728631 557\n",
      "0.9  \t0.53059101276 504\n",
      "1.0  \t0.53231000987 309\n"
     ]
    }
   ],
   "source": [
    "for x in [0.8,0.9,1.0]: \n",
    "    clf.set_params(skip_drop = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        skip_drop = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', colsample_bytree=0.4, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=270, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=110, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=2017, sigmoid=1.0, silent=True,\n",
       "        skip_drop=1.0, subsample=1.0, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_child_samples, colsample_bytree, subsample,\n",
    "            drop_rate, skip_drop, learning_rate=0.1):\n",
    "    skf = KFold(n_splits=5,random_state=seed)\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf.split(train_X)):\n",
    "        est=lgb.LGBMClassifier(boosting_type = 'dart',\n",
    "                               seed = seed,\n",
    "                               learning_rate=0.1,\n",
    "                               max_bin=int(max_bin),\n",
    "                               num_leaves=int(num_leaves),\n",
    "                               min_child_samples=int(min_child_samples),\n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               subsample=subsample,\n",
    "                               subsample_freq = 1,\n",
    "                               drop_rate = drop_rate,\n",
    "                               skip_drop = skip_drop\n",
    "                              )\n",
    " \n",
    "        train_x_fold = train_X.iloc[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_X.iloc[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params(n_estimators=100000)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                eval_set=[(val_x_fold, val_y_fold)],\n",
    "                eval_metric='multi_logloss',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False\n",
    "               )\n",
    "        val_y_predict_fold = est.predict_proba(val_x_fold)\n",
    "        score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv,\n",
    "                               {\n",
    "        'max_bin': (63,511),\n",
    "        'num_leaves': (15,127),\n",
    "        'min_child_samples' :(100,350),\n",
    "        'colsample_bytree': (0.2,0.7),\n",
    "        'subsample' : (0.7,1),\n",
    "        'drop_rate' : (0.05,0.4),\n",
    "        'skip_drop' : (0.4,0.9)\n",
    "    })\n",
    "\n",
    "lgbm_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_child_samples</th>\n",
       "      <th>max_bin</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>drop_rate</th>\n",
       "      <th>skip_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>24.369054</td>\n",
       "      <td>105.999784</td>\n",
       "      <td>347.890343</td>\n",
       "      <td>0.296539</td>\n",
       "      <td>0.958292</td>\n",
       "      <td>0.296937</td>\n",
       "      <td>0.761489</td>\n",
       "      <td>-0.527973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36.551311</td>\n",
       "      <td>103.138433</td>\n",
       "      <td>372.848850</td>\n",
       "      <td>0.341755</td>\n",
       "      <td>0.897879</td>\n",
       "      <td>0.351620</td>\n",
       "      <td>0.820402</td>\n",
       "      <td>-0.528236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24.835812</td>\n",
       "      <td>127.011433</td>\n",
       "      <td>379.260023</td>\n",
       "      <td>0.692687</td>\n",
       "      <td>0.983991</td>\n",
       "      <td>0.318513</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>-0.528337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53.980917</td>\n",
       "      <td>128.971711</td>\n",
       "      <td>200.086203</td>\n",
       "      <td>0.371401</td>\n",
       "      <td>0.868250</td>\n",
       "      <td>0.166791</td>\n",
       "      <td>0.607333</td>\n",
       "      <td>-0.528482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.459668</td>\n",
       "      <td>127.677774</td>\n",
       "      <td>224.976864</td>\n",
       "      <td>0.633188</td>\n",
       "      <td>0.953674</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>0.855635</td>\n",
       "      <td>-0.528548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36.393594</td>\n",
       "      <td>107.774077</td>\n",
       "      <td>162.760818</td>\n",
       "      <td>0.538180</td>\n",
       "      <td>0.962790</td>\n",
       "      <td>0.294802</td>\n",
       "      <td>0.831861</td>\n",
       "      <td>-0.528939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65.263138</td>\n",
       "      <td>103.264768</td>\n",
       "      <td>68.005549</td>\n",
       "      <td>0.441635</td>\n",
       "      <td>0.977634</td>\n",
       "      <td>0.367447</td>\n",
       "      <td>0.688544</td>\n",
       "      <td>-0.529395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23.274251</td>\n",
       "      <td>106.841420</td>\n",
       "      <td>257.351517</td>\n",
       "      <td>0.437205</td>\n",
       "      <td>0.721438</td>\n",
       "      <td>0.339741</td>\n",
       "      <td>0.878603</td>\n",
       "      <td>-0.529592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.450374</td>\n",
       "      <td>111.507409</td>\n",
       "      <td>70.716295</td>\n",
       "      <td>0.356903</td>\n",
       "      <td>0.860929</td>\n",
       "      <td>0.275919</td>\n",
       "      <td>0.581766</td>\n",
       "      <td>-0.529795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>62.818540</td>\n",
       "      <td>100.065105</td>\n",
       "      <td>305.343533</td>\n",
       "      <td>0.459058</td>\n",
       "      <td>0.828485</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.679024</td>\n",
       "      <td>-0.529982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_leaves  min_child_samples     max_bin  colsample_bytree  subsample  \\\n",
       "31   24.369054         105.999784  347.890343          0.296539   0.958292   \n",
       "25   36.551311         103.138433  372.848850          0.341755   0.897879   \n",
       "16   24.835812         127.011433  379.260023          0.692687   0.983991   \n",
       "7    53.980917         128.971711  200.086203          0.371401   0.868250   \n",
       "1    20.459668         127.677774  224.976864          0.633188   0.953674   \n",
       "5    36.393594         107.774077  162.760818          0.538180   0.962790   \n",
       "29   65.263138         103.264768   68.005549          0.441635   0.977634   \n",
       "11   23.274251         106.841420  257.351517          0.437205   0.721438   \n",
       "9    22.450374         111.507409   70.716295          0.356903   0.860929   \n",
       "18   62.818540         100.065105  305.343533          0.459058   0.828485   \n",
       "\n",
       "    drop_rate  skip_drop     score  \n",
       "31   0.296937   0.761489 -0.527973  \n",
       "25   0.351620   0.820402 -0.528236  \n",
       "16   0.318513   0.779600 -0.528337  \n",
       "7    0.166791   0.607333 -0.528482  \n",
       "1    0.058562   0.855635 -0.528548  \n",
       "5    0.294802   0.831861 -0.528939  \n",
       "29   0.367447   0.688544 -0.529395  \n",
       "11   0.339741   0.878603 -0.529592  \n",
       "9    0.275919   0.581766 -0.529795  \n",
       "18   0.167969   0.679024 -0.529982  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_bo_scores = pd.DataFrame([[s[0]['num_leaves'],\n",
    "                               s[0]['min_child_samples'],\n",
    "                               s[0]['max_bin'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['drop_rate'],\n",
    "                               s[0]['skip_drop'],\n",
    "                               s[1]] for s in zip(lgbm_BO.res['all']['params'],lgbm_BO.res['all']['values'])],\n",
    "                            columns = ['num_leaves',\n",
    "                                       'min_child_samples',\n",
    "                                       'max_bin',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'drop_rate',\n",
    "                                       'skip_drop',\n",
    "                                       'score'])\n",
    "gbm_bo_scores=gbm_bo_scores.sort_values('score',ascending=False)\n",
    "gbm_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=50):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(learning_rate = 0.01)\n",
    "        est.set_params(subsample_freq = 1)\n",
    "        est.set_params(objective = 'multiclass')\n",
    "        est.set_params(n_estimators = 1000000)\n",
    "        est.set_params(boosting_type = 'dart')\n",
    "        est.set_params(seed = seed)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est)) \n",
    "\n",
    "        \n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "            est.fit(train_x_fold, train_y_fold,\n",
    "                   eval_set = [(val_x_fold,val_y_fold)],\n",
    "                   eval_metric = 'multi_logloss',\n",
    "                   early_stopping_rounds = early_stopping_rounds,\n",
    "                   verbose = False)\n",
    "            \n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,num_iteration = best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,num_iteration=best_round)\n",
    "            \n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "           \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 10 folds\n",
      "Model 1: LGBMClassifier(boosting_type='dart', colsample_bytree=0.296539,\n",
      "        drop_rate=0.296937, is_unbalance=False, learning_rate=0.01,\n",
      "        max_bin=347, max_depth=-1, max_drop=50, min_child_samples=105,\n",
      "        min_child_weight=5, min_split_gain=0, n_estimators=1000000,\n",
      "        nthread=-1, num_leaves=24, objective='multiclass', reg_alpha=0,\n",
      "        reg_lambda=0, scale_pos_weight=1, seed=2017, sigmoid=1.0,\n",
      "        silent=True, skip_drop=0.761489, subsample=0.958292,\n",
      "        subsample_for_bin=50000, subsample_freq=1, uniform_drop=False,\n",
      "        xgboost_dart_mode=False)\n",
      "Model 1 fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d3059ae5dedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                500) #as the learning rate decreases the number of stopping rounds need to be increased\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-95f83a61953c>\u001b[0m in \u001b[0;36mlgbm_blend\u001b[0;34m(estimators, train_x, train_y, test_x, fold, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     36\u001b[0m                    \u001b[0meval_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'multi_logloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                    \u001b[0mearly_stopping_rounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                    verbose = False)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mbest_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\lightgbm-0.1-py2.7.egg\\lightgbm\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    585\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\lightgbm-0.1-py2.7.egg\\lightgbm\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_sample_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    408\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\lightgbm-0.1-py2.7.egg\\lightgbm\\engine.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, callbacks)\u001b[0m\n\u001b[1;32m    177\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\lightgbm-0.1-py2.7.egg\\lightgbm\\basic.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1343\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1345\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "est       = [lgb.LGBMClassifier(num_leaves = 24,\n",
    "                                min_child_samples = 105,\n",
    "                                colsample_bytree = 0.296539,\n",
    "                                subsample = 0.958292,\n",
    "                                max_bin = 347,\n",
    "                                drop_rate = 0.296937,\n",
    "                                skip_drop = 0.761489),\n",
    "             lgb.LGBMClassifier(num_leaves = 36,\n",
    "                                min_child_samples = 103,\n",
    "                                colsample_bytree = 0.341755,\n",
    "                                subsample = 0.897879,\n",
    "                                max_bin = 372,\n",
    "                                drop_rate = 0.351620,\n",
    "                                skip_drop = 0.820402),\n",
    "             lgb.LGBMClassifier(num_leaves = 24,\n",
    "                                min_child_samples = 127,\n",
    "                                colsample_bytree = 0.692687,\n",
    "                                subsample = 0.983991,\n",
    "                                max_bin = 379,\n",
    "                                drop_rate = 0.318513,\n",
    "                                skip_drop = 0.779600),\n",
    "             lgb.LGBMClassifier(num_leaves = 53,\n",
    "                                min_child_samples = 128,\n",
    "                                colsample_bytree = 0.371401,\n",
    "                                subsample = 0.868250,\n",
    "                                max_bin = 200,\n",
    "                                drop_rate = 0.166791,\n",
    "                                skip_drop = 0.607333),\n",
    "             lgb.LGBMClassifier(num_leaves = 20,\n",
    "                                min_child_samples = 127,\n",
    "                                colsample_bytree = 0.633188,\n",
    "                                subsample = 0.953674,\n",
    "                                max_bin = 224,\n",
    "                                drop_rate = 0.058562,\n",
    "                                skip_drop = 0.855635)]\n",
    "#  \t\tnum_leaves \tmin_child_samples \tmax_bin \tcolsample_bytree \tsubsample \tdrop_rate \tskip_drop \tscore\n",
    "# 31 \t24.369054 \t105.999784 \t\t\t347.890343 \t0.296539 \t\t\t0.958292 \t0.296937 \t0.761489 \t-0.527973\n",
    "# 25 \t36.551311 \t103.138433 \t\t\t372.848850 \t0.341755 \t\t\t0.897879 \t0.351620 \t0.820402 \t-0.528236\n",
    "# 16 \t24.835812 \t127.011433 \t\t\t379.260023 \t0.692687 \t\t\t0.983991 \t0.318513 \t0.779600 \t-0.528337\n",
    "# 7 \t53.980917 \t128.971711 \t\t\t200.086203 \t0.371401 \t\t\t0.868250 \t0.166791 \t0.607333 \t-0.528482\n",
    "# 1 \t20.459668 \t127.677774 \t\t\t224.976864 \t0.633188 \t\t\t0.953674 \t0.058562 \t0.855635 \t-0.528548\n",
    "\n",
    "(train_blend_x_gbm,\n",
    " test_blend_x_gbm_mean,\n",
    " test_blend_x_gbm_gmean,\n",
    " blend_scores_gbm,\n",
    " best_rounds_gbm)= lgbm_blend(est, \n",
    "                               train_X, train_y, \n",
    "                               test_X,\n",
    "                               10,\n",
    "                               500) #as the learning rate decreases the number of stopping rounds need to be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[25]\tvalid_0's multi_logloss: 0.865046\n",
      "[50]\tvalid_0's multi_logloss: 0.734173\n",
      "[75]\tvalid_0's multi_logloss: 0.678042\n",
      "[100]\tvalid_0's multi_logloss: 0.643931\n",
      "[125]\tvalid_0's multi_logloss: 0.623701\n",
      "[150]\tvalid_0's multi_logloss: 0.612349\n",
      "[175]\tvalid_0's multi_logloss: 0.597503\n",
      "[200]\tvalid_0's multi_logloss: 0.590527\n",
      "[225]\tvalid_0's multi_logloss: 0.586382\n",
      "[250]\tvalid_0's multi_logloss: 0.581891\n",
      "[275]\tvalid_0's multi_logloss: 0.579075\n",
      "[300]\tvalid_0's multi_logloss: 0.575144\n",
      "[325]\tvalid_0's multi_logloss: 0.572181\n",
      "[350]\tvalid_0's multi_logloss: 0.570597\n",
      "[375]\tvalid_0's multi_logloss: 0.566742\n",
      "[400]\tvalid_0's multi_logloss: 0.564122\n",
      "[425]\tvalid_0's multi_logloss: 0.562709\n",
      "[450]\tvalid_0's multi_logloss: 0.560168\n",
      "[475]\tvalid_0's multi_logloss: 0.558061\n",
      "[500]\tvalid_0's multi_logloss: 0.556797\n",
      "[525]\tvalid_0's multi_logloss: 0.554282\n",
      "[550]\tvalid_0's multi_logloss: 0.553685\n",
      "[575]\tvalid_0's multi_logloss: 0.552499\n",
      "[600]\tvalid_0's multi_logloss: 0.550735\n",
      "[625]\tvalid_0's multi_logloss: 0.549461\n",
      "[650]\tvalid_0's multi_logloss: 0.548251\n",
      "[675]\tvalid_0's multi_logloss: 0.547627\n",
      "[700]\tvalid_0's multi_logloss: 0.547005\n",
      "[725]\tvalid_0's multi_logloss: 0.54564\n",
      "[750]\tvalid_0's multi_logloss: 0.544965\n",
      "[775]\tvalid_0's multi_logloss: 0.544243\n",
      "[800]\tvalid_0's multi_logloss: 0.543141\n",
      "[825]\tvalid_0's multi_logloss: 0.542449\n",
      "[850]\tvalid_0's multi_logloss: 0.541694\n",
      "[875]\tvalid_0's multi_logloss: 0.540632\n",
      "[900]\tvalid_0's multi_logloss: 0.540128\n",
      "[925]\tvalid_0's multi_logloss: 0.539559\n",
      "[950]\tvalid_0's multi_logloss: 0.53886\n",
      "[975]\tvalid_0's multi_logloss: 0.538204\n",
      "[1000]\tvalid_0's multi_logloss: 0.53765\n",
      "[1025]\tvalid_0's multi_logloss: 0.537191\n",
      "[1050]\tvalid_0's multi_logloss: 0.536834\n",
      "[1075]\tvalid_0's multi_logloss: 0.536342\n",
      "[1100]\tvalid_0's multi_logloss: 0.535929\n",
      "[1125]\tvalid_0's multi_logloss: 0.535704\n",
      "[1150]\tvalid_0's multi_logloss: 0.535364\n",
      "[1175]\tvalid_0's multi_logloss: 0.535182\n",
      "[1200]\tvalid_0's multi_logloss: 0.53506\n",
      "[1225]\tvalid_0's multi_logloss: 0.534795\n",
      "[1250]\tvalid_0's multi_logloss: 0.534459\n",
      "[1275]\tvalid_0's multi_logloss: 0.534103\n",
      "[1300]\tvalid_0's multi_logloss: 0.533832\n",
      "[1325]\tvalid_0's multi_logloss: 0.533564\n",
      "[1350]\tvalid_0's multi_logloss: 0.533323\n",
      "[1375]\tvalid_0's multi_logloss: 0.533187\n",
      "[1400]\tvalid_0's multi_logloss: 0.533234\n",
      "[1425]\tvalid_0's multi_logloss: 0.533016\n",
      "[1450]\tvalid_0's multi_logloss: 0.532895\n",
      "[1475]\tvalid_0's multi_logloss: 0.532817\n",
      "[1500]\tvalid_0's multi_logloss: 0.532617\n",
      "[1525]\tvalid_0's multi_logloss: 0.532523\n",
      "[1550]\tvalid_0's multi_logloss: 0.532346\n",
      "[1575]\tvalid_0's multi_logloss: 0.532181\n",
      "[1600]\tvalid_0's multi_logloss: 0.532094\n",
      "[1625]\tvalid_0's multi_logloss: 0.531981\n",
      "[1650]\tvalid_0's multi_logloss: 0.531839\n",
      "[1675]\tvalid_0's multi_logloss: 0.531803\n",
      "[1700]\tvalid_0's multi_logloss: 0.531593\n",
      "[1725]\tvalid_0's multi_logloss: 0.53141\n",
      "[1750]\tvalid_0's multi_logloss: 0.531199\n",
      "[1775]\tvalid_0's multi_logloss: 0.530949\n",
      "[1800]\tvalid_0's multi_logloss: 0.530882\n",
      "[1825]\tvalid_0's multi_logloss: 0.530931\n",
      "Early stopping, best iteration is:\n",
      "[1797]\tvalid_0's multi_logloss: 0.530851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = lgb.LGBMClassifier(num_leaves = 24,\n",
    "                                min_child_samples = 105,\n",
    "                                colsample_bytree = 0.296539,\n",
    "                                subsample = 0.958292,\n",
    "                                max_bin = 347,\n",
    "                                drop_rate = 0.296937,\n",
    "                                skip_drop = 0.761489)\n",
    "clf.set_params(learning_rate = 0.03)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "clf.set_params(boosting_type = 'dart')\n",
    "clf.set_params(seed = seed)\n",
    "\n",
    "        \n",
    "clf = clf.fit(X_train, y_train,\n",
    "              eval_set = [(X_val,y_val)],\n",
    "              eval_metric = 'multi_logloss',\n",
    "              early_stopping_rounds = 50,\n",
    "              verbose = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52309656  0.52262419  0.52356597  0.52188342  0.52198129]\n",
      "[ 15266.3  12448.   15418.7   7687.9  11589.2]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_LightGBM_dart_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_LightGBM_dart_mean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../blend/test_blend_LightGBM_dart_gmean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_gbm,axis=0))\n",
    "print (np.mean(best_rounds_gbm,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_gbm, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_gbm_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_gbm_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LightGBM_dart_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_gbm_mean[:,9:12])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
