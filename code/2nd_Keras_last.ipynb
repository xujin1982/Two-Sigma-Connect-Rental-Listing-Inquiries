{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,ParametricSoftplus,ThresholdedReLU,SReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.ravel(pd.read_csv('../input/' + 'labels_BrandenMurray.csv'))\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "names = ['low_0','medium_0','high_0',\n",
    "        'low_1','medium_1','high_1',\n",
    "        'low_2','medium_2','high_2',\n",
    "        'low_3','medium_3','high_3',\n",
    "        'low_4','medium_4','high_4',\n",
    "        'low_5','medium_5','high_5',\n",
    "        'low_6','medium_6','high_6',\n",
    "        'low_7','medium_7','high_7',\n",
    "        'low_8','medium_8','high_8',\n",
    "        'low_9','medium_9','high_9']\n",
    "\n",
    "data_path = \"../2ndlast/\"\n",
    "total_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.355361      0.544070    0.100569\n",
      "1   0.508303      0.446720    0.044978\n",
      "2   0.603091      0.349880    0.047029\n",
      "3   0.616221      0.328639    0.055140\n",
      "4   0.947230      0.049863    0.002907\n",
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.288217      0.532268    0.179515\n",
      "1   0.970891      0.025801    0.003308\n",
      "2   0.908912      0.078535    0.012553\n",
      "3   0.400539      0.476918    0.122542\n",
      "4   0.700470      0.269945    0.029586\n"
     ]
    }
   ],
   "source": [
    "# RFC 1st level \n",
    "file_train      = 'train_blend_RFC_entropy_last_2017-04-21-11-06' + '.csv'\n",
    "file_test_mean  = 'test_blend_RFC_entropy_mean_last_2017-04-21-11-06' + '.csv'\n",
    "\n",
    "train_rfc = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_rfc  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_rfc.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_rfc.columns = ['rfc_' + x for x in names[:n_column]]\n",
    "test_rfc.columns  = ['rfc_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_rfc.iloc[:5,:3]\n",
    "\n",
    "print test_rfc.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.259359     0.652577   0.088065\n",
      "1  0.738646     0.234238   0.027115\n",
      "2  0.396965     0.512991   0.090045\n",
      "3  0.647052     0.312537   0.040412\n",
      "4  0.923203     0.072255   0.004543\n",
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.282172     0.549423   0.168405\n",
      "1  0.955532     0.040091   0.004377\n",
      "2  0.925692     0.065197   0.009112\n",
      "3  0.631038     0.309690   0.059272\n",
      "4  0.803117     0.187725   0.009158\n"
     ]
    }
   ],
   "source": [
    "# LR 1st level\n",
    "file_train      = 'train_blend_LR_last_2017-04-21-11-16' + '.csv'\n",
    "file_test_mean  = 'test_blend_LR_mean_last_2017-04-21-11-16' + '.csv'\n",
    "\n",
    "train_LR = pd.read_csv(data_path + file_train, header = None)\n",
    "test_LR  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_LR.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_LR.columns = ['LR_' + x for x in names[:n_column]]\n",
    "test_LR.columns  = ['LR_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_LR.iloc[:5,:3]\n",
    "print test_LR.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.332903     0.538085   0.129012\n",
      "1  0.471780     0.454812   0.073408\n",
      "2  0.582223     0.383893   0.033884\n",
      "3  0.622462     0.328557   0.048981\n",
      "4  0.926402     0.064996   0.008602\n",
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.309822     0.527181   0.162997\n",
      "1  0.984336     0.014059   0.001605\n",
      "2  0.956579     0.038080   0.005341\n",
      "3  0.518490     0.384524   0.096986\n",
      "4  0.759357     0.210049   0.030594\n"
     ]
    }
   ],
   "source": [
    "# ET 1st level\n",
    "file_train      = 'train_blend_ET_entropy_last_2017-04-21-11-48' + '.csv'\n",
    "file_test_mean  = 'test_blend_ET_entropy_mean_last_2017-04-21-11-48' + '.csv'\n",
    "\n",
    "train_ET = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_ET  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_ET.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ET.columns = ['ET_' + x for x in names[:n_column]]\n",
    "test_ET.columns  = ['ET_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ET.iloc[:5,:3]\n",
    "print test_ET.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.507812              0.390625            0.101562\n",
      "1           0.531250              0.359375            0.109375\n",
      "2           0.671875              0.273438            0.054688\n",
      "3           0.609375              0.250000            0.140625\n",
      "4           0.843750              0.140625            0.015625\n",
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.381250              0.457813            0.160938\n",
      "1           0.968750              0.031250            0.000000\n",
      "2           0.970313              0.029687            0.000000\n",
      "3           0.693750              0.259375            0.046875\n",
      "4           0.612500              0.321875            0.065625\n"
     ]
    }
   ],
   "source": [
    "# KNN 1st level\n",
    "file_train      = 'train_blend_KNN_uniform_last_2017-04-21-13-53' + '.csv'\n",
    "file_test_mean  = 'test_blend_KNN_uniform_mean_last_2017-04-21-13-53' + '.csv'\n",
    "\n",
    "\n",
    "train_KNN = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_KNN  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_KNN.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_KNN.columns      = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "test_KNN.columns  = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_KNN.iloc[:5,:3]\n",
    "print test_KNN.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.460187          0.436036        0.103776\n",
      "1       0.268598          0.571916        0.159486\n",
      "2       0.724799          0.239351        0.035851\n",
      "3       0.669683          0.286716        0.043600\n",
      "4       0.917878          0.073469        0.008653\n",
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.449136          0.411890        0.138974\n",
      "1       0.971615          0.020319        0.008066\n",
      "2       0.909864          0.074040        0.016096\n",
      "3       0.661175          0.269026        0.069799\n",
      "4       0.705851          0.263069        0.031080\n"
     ]
    }
   ],
   "source": [
    "# TFFM 1st level 0322\n",
    "file_train      = 'train_blend_FM_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "file_test_mean  = 'test_blend_FM_mean_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "\n",
    "train_FM_0322      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_FM_mean_0322  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_FM_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_FM_0322.columns      = ['FM_0322_' + x for x in names[:n_column]]\n",
    "test_FM_mean_0322.columns  = ['FM_0322_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_FM_0322.iloc[:5,:3]\n",
    "print test_FM_mean_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.216985      0.579031    0.203985\n",
      "1   0.560375      0.382293    0.057331\n",
      "2   0.621019      0.332482    0.046499\n",
      "3   0.312441      0.345115    0.342444\n",
      "4   0.901678      0.090035    0.008287\n",
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.218456      0.591575    0.189969\n",
      "1   0.992103      0.007102    0.000795\n",
      "2   0.971220      0.025168    0.003612\n",
      "3   0.501988      0.411198    0.086813\n",
      "4   0.754496      0.213158    0.032347\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes 1st level\n",
    "file_train      = 'train_blend_MNB_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "file_test_mean  = 'test_blend_MNB_mean_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "\n",
    "\n",
    "train_MNB      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_MNB_mean  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_MNB.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_MNB.columns      = ['MNB_' + x for x in names[:n_column]]\n",
    "test_MNB_mean.columns  = ['MNB_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_MNB.iloc[:5,:3]\n",
    "print test_MNB_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tsne_0     tsne_1    tsne_2\n",
      "0  -8.398991  -2.415894 -3.602143\n",
      "1   0.698237   0.335786  8.884257\n",
      "2  -5.811380 -16.669975  7.145837\n",
      "3  -0.371861 -25.894747 -2.076309\n",
      "4 -15.371799   9.656209  5.813590\n",
      "      tsne_0     tsne_1     tsne_2\n",
      "0  -5.176846  -0.768422  -2.339259\n",
      "1   9.003089  13.250301  -0.707032\n",
      "2   4.188036  14.397186   4.573307\n",
      "3  10.890132 -12.660774 -13.414140\n",
      "4   6.011381   5.177731  15.669250\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level\n",
    "\n",
    "file_train = 'X_train_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "\n",
    "train_tsne = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "\n",
    "n_column = train_tsne.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne.columns = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "test_tsne.columns  = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "\n",
    "\n",
    "print train_tsne.iloc[:5,:3]\n",
    "print test_tsne.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -6.649132    13.028168     8.329733\n",
      "1     7.615566     0.067456   -14.932181\n",
      "2     8.333528     8.561174   -13.536297\n",
      "3    12.819587   -20.027314     0.661660\n",
      "4    -5.513088    -5.609218    17.130673\n",
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -5.721674     7.011411    -6.499047\n",
      "1     8.238390    -8.589710    13.771045\n",
      "2   -11.383577   -16.071395    15.083511\n",
      "3    -6.111491     6.348311   -10.222012\n",
      "4     4.426022    15.553415    11.315777\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level 0322\n",
    "\n",
    "file_train = 'X_train_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "\n",
    "train_tsne_0322 = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne_0322  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "n_column = train_tsne_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne_0322.columns = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "test_tsne_0322.columns  = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "\n",
    "print train_tsne_0322.iloc[:5,:3]\n",
    "print test_tsne_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.338520      0.631375    0.030105\n",
      "1   0.563156      0.390819    0.046025\n",
      "2   0.462242      0.498429    0.039328\n",
      "3   0.930174      0.067369    0.002458\n",
      "4   0.892201      0.106567    0.001232\n",
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.178073      0.621011    0.200916\n",
      "1   0.978270      0.012231    0.009499\n",
      "2   0.939539      0.055440    0.005021\n",
      "3   0.151284      0.616193    0.232523\n",
      "4   0.698756      0.292996    0.008247\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb*2\n",
    "tmp_test  = test_xgb_mean*2\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb      = (tmp_train + train_xgb*20) / 22.0\n",
    "test_xgb_mean  = (tmp_test + test_xgb_mean*20) / 22.0\n",
    "\n",
    "n_column = train_xgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "test_xgb_mean.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb.iloc[:5,:3]\n",
    "print test_xgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.309433             0.660416           0.030151\n",
      "1          0.565861             0.382300           0.051838\n",
      "2          0.401785             0.566253           0.031961\n",
      "3          0.931191             0.066339           0.002470\n",
      "4          0.881444             0.117002           0.001555\n",
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.174706             0.640575           0.184719\n",
      "1          0.977405             0.013037           0.009558\n",
      "2          0.944000             0.051268           0.004732\n",
      "3          0.145425             0.592400           0.262175\n",
      "4          0.670348             0.322061           0.007591\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "file_test_mean  = 'test_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_30fold      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_30fold.columns      = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_30fold.columns  = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_30fold.iloc[:5,:3]\n",
    "print test_xgb_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.313141          0.658802        0.028057\n",
      "1       0.491897          0.450891        0.057212\n",
      "2       0.486618          0.486574        0.026809\n",
      "3       0.925839          0.071295        0.002866\n",
      "4       0.863436          0.134349        0.002214\n",
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.167358          0.646104        0.186538\n",
      "1       0.981203          0.010042        0.008756\n",
      "2       0.906588          0.088855        0.004558\n",
      "3       0.167107          0.602472        0.230421\n",
      "4       0.716008          0.274903        0.009089\n"
     ]
    }
   ],
   "source": [
    "# XGB one vs rest 1st level\n",
    "\n",
    "file_train      = 'train_blend_xgb_ovr_last_2017-04-21-10-09' + '.csv'\n",
    "file_test_mean  = 'test_blend_xgb_ovr_mean_last_2017-04-21-10-09' + '.csv'\n",
    "\n",
    "train_xgb_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_ovr.columns      = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_ovr.columns  = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_xgb_ovr,axis=1)\n",
    "sum_test  = np.sum(test_xgb_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_xgb_ovr.columns.values:\n",
    "    train_xgb_ovr[col] = train_xgb_ovr[col] / sum_train\n",
    "    test_xgb_mean_ovr[col] = test_xgb_mean_ovr[col] / sum_test\n",
    "\n",
    "\n",
    "print train_xgb_ovr.iloc[:5,:3]\n",
    "print test_xgb_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.317617            0.650112          0.032271\n",
      "1         0.597390            0.386899          0.015711\n",
      "2         0.450143            0.517132          0.032726\n",
      "3         0.898037            0.100391          0.001572\n",
      "4         0.880435            0.118283          0.001282\n",
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.186506            0.567313          0.246180\n",
      "1         0.964578            0.025110          0.010312\n",
      "2         0.913688            0.080543          0.005769\n",
      "3         0.114846            0.657136          0.228019\n",
      "4         0.650263            0.344835          0.004902\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 1st level\n",
    "\n",
    "file_train      = 'train_blend_LightGBM_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_LightGBM_mean_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "\n",
    "\n",
    "train_lgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_lgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_lgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_lgb.columns      = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "test_lgb_mean.columns  = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_lgb.iloc[:5,:3]\n",
    "print test_lgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.335217     0.604643   0.060140\n",
      "1  0.772956     0.207974   0.019070\n",
      "2  0.529658     0.446699   0.023643\n",
      "3  0.956680     0.042453   0.000867\n",
      "4  0.938238     0.060709   0.001053\n",
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.276664     0.582024   0.141312\n",
      "1  0.995422     0.004223   0.000356\n",
      "2  0.979827     0.019278   0.000895\n",
      "3  0.362736     0.446379   0.190885\n",
      "4  0.729782     0.259918   0.010300\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level No.1\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-21-23' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-21-23' + '.csv'\n",
    "\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "test_nn_mean = tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-22-05' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-22-05' + '.csv'\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = (train_nn + tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "test_nn_mean = (test_nn_mean + tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn.columns      = ['nn_' + x for x in names[:n_column]]\n",
    "test_nn_mean.columns  = ['nn_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn.iloc[:5,:3]\n",
    "print test_nn_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.378595            0.564656          0.056749\n",
      "1         0.763784            0.215613          0.020603\n",
      "2         0.577165            0.398481          0.024354\n",
      "3         0.949192            0.049937          0.000871\n",
      "4         0.958236            0.040963          0.000801\n",
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.286080            0.567262          0.146658\n",
      "1         0.996091            0.003613          0.000296\n",
      "2         0.986770            0.012566          0.000664\n",
      "3         0.370175            0.443003          0.186822\n",
      "4         0.738394            0.250685          0.010921\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "\n",
    "train_nn_30fold     = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_30fold.columns      = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "test_nn_mean_30fold.columns  = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_30fold.iloc[:5,:3]\n",
    "print test_nn_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.414059         0.533141       0.052800\n",
      "1      0.752638         0.210038       0.037324\n",
      "2      0.515976         0.440243       0.043781\n",
      "3      0.780622         0.219167       0.000211\n",
      "4      0.965501         0.032244       0.002255\n",
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.227279         0.679449       0.093272\n",
      "1      0.976299         0.014297       0.009403\n",
      "2      0.919964         0.066669       0.013367\n",
      "3      0.266264         0.389498       0.344238\n",
      "4      0.874583         0.119637       0.005780\n"
     ]
    }
   ],
   "source": [
    "# Keras one vs rest 1st level\n",
    "file_train      = 'train_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "\n",
    "train_nn_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_ovr.columns      = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "test_nn_mean_ovr.columns  = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_nn_ovr,axis=1)\n",
    "sum_test  = np.sum(test_nn_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_nn_ovr.columns.values:\n",
    "    train_nn_ovr[col] = train_nn_ovr[col] / sum_train\n",
    "    test_nn_mean_ovr[col] = test_nn_mean_ovr[col] / sum_test \n",
    "\n",
    "print train_nn_ovr.iloc[:5,:3]\n",
    "print test_nn_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.445882            0.488624          0.065494\n",
      "1         0.761473            0.222332          0.016195\n",
      "2         0.542404            0.415798          0.041798\n",
      "3         0.956819            0.041689          0.001492\n",
      "4         0.950208            0.048009          0.001783\n",
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.300305            0.540791          0.158904\n",
      "1         0.995423            0.004364          0.000213\n",
      "2         0.984619            0.014709          0.000672\n",
      "3         0.376123            0.446293          0.177584\n",
      "4         0.713087            0.267086          0.019827\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 3layer 20 bagging\n",
    "file_train      = 'train_blend_Keras_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "\n",
    "train_nn_3layer      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_3layer  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_3layer.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_3layer.columns      = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "test_nn_mean_3layer.columns  = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_nn_3layer.iloc[:5,:3]\n",
    "print test_nn_mean_3layer.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print total_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2nd: (49352, 48)\t test_2nd_mean:(74659, 48)\n"
     ]
    }
   ],
   "source": [
    "train_2nd      = pd.concat([train_rfc, train_LR, train_ET, train_KNN, train_FM_0322,    train_MNB,     train_tsne,\n",
    "                            train_tsne_0322, train_xgb,     train_xgb_30fold,     train_xgb_ovr, \n",
    "                            train_nn,     train_nn_30fold,     train_nn_ovr,     train_nn_3layer,\n",
    "                            train_lgb\n",
    "#                             train_gp\n",
    "                           ], axis = 1)\n",
    "\n",
    "test_2nd_mean  = pd.concat([test_rfc,  test_LR,  test_ET,  test_KNN, test_FM_mean_0322, test_MNB_mean, test_tsne, \n",
    "                            test_tsne_0322,  test_xgb_mean, test_xgb_mean_30fold, test_xgb_mean_ovr,\n",
    "                            test_nn_mean, test_nn_mean_30fold, test_nn_mean_ovr, test_nn_mean_3layer,\n",
    "                            test_lgb_mean\n",
    "#                             test_gp\n",
    "                           ], axis = 1)\n",
    "\n",
    "print 'train_2nd: {}\\t test_2nd_mean:{}'.\\\n",
    "            format(train_2nd.shape,test_2nd_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 222) (74659, 222) (49352, 3)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "\n",
    "ntrain = train_X_0322.shape[0]\n",
    "sub_id = test_X_0322.listing_id.astype('int32').values\n",
    "\n",
    "train_X = pd.read_csv(data_path + 'train_CV_MS_52571.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_CV_MS_52571.csv')\n",
    "\n",
    "train_X = train_X_0322[['listing_id']].merge(train_X,on='listing_id',how='left')\n",
    "test_X = test_X_0322[['listing_id']].merge(test_X,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 223)\n",
      "(74659, 223)\n"
     ]
    }
   ],
   "source": [
    "time_feature = pd.read_csv(data_path + 'listing_image_time.csv')\n",
    "time_feature.columns = ['listing_id','time_stamp']\n",
    "train_X = train_X.merge(time_feature,on='listing_id',how='left')\n",
    "test_X = test_X.merge(time_feature,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 271)\n",
      "(74659, 271)\n"
     ]
    }
   ],
   "source": [
    "train_X = pd.concat([train_X,train_2nd],axis=1)\n",
    "test_X = pd.concat([test_X,test_2nd_mean],axis=1)\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 271)\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.concat([train_X,test_X])\n",
    "print full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 271)\n",
      "(74659, 271)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_data.fillna(0)\n",
    "\n",
    "for col in full_data.columns.values:\n",
    "    full_data.loc[:,col] = (full_data[col]-full_data[col].mean())/full_data[col].std()\n",
    "train_df_nn = full_data[:ntrain]\n",
    "test_df_nn = full_data[ntrain:]\n",
    "\n",
    "train_df_nn = sparse.csr_matrix(train_df_nn)\n",
    "test_df_nn = sparse.csr_matrix(test_df_nn)\n",
    "\n",
    "\n",
    "print train_df_nn.shape\n",
    "print test_df_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df_nn, train_y, train_size=.80, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.8412 - val_loss: 0.5434\n",
      "Epoch 2/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5956 - val_loss: 0.5183\n",
      "Epoch 3/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5562 - val_loss: 0.5107\n",
      "Epoch 4/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5490 - val_loss: 0.5100\n",
      "Epoch 5/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5414 - val_loss: 0.5065\n",
      "Epoch 6/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5352 - val_loss: 0.5066\n",
      "Epoch 7/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5395 - val_loss: 0.5068\n",
      "Epoch 8/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5335 - val_loss: 0.5045\n",
      "Epoch 9/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5292 - val_loss: 0.5035\n",
      "Epoch 10/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5337 - val_loss: 0.5028\n",
      "Epoch 11/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5274 - val_loss: 0.5028\n",
      "Epoch 12/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5268 - val_loss: 0.5015\n",
      "Epoch 13/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5261 - val_loss: 0.5002\n",
      "Epoch 14/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5254 - val_loss: 0.5011\n",
      "Epoch 15/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5249 - val_loss: 0.4999\n",
      "Epoch 16/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5244 - val_loss: 0.4997\n",
      "Epoch 17/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5222 - val_loss: 0.5001\n",
      "Epoch 18/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5229 - val_loss: 0.4999\n",
      "Epoch 19/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5204 - val_loss: 0.4984\n",
      "Epoch 20/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5233 - val_loss: 0.4975\n",
      "Epoch 21/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5178 - val_loss: 0.4977\n",
      "Epoch 22/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5172 - val_loss: 0.4988\n",
      "Epoch 23/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5204 - val_loss: 0.4975\n",
      "Epoch 24/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5186 - val_loss: 0.4973\n",
      "Epoch 25/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5204 - val_loss: 0.4974\n",
      "Epoch 26/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5142 - val_loss: 0.4984\n",
      "Epoch 27/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5181 - val_loss: 0.4988\n",
      "Epoch 28/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5154 - val_loss: 0.4971\n",
      "Epoch 29/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5136 - val_loss: 0.4970\n",
      "Epoch 30/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5135 - val_loss: 0.4972\n",
      "Epoch 31/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5161 - val_loss: 0.4972\n",
      "Epoch 32/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5122 - val_loss: 0.4990\n",
      "Epoch 33/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5159 - val_loss: 0.4980\n",
      "Epoch 34/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5135 - val_loss: 0.4979\n",
      "Epoch 35/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5138 - val_loss: 0.4983\n",
      "0.496989972549\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    \n",
    "    model.add(Dense(70, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim, # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(20,init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "#     model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(BatchNormalization())    \n",
    "#     model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                   init = init,\n",
    "                   activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "#                   metrics=[mae_log],\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=ntrain,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )\n",
    "\n",
    "print min(fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    model.add(Dense(params['input_size'], # number of input units: needs to be tuned\n",
    "                    input_dim = params['input_dim'], # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(params['input_drop_out'])) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(params['hidden_size'],\n",
    "                    init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out'])) #dropout rate. needs to be tuned\n",
    "    \n",
    "#     model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(BatchNormalization())    \n",
    "#     model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                    init = init,\n",
    "                    activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=10, batch_size=128,randomseed = 1234):\n",
    "    \n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=early_stopping_rounds, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "\n",
    "    N_params = len(parameters)\n",
    "#     print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = KFold(n_splits=fold, shuffle=True, random_state=randomseed)\n",
    "    N_class = train_y.shape[1]\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    fold_start = time.time() \n",
    "\n",
    "    \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "#         print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "#             print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "\n",
    "            model = nn_model(nn_params)\n",
    "#             print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=70,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     verbose = 0,\n",
    "                                     callbacks=[early_stop, checkpointer]\n",
    "                                    )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "#             print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )\n",
    "            \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_proba(x=val_x_fold.toarray(),verbose=0)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "#             print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = model.predict_proba(x=test_x.toarray(),verbose=0)\n",
    "#             print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "#         print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print \"Score for blended models is %f in %0.3fm\" % (np.mean(scores), (time.time() - fold_start)/60)\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting.........\n",
      "Score for blended models is 0.489167 in 10.825m\n",
      "Score for blended models is 0.490623 in 10.973m\n",
      "Score for blended models is 0.488229 in 11.748m\n",
      "Score for blended models is 0.490783 in 10.206m\n",
      "Score for blended models is 0.493555 in 10.886m\n",
      "Score for blended models is 0.493179 in 10.781m\n",
      "Score for blended models is 0.480429 in 12.540m\n",
      "Score for blended models is 0.490841 in 10.006m\n",
      "Score for blended models is 0.485040 in 11.392m\n",
      "Score for blended models is 0.488277 in 11.149m\n",
      "Score for blended models is 0.487818 in 12.856m\n",
      "Score for blended models is 0.480561 in 12.095m\n",
      "Score for blended models is 0.497479 in 11.343m\n",
      "Score for blended models is 0.492343 in 10.683m\n",
      "Score for blended models is 0.489575 in 12.049m\n",
      "Score for blended models is 0.484178 in 12.336m\n",
      "Score for blended models is 0.496555 in 11.198m\n",
      "Score for blended models is 0.486307 in 11.967m\n",
      "Score for blended models is 0.492037 in 11.356m\n",
      "Score for blended models is 0.489696 in 11.544m\n",
      "Score for blended models is 0.486754 in 11.801m\n",
      "Score for blended models is 0.490197 in 11.319m\n",
      "Score for blended models is 0.486055 in 13.453m\n",
      "Score for blended models is 0.494469 in 10.395m\n",
      "Score for blended models is 0.500565 in 11.883m\n",
      "Score for blended models is 0.497213 in 11.181m\n",
      "Score for blended models is 0.488740 in 10.752m\n",
      "Score for blended models is 0.488423 in 11.526m\n",
      "Score for blended models is 0.482226 in 11.455m\n",
      "Score for blended models is 0.492961 in 10.260m\n",
      "Score for blended models is 0.481851 in 11.858m\n",
      "Score for blended models is 0.486496 in 11.311m\n",
      "Score for blended models is 0.486010 in 11.695m\n",
      "Score for blended models is 0.493094 in 11.073m\n",
      "Score for blended models is 0.485241 in 10.722m\n",
      "Score for blended models is 0.488036 in 12.028m\n",
      "Score for blended models is 0.494106 in 11.812m\n",
      "Score for blended models is 0.497430 in 11.701m\n",
      "Score for blended models is 0.488738 in 11.835m\n",
      "Score for blended models is 0.488840 in 10.818m\n",
      "Score for blended models is 0.488751 in 11.222m\n",
      "Score for blended models is 0.485352 in 11.776m\n",
      "Score for blended models is 0.487422 in 12.201m\n",
      "Score for blended models is 0.488496 in 11.910m\n",
      "Score for blended models is 0.492895 in 10.905m\n",
      "Score for blended models is 0.492939 in 11.125m\n",
      "Score for blended models is 0.492742 in 11.404m\n",
      "Score for blended models is 0.494560 in 10.739m\n",
      "Score for blended models is 0.483579 in 11.863m\n",
      "Score for blended models is 0.492790 in 12.167m\n",
      "Score for blended models is 0.498468 in 10.494m\n",
      "Score for blended models is 0.494187 in 10.220m\n",
      "Score for blended models is 0.488564 in 12.261m\n",
      "Score for blended models is 0.496554 in 11.720m\n",
      "Score for blended models is 0.491649 in 12.687m\n",
      "Score for blended models is 0.491798 in 12.114m\n",
      "Score for blended models is 0.488041 in 11.096m\n",
      "Score for blended models is 0.487135 in 10.467m\n",
      "Score for blended models is 0.496698 in 10.351m\n",
      "Score for blended models is 0.493442 in 10.897m\n",
      "Score for blended models is 0.492721 in 11.366m\n",
      "Score for blended models is 0.487229 in 12.124m\n",
      "Score for blended models is 0.487690 in 11.081m\n",
      "Score for blended models is 0.489018 in 11.115m\n",
      "Score for blended models is 0.484114 in 10.912m\n",
      "Score for blended models is 0.490185 in 11.195m\n",
      "Score for blended models is 0.483151 in 11.326m\n",
      "Score for blended models is 0.486665 in 11.950m\n",
      "Score for blended models is 0.490415 in 11.170m\n",
      "Score for blended models is 0.493017 in 10.224m\n",
      "Score for blended models is 0.494090 in 10.548m\n",
      "Score for blended models is 0.481805 in 12.431m\n",
      "Score for blended models is 0.488723 in 10.986m\n",
      "Score for blended models is 0.490039 in 12.142m\n",
      "Score for blended models is 0.486614 in 10.647m\n",
      "Score for blended models is 0.487606 in 11.830m\n",
      "Score for blended models is 0.480582 in 13.361m\n",
      "Score for blended models is 0.492260 in 11.887m\n",
      "Score for blended models is 0.492668 in 10.862m\n",
      "Score for blended models is 0.489590 in 10.018m\n",
      "Score for blended models is 0.488870 in 11.623m\n",
      "Score for blended models is 0.495422 in 11.787m\n",
      "Score for blended models is 0.489713 in 11.315m\n",
      "Score for blended models is 0.492431 in 12.589m\n",
      "Score for blended models is 0.496687 in 11.801m\n",
      "Score for blended models is 0.486196 in 12.077m\n",
      "Score for blended models is 0.490141 in 11.700m\n",
      "Score for blended models is 0.494904 in 10.563m\n",
      "Score for blended models is 0.493860 in 10.289m\n",
      "Score for blended models is 0.485220 in 12.029m\n",
      "Score for blended models is 0.486327 in 10.697m\n",
      "Score for blended models is 0.485020 in 12.308m\n",
      "Score for blended models is 0.491084 in 12.752m\n",
      "Score for blended models is 0.486593 in 12.493m\n",
      "Score for blended models is 0.488396 in 10.676m\n",
      "Score for blended models is 0.489734 in 10.145m\n",
      "Score for blended models is 0.491275 in 10.402m\n",
      "Score for blended models is 0.487373 in 11.831m\n",
      "Score for blended models is 0.494975 in 12.392m\n",
      "Score for blended models is 0.492728 in 12.190m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_df_nn.shape[0],3))\n",
    "test_total = np.zeros((test_df_nn.shape[0],3))\n",
    "score_total = 0\n",
    "count = 100\n",
    "print 'Starting.........'\n",
    "for n in range(count):\n",
    "#     print n\n",
    "    nn_parameters = [\n",
    "        { 'input_size' :70 ,\n",
    "         'input_dim' : train_X.shape[1],\n",
    "         'input_drop_out' : 0.4 ,\n",
    "         'hidden_size' : 20 ,\n",
    "         'hidden_drop_out' :0.4},\n",
    "\n",
    "    ]\n",
    "\n",
    "    (train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_df_nn, train_y, test_df_nn,\n",
    "                                                             5,\n",
    "                                                             10,128,n+500)\n",
    "    train_total += train_blend_x\n",
    "    test_total += test_blend_x\n",
    "    score_total += np.mean(blend_scores)\n",
    "    \n",
    "    name_train_blend = '../tmp/train_2nd.csv'\n",
    "    name_test_blend = '../tmp/test_2nd.csv'\n",
    "\n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.62490442e-01,   5.84023116e-01,   2.53486442e-01],\n",
       "       [  9.95380871e-01,   4.23049504e-03,   3.88634972e-04],\n",
       "       [  9.72845538e-01,   2.58905193e-02,   1.26394330e-03],\n",
       "       ..., \n",
       "       [  9.81898498e-01,   1.67049272e-02,   1.39657408e-03],\n",
       "       [  9.58855166e-01,   3.91550019e-02,   1.98983122e-03],\n",
       "       [  5.60936439e-01,   3.97504595e-01,   4.15589648e-02]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_2ndKeras_last_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_total)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49272779]\n",
      "[ 38.4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_2ndKeras_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_2ndKeras_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores,axis=0))\n",
    "print (np.mean(best_round,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
