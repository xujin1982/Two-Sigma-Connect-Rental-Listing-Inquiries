{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy import sparse\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing, model_selection\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "# from scipy.stats import boxcox\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (49352, 15)\n",
      "test shape: (74659, 14)\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "train = pd.read_json(open(\"../input/train.json\", \"r\"))\n",
    "test = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "print 'train shape:',train.shape\n",
    "print 'test shape:',test.shape\n",
    "\n",
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "target = train['interest_level'].apply(lambda x: y_map[x]).values\n",
    "\n",
    "# ntrain = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "tmp = pd.concat([train['price'],test['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "tmp.ix[tmp>ulimit] = ulimit\n",
    "\n",
    "train['price'].ix[train['price']>ulimit] = ulimit\n",
    "train.loc[:,'sc_price'] = (train['price'] - tmp.mean()) / tmp.std()\n",
    "\n",
    "test['price'].ix[test['price']>ulimit] = ulimit\n",
    "test.loc[:,'sc_price'] = (test['price'] - tmp.mean()) / tmp.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train['bathrooms'],test['bathrooms']])\n",
    "ulimit = 5\n",
    "tmp.ix[tmp> ulimit] = ulimit\n",
    "\n",
    "\n",
    "train['bathrooms'].ix[train['bathrooms']>ulimit] = ulimit\n",
    "train.loc[:,'sc_bathrooms'] = (train['bathrooms'] - tmp.mean()) / tmp.std()\n",
    "\n",
    "test['bathrooms'].ix[test['bathrooms']>ulimit] = ulimit\n",
    "test.loc[:,'sc_bathrooms'] = (test['bathrooms'] - tmp.mean()) / tmp.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train['bedrooms'],test['bedrooms']])\n",
    "ulimit = 7\n",
    "tmp.ix[tmp> ulimit] = ulimit\n",
    "\n",
    "\n",
    "train['bedrooms'].ix[train['bedrooms']>ulimit] = ulimit\n",
    "train.loc[:,'sc_bedrooms'] = (train['bedrooms'] - tmp.mean()) / tmp.std()\n",
    "\n",
    "test['bedrooms'].ix[test['bedrooms']>ulimit] = ulimit\n",
    "test.loc[:,'sc_bedrooms'] = (test['bedrooms'] - tmp.mean()) / tmp.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitude & Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train['longitude'],test['longitude']])\n",
    "llimit = np.percentile(tmp.values, 1)\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "tmp.ix[tmp > ulimit] = ulimit\n",
    "tmp.ix[tmp < llimit] = llimit\n",
    "\n",
    "\n",
    "train['longitude'].ix[train['longitude']>ulimit] = ulimit\n",
    "train['longitude'].ix[train['longitude']<llimit] = llimit\n",
    "train.loc[:,'sc_longitude'] = (train['longitude'] - tmp.mean()) / tmp.std()\n",
    "\n",
    "test['longitude'].ix[test['longitude']>ulimit] = ulimit\n",
    "test['longitude'].ix[test['longitude']<llimit] = llimit\n",
    "test.loc[:,'sc_longitude'] = (test['longitude'] - tmp.mean()) / tmp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train['latitude'],test['latitude']])\n",
    "llimit = np.percentile(tmp.values, 1)\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "tmp.ix[tmp > ulimit] = ulimit\n",
    "tmp.ix[tmp < llimit] = llimit\n",
    "\n",
    "\n",
    "train['latitude'].ix[train['latitude']>ulimit] = ulimit\n",
    "train['latitude'].ix[train['latitude']<llimit] = llimit\n",
    "train.loc[:,'sc_latitude'] = (train['latitude'] - tmp.mean()) / tmp.std()\n",
    "\n",
    "test['latitude'].ix[test['latitude']>ulimit] = ulimit\n",
    "test['latitude'].ix[test['latitude']<llimit] = llimit\n",
    "test.loc[:,'sc_latitude'] = (test['latitude'] - tmp.mean()) / tmp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use = ['sc_price','sc_bathrooms','sc_bedrooms','sc_longitude', 'sc_latitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train[f].dtype=='object':\n",
    "            lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "            train[f+'_lbl'] = lbl.transform(list(train[f].values))\n",
    "            test[f+'_lbl'] = lbl.transform(list(test[f].values))\n",
    "            features_to_use.append(f+'_lbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"created\"] = pd.to_datetime(train[\"created\"])\n",
    "test[\"created\"] = pd.to_datetime(test[\"created\"])\n",
    "\n",
    "train[\"created_month\"] = train[\"created\"].dt.month\n",
    "test[\"created_month\"] = test[\"created\"].dt.month\n",
    "\n",
    "train['created_weekday'] = train[\"created\"].dt.weekday\n",
    "test['created_weekday'] = test[\"created\"].dt.weekday\n",
    "\n",
    "train[\"created_day\"] = train[\"created\"].dt.day\n",
    "test[\"created_day\"] = test[\"created\"].dt.day\n",
    "\n",
    "train['created_hour'] = train[\"created\"].dt.hour\n",
    "test['created_hour'] = test[\"created\"].dt.hour\n",
    "\n",
    "\n",
    "features_to_use.extend([\"created_month\",'created_weekday',\"created_day\",'created_hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description & Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count of \"features\" #\n",
    "train[\"num_features\"] = train[\"features\"].apply(len)\n",
    "test[\"num_features\"] = test[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train['description'] = train['description'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "train['description'] = train['description'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "test['description'] = test['description'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "test['description'] = test['description'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "\n",
    "string.punctuation.__add__('!!')\n",
    "string.punctuation.__add__('(')\n",
    "string.punctuation.__add__(')')\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "train['description'] = train['description'].apply(lambda x: x.translate(remove_punct_map))\n",
    "test['description'] = test['description'].apply(lambda x: x.translate(remove_punct_map))\n",
    "\n",
    "train['num_description_letter'] = train['description'].apply(lambda x: len(x.strip()))\n",
    "train['num_description_words'] = train['description'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "test['num_description_letter'] = test['description'].apply(lambda x: len(x.strip()))\n",
    "test['num_description_words'] = test['description'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "\n",
    "\n",
    "features_to_use.extend([\"num_features\",\"num_description_words\", 'num_description_letter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['features'] = train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test['features'] = test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 200\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=num)\n",
    "c_vect.fit(pd.concat([train['features'],test['features']]))\n",
    "tr_sparse = c_vect.fit_transform(train[\"features\"])\n",
    "te_sparse = c_vect.transform(test[\"features\"])\n",
    "sparse_cols = c_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_test['desc'] = train_test['description']\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "# string.punctuation.__add__('!!')\n",
    "# string.punctuation.__add__('(')\n",
    "# string.punctuation.__add__(')')\n",
    "\n",
    "# remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "# train_test['desc_letters_count'] = train_test['description'].apply(lambda x: len(x.strip()))\n",
    "# train_test['desc_words_count'] = train_test['desc'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "\n",
    "# train_test.drop(['description', 'desc'], axis=1, inplace=True)\n",
    "# print 'train_test shape: ', train_test.shape\n",
    "\n",
    "# train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "# train_test['features2'] = train_test['features']\n",
    "# train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "# c_vect.fit(train_test['features2'])\n",
    "\n",
    "# c_vect_sparse_1 = c_vect.transform(train_test['features2'])\n",
    "# c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "# train_test.drop(['features', 'features2'], axis=1, inplace=True)\n",
    "# print 'train_test shape: ', train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_test['desc'] = train_test['description']\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.replace('!<br /><br />', ''))\n",
    "\n",
    "# string.punctuation.__add__('!!')\n",
    "# string.punctuation.__add__('(')\n",
    "# string.punctuation.__add__(')')\n",
    "\n",
    "# remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "# train_test['desc'] = train_test['desc'].apply(lambda x: x.translate(remove_punct_map))\n",
    "\n",
    "\n",
    "# train_test['features']=train_test['features'].apply(lambda x: ', '.join(x))\n",
    "# import nltk\n",
    "# from nltk.tag import pos_tag\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "# def cleaning_text(sentence):\n",
    "#    sentence=sentence.lower()\n",
    "#    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "#    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "#    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "#    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "#    #selecting only nouns and adjectives\n",
    "#    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "#    cleaned=cleaned.strip()\n",
    "#    return cleaned\n",
    " \n",
    "# train_test['desc_cleaned']= train_test['desc'].apply(lambda x: cleaning_text(x))\n",
    "# train_test['feat_cleaned']= train_test['features'].apply(lambda x: cleaning_text(x))\n",
    "# train_test[\"final_feat\"] = train_test[\"desc_cleaned\"].map(str) +\" \"+train_test[\"feat_cleaned\"]\n",
    "\n",
    "# c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "# c_vect.fit(train_test['final_feat'])\n",
    "\n",
    "# c_vect_sparse_1 = c_vect.transform(train_test['final_feat'])\n",
    "# c_vect_sparse1_cols = c_vect.get_feature_names()\n",
    "\n",
    "\n",
    "# train_test.drop(['description', 'desc','desc_cleaned'], axis=1, inplace=True)\n",
    "# train_test.drop(['features','feat_cleaned','final_feat'], axis=1, inplace=True)\n",
    "# print 'train_test shape: ', train_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['address1'] = train['display_address']\n",
    "train['address1'] = train['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "test['address1'] = test['display_address']\n",
    "test['address1'] = test['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "train['address1'] = train['address1'].apply(lambda x: x.translate(remove_punct_map))\n",
    "test['address1'] = test['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "for col in new_cols:\n",
    "    train[col] = train['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "    test[col] = test['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "train['other_address'] = train[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "test['other_address'] = test[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "features_to_use.extend(['street', 'avenue', 'east', 'west', 'north', 'south','other_address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train[\"num_photos\"] = train[\"photos\"].apply(len)\n",
    "test[\"num_photos\"] = test[\"photos\"].apply(len)\n",
    "\n",
    "features_to_use.append(\"num_photos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "# train_test.drop(['photos', 'display_address', 'street_address'], axis=1, inplace=True)\n",
    "# print 'train_test shape: ', train_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manager_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "temp = pd.concat([train.manager_id_lbl,pd.get_dummies(train['interest_level'])], axis = 1).groupby('manager_id_lbl').mean()\n",
    "temp.columns = ['high_frac','low_frac', 'medium_frac']\n",
    "temp['count'] = train.groupby('manager_id_lbl').count().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute skill\n",
    "temp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\n",
    "\n",
    "# get ixes for unranked managers...\n",
    "unranked_managers_ixes = temp['count']<20\n",
    "# ... and ranked ones\n",
    "ranked_managers_ixes = ~unranked_managers_ixes\n",
    "\n",
    "# compute mean values from ranked managers and assign them to unranked ones\n",
    "mean_values = temp.loc[~unranked_managers_ixes, ['high_frac','low_frac', 'medium_frac','manager_skill']].mean()\n",
    "# print(mean_values)\n",
    "mean_count = np.mean(temp.loc[unranked_managers_ixes,'count'])\n",
    "temp.loc[unranked_managers_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\n",
    "# print(temp.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inner join to assign manager features to the managers in the training dataframe\n",
    "train = train.merge(temp.reset_index(),how='left', left_on='manager_id_lbl', right_on='manager_id_lbl')\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(['high_frac','low_frac', 'medium_frac','manager_skill','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inner join to assign manager features to the managers in the testing dataframe\n",
    "test = test.merge(temp.reset_index(),how='left', left_on='manager_id_lbl', right_on='manager_id_lbl')\n",
    "index = test['manager_skill'].index[test['manager_skill'].apply(np.isnan)]\n",
    "test.loc[index,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\n",
    "test.loc[index,'count'] = int(mean_count)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manager = train[['manager_id','interest_level']]\n",
    "# interest_dummies = pd.get_dummies(manager.interest_level)\n",
    "# manager = pd.concat([manager,interest_dummies[['low','medium','high']]],\n",
    "#                     axis = 1).drop('interest_level', axis = 1)\n",
    "\n",
    "# manager_socre = pd.concat([manager.groupby('manager_id',as_index=False).mean(),\n",
    "#                            manager.groupby('manager_id',as_index=False).count()], \n",
    "#                           axis = 1).iloc[:,[0,1,2,3,5]]\n",
    "# manager_socre.columns = ['manager_id','low','medium','high','count']\n",
    "# manager_socre['manager_score'] = manager_socre['medium']*1 + manager_socre['low']*2 \n",
    "\n",
    "\n",
    "# train_test = train_test.merge(manager_socre[['manager_id','manager_score']],how = 'left')\n",
    "# print 'train_test shape: ', train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# managers_count = train_test['manager_id'].value_counts()\n",
    "\n",
    "# train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "# train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "# train_test['top_5_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "# train_test['top_50_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "# train_test['top_1_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "# train_test['top_2_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "# train_test['top_15_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "# train_test['top_20_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "# train_test['top_30_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "#     managers_count.values >= np.percentile(managers_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_test['Zero_building_id'] = train_test['building_id'].apply(lambda x: 1 if x == '0' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# buildings_count = train_test['building_id'].value_counts()\n",
    "\n",
    "# train_test['top_10_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "# train_test['top_25_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "# train_test['top_5_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "# train_test['top_50_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "# train_test['top_1_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "# train_test['top_2_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "# train_test['top_15_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "# train_test['top_20_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "# train_test['top_30_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "#     buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build = train[['building_id','interest_level']]\n",
    "# interest_dummies = pd.get_dummies(build.interest_level)\n",
    "# build = pd.concat([build,interest_dummies[['low','medium','high']]],\n",
    "#                   axis = 1).drop('interest_level', axis = 1)\n",
    "\n",
    "# build_socre = pd.concat([build.groupby('building_id',as_index=False).mean(),\n",
    "#                          build.groupby('building_id',as_index=False).count()], \n",
    "#                         axis = 1).iloc[:,[0,1,2,3,5]]\n",
    "# build_socre.columns = ['building_id','low','medium','high','count']\n",
    "# build_socre['building_score'] = build_socre['medium']*1 + build_socre['low']*2 \n",
    "\n",
    "\n",
    "\n",
    "# train_test = train_test.merge(build_socre[['building_id','building_score']],how = 'left')\n",
    "# print 'train_test shape: ', train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49352 entries, 0 to 49351\n",
      "Data columns (total 45 columns):\n",
      "bathrooms                 49352 non-null float64\n",
      "bedrooms                  49352 non-null int64\n",
      "building_id               49352 non-null object\n",
      "created                   49352 non-null datetime64[ns]\n",
      "description               49352 non-null object\n",
      "display_address           49352 non-null object\n",
      "features                  49352 non-null object\n",
      "interest_level            49352 non-null object\n",
      "latitude                  49352 non-null float64\n",
      "listing_id                49352 non-null int64\n",
      "longitude                 49352 non-null float64\n",
      "manager_id                49352 non-null object\n",
      "photos                    49352 non-null object\n",
      "price                     49352 non-null float64\n",
      "street_address            49352 non-null object\n",
      "sc_price                  49352 non-null float64\n",
      "sc_bathrooms              49352 non-null float64\n",
      "sc_bedrooms               49352 non-null float64\n",
      "sc_longitude              49352 non-null float64\n",
      "sc_latitude               49352 non-null float64\n",
      "display_address_lbl       49352 non-null int64\n",
      "manager_id_lbl            49352 non-null int64\n",
      "building_id_lbl           49352 non-null int64\n",
      "street_address_lbl        49352 non-null int64\n",
      "created_month             49352 non-null int64\n",
      "created_weekday           49352 non-null int64\n",
      "created_day               49352 non-null int64\n",
      "created_hour              49352 non-null int64\n",
      "num_features              49352 non-null int64\n",
      "num_description_letter    49352 non-null int64\n",
      "num_description_words     49352 non-null int64\n",
      "address1                  49352 non-null object\n",
      "street                    49352 non-null int64\n",
      "avenue                    49352 non-null int64\n",
      "east                      49352 non-null int64\n",
      "west                      49352 non-null int64\n",
      "north                     49352 non-null int64\n",
      "south                     49352 non-null int64\n",
      "other_address             49352 non-null int64\n",
      "num_photos                49352 non-null int64\n",
      "high_frac                 49352 non-null float64\n",
      "low_frac                  49352 non-null float64\n",
      "medium_frac               49352 non-null float64\n",
      "count                     49352 non-null int64\n",
      "manager_skill             49352 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(13), int64(22), object(9)\n",
      "memory usage: 17.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sc_price',\n",
       " 'sc_bathrooms',\n",
       " 'sc_bedrooms',\n",
       " 'sc_longitude',\n",
       " 'sc_latitude',\n",
       " 'display_address_lbl',\n",
       " 'manager_id_lbl',\n",
       " 'building_id_lbl',\n",
       " 'street_address_lbl',\n",
       " 'created_month',\n",
       " 'created_weekday',\n",
       " 'created_day',\n",
       " 'created_hour',\n",
       " 'num_features',\n",
       " 'num_description_words',\n",
       " 'num_description_letter',\n",
       " 'street',\n",
       " 'avenue',\n",
       " 'east',\n",
       " 'west',\n",
       " 'north',\n",
       " 'south',\n",
       " 'other_address',\n",
       " 'num_photos',\n",
       " 'high_frac',\n",
       " 'low_frac',\n",
       " 'medium_frac',\n",
       " 'manager_skill',\n",
       " 'count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = sparse.hstack([train[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test[features_to_use], te_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_feautres = '../input/featurestouse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_train_X = '../input/train_X_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_test_X = '../input/test_X_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_y = '../input/y_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_sparse = '../input/sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_train = '../input/train_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_test = '../input/test_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "\n",
    "pd.to_pickle(features_to_use,name_feautres)\n",
    "pd.to_pickle(train_X, name_train_X)\n",
    "pd.to_pickle(test_X, name_test_X)\n",
    "pd.to_pickle(target, name_y)\n",
    "pd.to_pickle(sparse_cols, name_sparse)\n",
    "pd.to_pickle(train, name_train)\n",
    "pd.to_pickle(test, name_test)\n",
    "\n",
    "name_tr_sparse = '../input/tr_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_te_sparse = '../input/te_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "pd.to_pickle(tr_sparse,name_tr_sparse)\n",
    "pd.to_pickle(te_sparse, name_te_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
