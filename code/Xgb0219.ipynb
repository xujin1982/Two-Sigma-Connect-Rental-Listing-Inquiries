{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 229) (74659, 229)\n"
     ]
    }
   ],
   "source": [
    "train_X = pd.read_pickle('../input/' + 'train_X_2017-02-16-20-51.pkl')\n",
    "test_X = pd.read_pickle('../input/' + 'test_X_2017-02-16-20-51.pkl')\n",
    "test_listing = pd.read_pickle('../input/' + 'listing_id.pkl')\n",
    "\n",
    "train_y = pd.read_pickle('../input/' + 'y_2017-02-16-20-51.pkl') \n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09181+1.54583e-05\ttest-mlogloss:1.09193+1.43889e-05\n",
      "[25]\ttrain-mlogloss:0.952515+0.000228609\ttest-mlogloss:0.955351+0.00052189\n",
      "[50]\ttrain-mlogloss:0.855399+0.000530515\ttest-mlogloss:0.860855+0.00083777\n",
      "[75]\ttrain-mlogloss:0.78536+0.000644246\ttest-mlogloss:0.793402+0.0010361\n",
      "[100]\ttrain-mlogloss:0.733626+0.00088164\ttest-mlogloss:0.744086+0.00134885\n",
      "[125]\ttrain-mlogloss:0.694538+0.000931174\ttest-mlogloss:0.707257+0.00148517\n",
      "[150]\ttrain-mlogloss:0.664556+0.000966226\ttest-mlogloss:0.679356+0.0018101\n",
      "[175]\ttrain-mlogloss:0.640918+0.00106951\ttest-mlogloss:0.657685+0.00197618\n",
      "[200]\ttrain-mlogloss:0.622062+0.00115715\ttest-mlogloss:0.640714+0.00218976\n",
      "[225]\ttrain-mlogloss:0.606891+0.00118513\ttest-mlogloss:0.627324+0.00248528\n",
      "[250]\ttrain-mlogloss:0.594301+0.00127697\ttest-mlogloss:0.616446+0.00260129\n",
      "[275]\ttrain-mlogloss:0.583457+0.00122491\ttest-mlogloss:0.607283+0.00282843\n",
      "[300]\ttrain-mlogloss:0.574255+0.00126443\ttest-mlogloss:0.599781+0.00297424\n",
      "[325]\ttrain-mlogloss:0.566379+0.00129274\ttest-mlogloss:0.593535+0.00312574\n",
      "[350]\ttrain-mlogloss:0.559373+0.00132019\ttest-mlogloss:0.588229+0.00324428\n",
      "[375]\ttrain-mlogloss:0.553127+0.00133559\ttest-mlogloss:0.583635+0.00331901\n",
      "[400]\ttrain-mlogloss:0.547511+0.00132125\ttest-mlogloss:0.579639+0.00343583\n",
      "[425]\ttrain-mlogloss:0.542464+0.00129952\ttest-mlogloss:0.57619+0.0035674\n",
      "[450]\ttrain-mlogloss:0.537802+0.00125784\ttest-mlogloss:0.573173+0.00366124\n",
      "[475]\ttrain-mlogloss:0.533378+0.00129893\ttest-mlogloss:0.570421+0.00362252\n",
      "[500]\ttrain-mlogloss:0.529312+0.00136102\ttest-mlogloss:0.567965+0.00360568\n",
      "[525]\ttrain-mlogloss:0.525371+0.00125416\ttest-mlogloss:0.5657+0.00370979\n",
      "[550]\ttrain-mlogloss:0.521734+0.00130792\ttest-mlogloss:0.563735+0.00374196\n",
      "[575]\ttrain-mlogloss:0.518153+0.00130116\ttest-mlogloss:0.561873+0.00380459\n",
      "[600]\ttrain-mlogloss:0.514682+0.00132281\ttest-mlogloss:0.560119+0.00381335\n",
      "[625]\ttrain-mlogloss:0.511449+0.00128464\ttest-mlogloss:0.558558+0.00387424\n",
      "[650]\ttrain-mlogloss:0.508298+0.0012954\ttest-mlogloss:0.557102+0.00392408\n",
      "[675]\ttrain-mlogloss:0.505306+0.00130601\ttest-mlogloss:0.555766+0.00394674\n",
      "[700]\ttrain-mlogloss:0.502372+0.00128022\ttest-mlogloss:0.554473+0.00400872\n",
      "[725]\ttrain-mlogloss:0.499535+0.00124541\ttest-mlogloss:0.553273+0.0040947\n",
      "[750]\ttrain-mlogloss:0.496765+0.00127245\ttest-mlogloss:0.552112+0.00409177\n",
      "[775]\ttrain-mlogloss:0.494131+0.0012925\ttest-mlogloss:0.551031+0.00409671\n",
      "[800]\ttrain-mlogloss:0.491614+0.00128724\ttest-mlogloss:0.550056+0.00410121\n",
      "[825]\ttrain-mlogloss:0.48913+0.00132343\ttest-mlogloss:0.549137+0.00413661\n",
      "[850]\ttrain-mlogloss:0.48668+0.00127686\ttest-mlogloss:0.548243+0.00419335\n",
      "[875]\ttrain-mlogloss:0.484297+0.00132018\ttest-mlogloss:0.547448+0.00423049\n",
      "[900]\ttrain-mlogloss:0.481962+0.00127538\ttest-mlogloss:0.546681+0.00427105\n",
      "[925]\ttrain-mlogloss:0.479601+0.00131931\ttest-mlogloss:0.54591+0.00431206\n",
      "[950]\ttrain-mlogloss:0.477392+0.00129947\ttest-mlogloss:0.54522+0.00437054\n",
      "[975]\ttrain-mlogloss:0.47511+0.0013106\ttest-mlogloss:0.544547+0.00442709\n",
      "[1000]\ttrain-mlogloss:0.472937+0.00136453\ttest-mlogloss:0.543909+0.00443403\n",
      "[1025]\ttrain-mlogloss:0.470778+0.00134382\ttest-mlogloss:0.543318+0.00449348\n",
      "[1050]\ttrain-mlogloss:0.468689+0.00134126\ttest-mlogloss:0.54274+0.00453604\n",
      "[1075]\ttrain-mlogloss:0.466575+0.00131118\ttest-mlogloss:0.542195+0.00455175\n",
      "[1100]\ttrain-mlogloss:0.464426+0.00133167\ttest-mlogloss:0.541626+0.00459712\n",
      "[1125]\ttrain-mlogloss:0.462306+0.00137274\ttest-mlogloss:0.541122+0.00464193\n",
      "[1150]\ttrain-mlogloss:0.460259+0.00138304\ttest-mlogloss:0.540639+0.00466317\n",
      "[1175]\ttrain-mlogloss:0.458257+0.00134052\ttest-mlogloss:0.54016+0.00471529\n",
      "[1200]\ttrain-mlogloss:0.456255+0.00129123\ttest-mlogloss:0.539714+0.00473869\n",
      "[1225]\ttrain-mlogloss:0.454314+0.00129376\ttest-mlogloss:0.539294+0.00479489\n",
      "[1250]\ttrain-mlogloss:0.452364+0.0013321\ttest-mlogloss:0.538859+0.0048443\n",
      "[1275]\ttrain-mlogloss:0.45041+0.00126976\ttest-mlogloss:0.538449+0.00488589\n",
      "[1300]\ttrain-mlogloss:0.448485+0.00123984\ttest-mlogloss:0.538072+0.00490562\n",
      "[1325]\ttrain-mlogloss:0.446647+0.00124109\ttest-mlogloss:0.537741+0.00494371\n",
      "[1350]\ttrain-mlogloss:0.444806+0.00123933\ttest-mlogloss:0.537382+0.00495592\n",
      "[1375]\ttrain-mlogloss:0.442907+0.00125677\ttest-mlogloss:0.537042+0.0050042\n",
      "[1400]\ttrain-mlogloss:0.441109+0.00126358\ttest-mlogloss:0.53671+0.00501342\n",
      "[1425]\ttrain-mlogloss:0.439356+0.00121493\ttest-mlogloss:0.536436+0.00502384\n",
      "[1450]\ttrain-mlogloss:0.437547+0.00121479\ttest-mlogloss:0.536155+0.00504604\n",
      "[1475]\ttrain-mlogloss:0.43577+0.00125714\ttest-mlogloss:0.535889+0.00507244\n",
      "[1500]\ttrain-mlogloss:0.434022+0.00121343\ttest-mlogloss:0.535617+0.00509312\n",
      "[1525]\ttrain-mlogloss:0.432306+0.0012081\ttest-mlogloss:0.535365+0.00509605\n",
      "[1550]\ttrain-mlogloss:0.43056+0.00118673\ttest-mlogloss:0.535084+0.00512976\n",
      "[1575]\ttrain-mlogloss:0.42887+0.00122448\ttest-mlogloss:0.534851+0.00514304\n",
      "[1600]\ttrain-mlogloss:0.427155+0.00123509\ttest-mlogloss:0.534604+0.00517971\n",
      "[1625]\ttrain-mlogloss:0.425531+0.00128667\ttest-mlogloss:0.534374+0.0052056\n",
      "[1650]\ttrain-mlogloss:0.423874+0.00129664\ttest-mlogloss:0.534126+0.00523389\n",
      "[1675]\ttrain-mlogloss:0.422241+0.00130323\ttest-mlogloss:0.533936+0.00524387\n",
      "[1700]\ttrain-mlogloss:0.420599+0.00132594\ttest-mlogloss:0.533754+0.00527668\n",
      "[1725]\ttrain-mlogloss:0.418965+0.00132383\ttest-mlogloss:0.533518+0.00529393\n",
      "[1750]\ttrain-mlogloss:0.417382+0.00131189\ttest-mlogloss:0.533352+0.00532039\n",
      "[1775]\ttrain-mlogloss:0.415831+0.00132419\ttest-mlogloss:0.533166+0.00532025\n",
      "[1800]\ttrain-mlogloss:0.414266+0.00130794\ttest-mlogloss:0.532982+0.00532148\n",
      "[1825]\ttrain-mlogloss:0.412712+0.00133489\ttest-mlogloss:0.532844+0.00535394\n",
      "[1850]\ttrain-mlogloss:0.411186+0.001305\ttest-mlogloss:0.532673+0.00537132\n",
      "[1875]\ttrain-mlogloss:0.409649+0.00128487\ttest-mlogloss:0.532489+0.00539094\n",
      "[1900]\ttrain-mlogloss:0.408096+0.00128282\ttest-mlogloss:0.532363+0.00541864\n",
      "[1925]\ttrain-mlogloss:0.40665+0.00130266\ttest-mlogloss:0.532229+0.00542852\n",
      "[1950]\ttrain-mlogloss:0.405199+0.00132788\ttest-mlogloss:0.532115+0.00546185\n",
      "[1975]\ttrain-mlogloss:0.403773+0.0012931\ttest-mlogloss:0.531987+0.00548738\n",
      "[2000]\ttrain-mlogloss:0.402277+0.00132891\ttest-mlogloss:0.531812+0.00550604\n",
      "[2025]\ttrain-mlogloss:0.400825+0.00138232\ttest-mlogloss:0.531701+0.00551575\n",
      "[2050]\ttrain-mlogloss:0.399354+0.00133061\ttest-mlogloss:0.531598+0.00551644\n",
      "[2075]\ttrain-mlogloss:0.397938+0.00133264\ttest-mlogloss:0.531485+0.00554144\n",
      "[2100]\ttrain-mlogloss:0.396538+0.00133554\ttest-mlogloss:0.531372+0.00557353\n",
      "[2125]\ttrain-mlogloss:0.395116+0.00131861\ttest-mlogloss:0.531283+0.00557302\n",
      "[2150]\ttrain-mlogloss:0.393719+0.00127178\ttest-mlogloss:0.531194+0.00561765\n",
      "[2175]\ttrain-mlogloss:0.392376+0.00124\ttest-mlogloss:0.531126+0.0056218\n",
      "[2200]\ttrain-mlogloss:0.391048+0.0012436\ttest-mlogloss:0.531045+0.00563659\n",
      "[2225]\ttrain-mlogloss:0.389642+0.00126777\ttest-mlogloss:0.530966+0.0056453\n",
      "[2250]\ttrain-mlogloss:0.388276+0.00131916\ttest-mlogloss:0.53087+0.00567968\n",
      "[2275]\ttrain-mlogloss:0.386936+0.00129171\ttest-mlogloss:0.530775+0.00570803\n",
      "[2300]\ttrain-mlogloss:0.385573+0.00129716\ttest-mlogloss:0.530711+0.00571918\n",
      "[2325]\ttrain-mlogloss:0.384214+0.00130898\ttest-mlogloss:0.530638+0.00577487\n",
      "[2350]\ttrain-mlogloss:0.382879+0.00128892\ttest-mlogloss:0.530574+0.00577932\n",
      "[2375]\ttrain-mlogloss:0.381585+0.00130595\ttest-mlogloss:0.530501+0.00580968\n",
      "[2400]\ttrain-mlogloss:0.380268+0.00131063\ttest-mlogloss:0.530484+0.00583171\n",
      "[2425]\ttrain-mlogloss:0.37895+0.00127996\ttest-mlogloss:0.530443+0.00585215\n",
      "[2450]\ttrain-mlogloss:0.377645+0.00127485\ttest-mlogloss:0.530394+0.00585834\n",
      "[2475]\ttrain-mlogloss:0.37633+0.00129801\ttest-mlogloss:0.530352+0.00586933\n",
      "[2500]\ttrain-mlogloss:0.375019+0.00129428\ttest-mlogloss:0.530306+0.00590097\n",
      "[2525]\ttrain-mlogloss:0.373747+0.00127595\ttest-mlogloss:0.530251+0.00590799\n",
      "[2550]\ttrain-mlogloss:0.372502+0.00125829\ttest-mlogloss:0.530214+0.00591254\n",
      "[2575]\ttrain-mlogloss:0.371207+0.00123706\ttest-mlogloss:0.530196+0.00595761\n",
      "[2600]\ttrain-mlogloss:0.369991+0.00122765\ttest-mlogloss:0.530149+0.00598247\n",
      "[2625]\ttrain-mlogloss:0.368735+0.00125189\ttest-mlogloss:0.530124+0.0060131\n",
      "[2650]\ttrain-mlogloss:0.36746+0.00128343\ttest-mlogloss:0.530118+0.00602202\n",
      "[2675]\ttrain-mlogloss:0.366234+0.00130175\ttest-mlogloss:0.53009+0.00605778\n",
      "[2700]\ttrain-mlogloss:0.364983+0.00136182\ttest-mlogloss:0.530072+0.00606501\n",
      "[2725]\ttrain-mlogloss:0.363771+0.00135085\ttest-mlogloss:0.530053+0.00607383\n",
      "[2750]\ttrain-mlogloss:0.362528+0.0014004\ttest-mlogloss:0.530036+0.00610654\n",
      "[2775]\ttrain-mlogloss:0.361266+0.0014088\ttest-mlogloss:0.530029+0.00613869\n",
      "[2800]\ttrain-mlogloss:0.360034+0.001374\ttest-mlogloss:0.530005+0.00615396\n",
      "[2825]\ttrain-mlogloss:0.358804+0.00136001\ttest-mlogloss:0.529994+0.00616175\n",
      "[2850]\ttrain-mlogloss:0.357572+0.00134728\ttest-mlogloss:0.530004+0.00617419\n",
      "[2875]\ttrain-mlogloss:0.356388+0.00132087\ttest-mlogloss:0.529993+0.00618372\n",
      "[2900]\ttrain-mlogloss:0.355248+0.00134912\ttest-mlogloss:0.529971+0.00620399\n",
      "[2925]\ttrain-mlogloss:0.354076+0.00136228\ttest-mlogloss:0.529979+0.00620518\n",
      "[2950]\ttrain-mlogloss:0.352884+0.00136438\ttest-mlogloss:0.529986+0.00623867\n"
     ]
    }
   ],
   "source": [
    "# SEED = 777\n",
    "# NFOLDS = 5\n",
    "\n",
    "# params = {\n",
    "#     'eta':.01,\n",
    "#     'colsample_bytree':.8,\n",
    "#     'subsample':.8,\n",
    "#     'seed':0,\n",
    "#     'nthread':8,\n",
    "#     'objective':'multi:softprob',\n",
    "#     'eval_metric':'mlogloss',\n",
    "#     'num_class':3,\n",
    "#     'silent':1\n",
    "# }\n",
    "\n",
    "# dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "# dtest = xgb.DMatrix(data=test_X)\n",
    "\n",
    "\n",
    "# bst = xgb.cv(params, dtrain, 10000, NFOLDS, early_stopping_rounds=50, verbose_eval=25)\n",
    "\n",
    "# best_rounds = np.argmin(bst['test-mlogloss-mean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5   0.535599\n",
      "6   0.537981\n",
      "7   0.539522\n",
      "8   0.541843\n",
      "9   0.54299\n",
      "10   0.548744\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "for x in [5,6,7,8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "    # print 'score    :\\t{}\\n'.format(rgr.best_score)\n",
    "\n",
    "    print rgr.get_xgb_params()['max_depth'], ' ', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130   0.538513\n",
      "140   0.537914\n",
      "150   0.539292\n",
      "160   0.539588\n",
      "170   0.540907\n",
      "180   0.541307\n",
      "190   0.542417\n"
     ]
    }
   ],
   "source": [
    "for x in [130,140,150,160,170,180,190]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "    # print 'score    :\\t{}\\n'.format(rgr.best_score)\n",
    "\n",
    "    print rgr.get_xgb_params()['min_child_weight'], ' ', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_weight = 10\n",
    "# 1   0.535599\n",
    "# 5   0.53421\n",
    "# 10   0.534681\n",
    "# 20   0.534445\n",
    "# 30   0.536634\n",
    "# 40   0.537307\n",
    "# 50   0.536874\n",
    "# 60   0.538\n",
    "# 70   0.537298\n",
    "# 80   0.538018\n",
    "# 90   0.537785\n",
    "# 100   0.536762\n",
    "# 110   0.538405\n",
    "# 120   0.536963\n",
    "# 130   0.538513\n",
    "# 140   0.537914\n",
    "# 150   0.539292\n",
    "# 160   0.539588\n",
    "# 170   0.540907\n",
    "# 180   0.541307\n",
    "# 190   0.542417\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3   0.533581\n",
      "0.4   0.536133\n",
      "0.5   0.533623\n",
      "0.6   0.533882\n",
      "0.7   0.534225\n",
      "0.8   0.532032\n",
      "0.9   0.533586\n",
      "1.0   0.534681\n"
     ]
    }
   ],
   "source": [
    "for x in [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "    # print 'score    :\\t{}\\n'.format(rgr.best_score)\n",
    "\n",
    "    print rgr.get_xgb_params()['colsample_bytree'], ' ', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5   0.534732\n",
      "0.6   0.535824\n",
      "0.7   0.53323\n",
      "0.8   0.531295\n",
      "0.9   0.53185\n",
      "1.0   0.532032\n"
     ]
    }
   ],
   "source": [
    "for x in [0.5,0.6,0.7,0.8,0.9,1.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "    # print 'score    :\\t{}\\n'.format(rgr.best_score)\n",
    "\n",
    "    print rgr.get_xgb_params()['subsample'], ' ', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.531295\n",
      "0.3   0.532525\n",
      "0.6   0.533081\n",
      "0.9   0.533356\n",
      "1.2   0.532935\n",
      "1.5   0.533338\n",
      "1.8   0.533093\n",
      "2.1   0.532699\n",
      "2.4   0.533399\n",
      "2.7   0.531466\n",
      "3.0   0.532402\n"
     ]
    }
   ],
   "source": [
    "for x in [0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # print 'max_depth:\\t{}\\n'.format(rgr.get_xgb_params()['max_depth'])\n",
    "    # print 'score    :\\t{}\\n'.format(rgr.best_score)\n",
    "\n",
    "    print rgr.get_xgb_params()['gamma'], ' ', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-ee6369812d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mxgb_BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0my_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-ee6369812d95>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(min_child_weight, colsample_bytree, max_depth, subsample, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mlogloss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    398\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (3,7),\n",
    "        'min_child_weight': (1,50),\n",
    "        'colsample_bytree': (0.7,1),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0,3)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.884599</td>\n",
       "      <td>12.897203</td>\n",
       "      <td>0.803018</td>\n",
       "      <td>0.960795</td>\n",
       "      <td>2.690196</td>\n",
       "      <td>-0.530282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.126611</td>\n",
       "      <td>2.067154</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>2.751725</td>\n",
       "      <td>-0.530455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.126611</td>\n",
       "      <td>2.067154</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>2.751725</td>\n",
       "      <td>-0.530455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.126611</td>\n",
       "      <td>2.067160</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>2.751725</td>\n",
       "      <td>-0.530455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.126611</td>\n",
       "      <td>2.067154</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>2.751725</td>\n",
       "      <td>-0.530455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.897646</td>\n",
       "      <td>12.958748</td>\n",
       "      <td>0.736776</td>\n",
       "      <td>0.947351</td>\n",
       "      <td>2.677209</td>\n",
       "      <td>-0.530621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.897647</td>\n",
       "      <td>12.958754</td>\n",
       "      <td>0.736769</td>\n",
       "      <td>0.947350</td>\n",
       "      <td>2.677208</td>\n",
       "      <td>-0.530917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.698249</td>\n",
       "      <td>7.818157</td>\n",
       "      <td>0.838006</td>\n",
       "      <td>0.930783</td>\n",
       "      <td>2.668471</td>\n",
       "      <td>-0.530937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.092796</td>\n",
       "      <td>25.284920</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.949351</td>\n",
       "      <td>2.942120</td>\n",
       "      <td>-0.530973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.441614</td>\n",
       "      <td>1.988824</td>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.791367</td>\n",
       "      <td>2.169951</td>\n",
       "      <td>-0.531241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "22   5.884599         12.897203          0.803018   0.960795  2.690196   \n",
       "12   4.126611          2.067154          0.828660   0.821156  2.751725   \n",
       "3    4.126611          2.067154          0.828660   0.821156  2.751725   \n",
       "14   4.126611          2.067160          0.828660   0.821156  2.751725   \n",
       "13   4.126611          2.067154          0.828660   0.821156  2.751725   \n",
       "16   5.897646         12.958748          0.736776   0.947351  2.677209   \n",
       "10   5.897647         12.958754          0.736769   0.947350  2.677208   \n",
       "18   4.698249          7.818157          0.838006   0.930783  2.668471   \n",
       "0    6.092796         25.284920          0.932358   0.949351  2.942120   \n",
       "1    5.441614          1.988824          0.976038   0.791367  2.169951   \n",
       "\n",
       "       score  \n",
       "22 -0.530282  \n",
       "12 -0.530455  \n",
       "3  -0.530455  \n",
       "14 -0.530455  \n",
       "13 -0.530455  \n",
       "16 -0.530621  \n",
       "10 -0.530917  \n",
       "18 -0.530937  \n",
       "0  -0.530973  \n",
       "1  -0.531241  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(params, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(params)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((N_params))\n",
    "    \n",
    "    for j, param in enumerate(params):\n",
    "        param['objective']='multi:softprob'\n",
    "        param['eval_metric']='mlogloss',\n",
    "        param['num_class']=3\n",
    "        param['silent']= False\n",
    "        param['eta'] = 0.03\n",
    "#         param['verbose_eval'] = 10  \n",
    "        print (\"Model %d:\" %(j+1))\n",
    "        \n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "        cv_result = xgb.cv(param, xgtrain,\n",
    "                           num_boost_round=10000, nfold=fold,\n",
    "                           metrics = 'mlogloss',\n",
    "                           seed=seed,callbacks=[xgb.callback.early_stop(early_stopping_rounds)])    \n",
    "        best_round = cv_result.shape[0] - 1\n",
    "        print 'best_round',best_round\n",
    "        best_rounds[j]=best_round\n",
    "        \n",
    "        param.pop('eval_metric')\n",
    "        all_round = best_round / (1 - 1. / fold)\n",
    "        est_test_blend = xgb.train(param, xgtrain,num_boost_round=int(all_round))\n",
    "\n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = est_test_blend.predict(xgb.DMatrix(test_x))\n",
    "\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "            xgtrain_fold = xgb.DMatrix(train_x_fold, label=train_y_fold)\n",
    "            est_train_blend = xgb.train(param, xgtrain_fold,num_boost_round=best_round)\n",
    "            \n",
    "            val_y_predict_fold = est_train_blend.predict(xgb.DMatrix(val_x_fold))\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 5 folds\n",
      "Model 1:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2794]\ttrain-mlogloss:0.402639+0.00175193\ttest-mlogloss:0.52947+0.0068331\n",
      "\n",
      "best_round 2794\n",
      "Model 1 fold 1\n",
      "('Score: ', 0.53690951454152624)\n",
      "Model 1 fold 1 fitting finished in 388.677s\n",
      "Model 1 fold 2\n",
      "('Score: ', 0.52401377183494546)\n",
      "Model 1 fold 2 fitting finished in 368.404s\n",
      "Model 1 fold 3\n",
      "('Score: ', 0.52100373678403)\n",
      "Model 1 fold 3 fitting finished in 370.817s\n",
      "Model 1 fold 4\n",
      "('Score: ', 0.53049319689493357)\n",
      "Model 1 fold 4 fitting finished in 381.897s\n",
      "Model 1 fold 5\n",
      "('Score: ', 0.53938186191760162)\n",
      "Model 1 fold 5 fitting finished in 369.014s\n",
      "Score for model 1 is 0.530360\n",
      "Model 2:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4134]\ttrain-mlogloss:0.406191+0.0017308\ttest-mlogloss:0.528614+0.00698858\n",
      "\n",
      "best_round 4134\n",
      "Model 2 fold 1\n",
      "('Score: ', 0.53499674020953703)\n",
      "Model 2 fold 1 fitting finished in 470.712s\n",
      "Model 2 fold 2\n",
      "('Score: ', 0.52347642606212919)\n",
      "Model 2 fold 2 fitting finished in 467.585s\n",
      "Model 2 fold 3\n",
      "('Score: ', 0.52123565754676715)\n",
      "Model 2 fold 3 fitting finished in 471.910s\n",
      "Model 2 fold 4\n",
      "('Score: ', 0.52919637660112151)\n",
      "Model 2 fold 4 fitting finished in 465.676s\n",
      "Model 2 fold 5\n",
      "('Score: ', 0.53855599699212375)\n",
      "Model 2 fold 5 fitting finished in 463.869s\n",
      "Score for model 2 is 0.529492\n",
      "Model 3:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2798]\ttrain-mlogloss:0.401569+0.00145798\ttest-mlogloss:0.529421+0.00698544\n",
      "\n",
      "best_round 2798\n",
      "Model 3 fold 1\n",
      "('Score: ', 0.536117620845598)\n",
      "Model 3 fold 1 fitting finished in 359.428s\n",
      "Model 3 fold 2\n",
      "('Score: ', 0.5240328961498335)\n",
      "Model 3 fold 2 fitting finished in 353.661s\n",
      "Model 3 fold 3\n",
      "('Score: ', 0.52079351210868363)\n",
      "Model 3 fold 3 fitting finished in 353.691s\n",
      "Model 3 fold 4\n",
      "('Score: ', 0.53039061643612806)\n",
      "Model 3 fold 4 fitting finished in 351.004s\n",
      "Model 3 fold 5\n",
      "('Score: ', 0.53942949776848848)\n",
      "Model 3 fold 5 fitting finished in 353.133s\n",
      "Score for model 3 is 0.530153\n",
      "Model 4:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3053]\ttrain-mlogloss:0.397388+0.00134218\ttest-mlogloss:0.52934+0.00677767\n",
      "\n",
      "best_round 3053\n",
      "Model 4 fold 1\n",
      "('Score: ', 0.53621563328988575)\n",
      "Model 4 fold 1 fitting finished in 385.172s\n",
      "Model 4 fold 2\n",
      "('Score: ', 0.52434769352526067)\n",
      "Model 4 fold 2 fitting finished in 386.609s\n",
      "Model 4 fold 3\n",
      "('Score: ', 0.5206656580207033)\n",
      "Model 4 fold 3 fitting finished in 383.553s\n",
      "Model 4 fold 4\n",
      "('Score: ', 0.53029782320598695)\n",
      "Model 4 fold 4 fitting finished in 387.185s\n",
      "Model 4 fold 5\n",
      "('Score: ', 0.53935659205712205)\n",
      "Model 4 fold 5 fitting finished in 384.748s\n",
      "Score for model 4 is 0.530177\n",
      "Model 5:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4475]\ttrain-mlogloss:0.410011+0.00157575\ttest-mlogloss:0.529223+0.00687145\n",
      "\n",
      "best_round 4475\n",
      "Model 5 fold 1\n",
      "('Score: ', 0.53647045149755246)\n",
      "Model 5 fold 1 fitting finished in 505.903s\n",
      "Model 5 fold 2\n",
      "('Score: ', 0.52402995841739042)\n",
      "Model 5 fold 2 fitting finished in 501.831s\n",
      "Model 5 fold 3\n",
      "('Score: ', 0.52091174658316675)\n",
      "Model 5 fold 3 fitting finished in 502.451s\n",
      "Model 5 fold 4\n",
      "('Score: ', 0.53023943255902695)\n",
      "Model 5 fold 4 fitting finished in 498.795s\n",
      "Model 5 fold 5\n",
      "('Score: ', 0.53894120855110583)\n",
      "Model 5 fold 5 fitting finished in 499.818s\n",
      "Score for model 5 is 0.530119\n",
      "Score for blended models is 0.530060\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_params = [{'max_depth':5,\n",
    "               'min_child_weight':12,\n",
    "               'colsample_bytree':0.803018,\n",
    "               'subsample':0.960795,\n",
    "               'gamma':2.690196},\n",
    "#               score -0.530282              \n",
    "\n",
    "              {'max_depth':4,\n",
    "               'min_child_weight':2,\n",
    "               'colsample_bytree':0.828660,\n",
    "               'subsample':0.821156,\n",
    "               'gamma':2.751725},\n",
    "#               score -0.530455\n",
    "          \n",
    "              {'max_depth':5,\n",
    "               'min_child_weight':12,\n",
    "               'colsample_bytree':0.736776,\n",
    "               'subsample':0.947351,\n",
    "               'gamma':2.677209},\n",
    "#               score -0.530621    \n",
    "              \n",
    "              {'max_depth':5,\n",
    "               'min_child_weight':12,\n",
    "               'colsample_bytree':0.736769,\n",
    "               'subsample':0.947350,\n",
    "               'gamma':2.677208},\n",
    "#               score -0.530917                 \n",
    "\n",
    "              {'max_depth':4,\n",
    "               'min_child_weight':7,\n",
    "               'colsample_bytree':0.838006,\n",
    "               'subsample':0.930783,\n",
    "               'gamma':2.668471}\n",
    "#               score -0.530937\n",
    "             ]\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(xgb_params,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              5,\n",
    "                              300)\n",
    "\n",
    "# print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "# print (np.mean(best_rounds_xgb_le,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2794.,  4134.,  2798.,  3053.,  4475.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rounds_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53036042  0.52949224  0.53015283  0.53017668  0.53011856]\n",
      "3450.8\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_xgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13749345,  0.49183246,  0.37067413],\n",
       "       [ 0.0594456 ,  0.09174278,  0.84881157],\n",
       "       [ 0.02579494,  0.27639574,  0.69780934],\n",
       "       ..., \n",
       "       [ 0.13193671,  0.36944345,  0.49861979],\n",
       "       [ 0.40039733,  0.48122504,  0.11837757],\n",
       "       [ 0.02176894,  0.17235944,  0.80587167]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb[:,:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_listing\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_blend_x_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5596b6023bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_blend_x_xgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_blend_x_xgb' is not defined"
     ]
    }
   ],
   "source": [
    "test_blend_x_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
