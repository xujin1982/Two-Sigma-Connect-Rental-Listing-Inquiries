{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../input/\"\n",
    "\n",
    "train_df = pd.read_pickle(data_path + 'train_2017-03-05-22-40.pkl')\n",
    "train_y = pd.read_pickle(data_path + 'y_2017-03-05-22-40.pkl')\n",
    "test_df = pd.read_pickle(data_path + 'test_2017-03-05-22-40.pkl')\n",
    "features_to_use = pd.read_pickle(data_path + 'featurestouse_2017-03-05-22-40.pkl')\n",
    "\n",
    "tr_desc_sparse = pd.read_pickle(data_path + 'tr_desc_sparse_2017-03-05-22-40.pkl')\n",
    "tr_feat_sparse = pd.read_pickle(data_path + 'tr_feat_sparse_2017-03-05-22-40.pkl')\n",
    "te_desc_sparse = pd.read_pickle(data_path + 'te_desc_sparse_2017-03-05-22-40.pkl')\n",
    "te_feat_sparse = pd.read_pickle(data_path + 'te_feat_sparse_2017-03-05-22-40.pkl')\n",
    "\n",
    "desc_sparse_cols = pd.read_pickle(data_path + 'desc_sparse_cols_2017-03-05-22-40.pkl')\n",
    "feat_sparse_cols = pd.read_pickle(data_path + 'feat_sparse_cols_2017-03-05-22-40.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 457) (74659, 457)\n"
     ]
    }
   ],
   "source": [
    "train_X = pd.read_pickle(data_path + 'train_y_0319.pkl')\n",
    "test_X = pd.read_pickle(data_path + 'test_X_0319.pkl')\n",
    "\n",
    "\n",
    "all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 457)\n",
      "(9871, 457)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.04205\n",
      "Will train until validation_0-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\tvalidation_0-mlogloss:0.677803\n",
      "[50]\tvalidation_0-mlogloss:0.636772\n",
      "[75]\tvalidation_0-mlogloss:0.618659\n",
      "[100]\tvalidation_0-mlogloss:0.607344\n",
      "[125]\tvalidation_0-mlogloss:0.599361\n",
      "[150]\tvalidation_0-mlogloss:0.594042\n",
      "[175]\tvalidation_0-mlogloss:0.589287\n",
      "[200]\tvalidation_0-mlogloss:0.585338\n",
      "[225]\tvalidation_0-mlogloss:0.582099\n",
      "[250]\tvalidation_0-mlogloss:0.579162\n",
      "[275]\tvalidation_0-mlogloss:0.57698\n",
      "[300]\tvalidation_0-mlogloss:0.575315\n",
      "[325]\tvalidation_0-mlogloss:0.573552\n",
      "[350]\tvalidation_0-mlogloss:0.571865\n",
      "[375]\tvalidation_0-mlogloss:0.570756\n",
      "[400]\tvalidation_0-mlogloss:0.569717\n",
      "[425]\tvalidation_0-mlogloss:0.568638\n",
      "[450]\tvalidation_0-mlogloss:0.567676\n",
      "[475]\tvalidation_0-mlogloss:0.567038\n",
      "[500]\tvalidation_0-mlogloss:0.5663\n",
      "[525]\tvalidation_0-mlogloss:0.565627\n",
      "[550]\tvalidation_0-mlogloss:0.564942\n",
      "[575]\tvalidation_0-mlogloss:0.564367\n",
      "[600]\tvalidation_0-mlogloss:0.563886\n",
      "[625]\tvalidation_0-mlogloss:0.563768\n",
      "[650]\tvalidation_0-mlogloss:0.563059\n",
      "[675]\tvalidation_0-mlogloss:0.562559\n",
      "[700]\tvalidation_0-mlogloss:0.561997\n",
      "[725]\tvalidation_0-mlogloss:0.561908\n",
      "[750]\tvalidation_0-mlogloss:0.56153\n",
      "[775]\tvalidation_0-mlogloss:0.561142\n",
      "[800]\tvalidation_0-mlogloss:0.560916\n",
      "[825]\tvalidation_0-mlogloss:0.560663\n",
      "[850]\tvalidation_0-mlogloss:0.560349\n",
      "[875]\tvalidation_0-mlogloss:0.560171\n",
      "[900]\tvalidation_0-mlogloss:0.560067\n",
      "[925]\tvalidation_0-mlogloss:0.559879\n",
      "[950]\tvalidation_0-mlogloss:0.55947\n",
      "[975]\tvalidation_0-mlogloss:0.559559\n",
      "Stopping. Best iteration:\n",
      "[949]\tvalidation_0-mlogloss:0.559436\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=10000, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgr = xgb.XGBClassifier(objective = 'multi:softprob',\n",
    "                       learning_rate = 0.1,\n",
    "                       n_estimators = 10000,\n",
    "                       nthread = -1)\n",
    "\n",
    "rgr.fit(X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "#         num_class = 3,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=25\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = rgr.predict_proba(test_X, ntree_limit = rgr.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(pred_y[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.555392 1038\n",
      "4 \t0.552951 554\n",
      "5 \t0.551107 382\n",
      "6 \t0.552202 317\n",
      "7 \t0.553047 200\n",
      "8 \t0.555228 146\n",
      "9 \t0.560529 123\n",
      "10 \t0.566363 87\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7,8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "max_depth = train_param\n",
    "print train_param\n",
    "# 3 \t0.561939\n",
    "# 4 \t0.558393\n",
    "# 5 \t0.55522\n",
    "# 6 \t0.558043\n",
    "# 7 \t0.55804\n",
    "# 8 \t0.561611\n",
    "# 9 \t0.56762\n",
    "# 10 \t0.572505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t0.551747 447\n",
      "10 \t0.548771 495\n",
      "15 \t0.550488 407\n",
      "20 \t0.551501 441\n",
      "25 \t0.549999 420\n",
      "30 \t0.549632 497\n",
      "40 \t0.550637 490\n",
      "50 \t0.547949 555\n",
      "80 \t0.549408 625\n",
      "120 \t0.55105 654\n",
      "180 \t0.554957 494\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [5,10,15,20,25,30,40,50,80,120,180]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight\n",
    "# 5 \t0.556379\n",
    "# 10 \t0.55509\n",
    "# 15 \t0.555654\n",
    "# 20 \t0.554354\n",
    "# 25 \t0.557894\n",
    "# 30 \t0.558761\n",
    "# 40 \t0.554595\n",
    "# 50 \t0.559861\n",
    "# 80 \t0.556099\n",
    "# 120 \t0.560399\n",
    "# 180 \t0.55925\n",
    "# 240 \t0.566327\n",
    "# 300 \t0.567732\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.550644\n",
      "0.4 \t0.548466\n",
      "0.5 \t0.549137\n",
      "0.6 \t0.549148\n",
      "0.7 \t0.550557\n",
      "0.8 \t0.549855\n",
      "0.9 \t0.550225\n",
      "1 \t0.547949\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.553251 518\n",
      "0.6 \t0.554923 507\n",
      "0.7 \t0.551352 447\n",
      "0.8 \t0.548554 544\n",
      "0.9 \t0.549394 464\n",
      "1 \t0.547949 555\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t0.547949 555\n",
      "0.3 \t0.550121 440\n",
      "0.6 \t0.549192 523\n",
      "0.9 \t0.548671 525\n",
      "1.2 \t0.54916 500\n",
      "1.5 \t0.551122 397\n",
      "1.8 \t0.549421 488\n",
      "2.1 \t0.549759 492\n",
      "2.4 \t0.551915 372\n",
      "2.7 \t0.550643 390\n",
      "3.0 \t0.551964 384\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=10,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (3,6),\n",
    "        'min_child_weight': (5,80),\n",
    "        'colsample_bytree': (0.7,1),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0,2.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.977517</td>\n",
       "      <td>7.085047</td>\n",
       "      <td>0.755216</td>\n",
       "      <td>0.832299</td>\n",
       "      <td>2.072250</td>\n",
       "      <td>-0.543320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.294358</td>\n",
       "      <td>6.430458</td>\n",
       "      <td>0.778670</td>\n",
       "      <td>0.794628</td>\n",
       "      <td>2.053074</td>\n",
       "      <td>-0.543492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.831209</td>\n",
       "      <td>1.988818</td>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.791368</td>\n",
       "      <td>1.601972</td>\n",
       "      <td>-0.543742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.337825</td>\n",
       "      <td>2.370541</td>\n",
       "      <td>0.747375</td>\n",
       "      <td>0.727643</td>\n",
       "      <td>1.396417</td>\n",
       "      <td>-0.544077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.844964</td>\n",
       "      <td>2.067151</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>1.951036</td>\n",
       "      <td>-0.544450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.003396</td>\n",
       "      <td>2.082787</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.893346</td>\n",
       "      <td>0.512947</td>\n",
       "      <td>-0.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.607936</td>\n",
       "      <td>5.782687</td>\n",
       "      <td>0.904582</td>\n",
       "      <td>0.933866</td>\n",
       "      <td>0.366536</td>\n",
       "      <td>-0.545158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.319597</td>\n",
       "      <td>25.284915</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.949350</td>\n",
       "      <td>2.065272</td>\n",
       "      <td>-0.545291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.126909</td>\n",
       "      <td>16.160868</td>\n",
       "      <td>0.857025</td>\n",
       "      <td>0.913964</td>\n",
       "      <td>0.708522</td>\n",
       "      <td>-0.545368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.624415</td>\n",
       "      <td>8.166120</td>\n",
       "      <td>0.709067</td>\n",
       "      <td>0.845712</td>\n",
       "      <td>1.583732</td>\n",
       "      <td>-0.545375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "1    4.977517          7.085047          0.755216   0.832299  2.072250   \n",
       "12   5.294358          6.430458          0.778670   0.794628  2.053074   \n",
       "10   4.831209          1.988818          0.976038   0.791368  1.601972   \n",
       "2    5.337825          2.370541          0.747375   0.727643  1.396417   \n",
       "3    3.844964          2.067151          0.828660   0.821156  1.951036   \n",
       "0    4.003396          2.082787          0.999265   0.893346  0.512947   \n",
       "25   4.607936          5.782687          0.904582   0.933866  0.366536   \n",
       "17   5.319597         25.284915          0.932358   0.949350  2.065272   \n",
       "13   4.126909         16.160868          0.857025   0.913964  0.708522   \n",
       "21   3.624415          8.166120          0.709067   0.845712  1.583732   \n",
       "\n",
       "       score  \n",
       "1  -0.543320  \n",
       "12 -0.543492  \n",
       "10 -0.543742  \n",
       "2  -0.544077  \n",
       "3  -0.544450  \n",
       "0  -0.544678  \n",
       "25 -0.545158  \n",
       "17 -0.545291  \n",
       "13 -0.545368  \n",
       "21 -0.545375  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.03)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]   \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.6,\n",
      "       gamma=3, learning_rate=0.03, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=5, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.9)\n",
      "Model 1 fold 1\n",
      "best round 4153\n",
      "('Score: ', 0.54316354151854473)\n",
      "Model 1 fold 1 fitting finished in 563.207s\n",
      "Model 1 fold 2\n",
      "best round 4213\n",
      "('Score: ', 0.53714232689948793)\n",
      "Model 1 fold 2 fitting finished in 575.166s\n",
      "Model 1 fold 3\n",
      "best round 4785\n",
      "('Score: ', 0.51961801525155782)\n",
      "Model 1 fold 3 fitting finished in 643.666s\n",
      "Model 1 fold 4\n",
      "best round 3268\n",
      "('Score: ', 0.53183819808578614)\n",
      "Model 1 fold 4 fitting finished in 452.333s\n",
      "Model 1 fold 5\n",
      "best round 3046\n",
      "('Score: ', 0.52786289571353395)\n",
      "Model 1 fold 5 fitting finished in 423.868s\n",
      "Model 1 fold 6\n",
      "best round 4562\n",
      "('Score: ', 0.53641104075152768)\n",
      "Model 1 fold 6 fitting finished in 613.908s\n",
      "Model 1 fold 7\n",
      "best round 3108\n",
      "('Score: ', 0.54518282591717238)\n",
      "Model 1 fold 7 fitting finished in 429.867s\n",
      "Model 1 fold 8\n",
      "best round 3142\n",
      "('Score: ', 0.53617714715648968)\n",
      "Model 1 fold 8 fitting finished in 435.438s\n",
      "Model 1 fold 9\n",
      "best round 4088\n",
      "('Score: ', 0.54865090000661543)\n",
      "Model 1 fold 9 fitting finished in 566.154s\n",
      "Model 1 fold 10\n",
      "best round 4031\n",
      "('Score: ', 0.54847922493536649)\n",
      "Model 1 fold 10 fitting finished in 552.052s\n",
      "Score for model 1 is 0.537453\n",
      "Score for blended models is 0.537453\n"
     ]
    }
   ],
   "source": [
    "estimators = [xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 5,\n",
    "                              colsample_bytree = 0.6,\n",
    "                              subsample = 0.9,\n",
    "                              gamma = 3),\n",
    "#              xgb.XGBClassifier(max_depth = 5,\n",
    "#                               min_child_weight = 6,\n",
    "#                               colsample_bytree = 0.778670,\n",
    "#                               subsample = 0.794628,\n",
    "#                               gamma = 2.053074)\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xgb_params = [{'max_depth':6,\n",
    "#                'min_child_weight':5,\n",
    "#                'colsample_bytree':0.6,\n",
    "#                'subsample':0.9,\n",
    "#                'gamma':3},\n",
    "# #               score -0.543320        \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':6,\n",
    "#                'colsample_bytree':0.778670,\n",
    "#                'subsample':0.794628,\n",
    "#                'gamma':2.053074},\n",
    "#               score -0.543492\n",
    "#               {'max_depth':4,\n",
    "#                'min_child_weight':1,\n",
    "#                'colsample_bytree':0.976038,\n",
    "#                'subsample':0.791368,\n",
    "#                'gamma':1.601972},\n",
    "# #               score -0.543742  \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.747375,\n",
    "#                'subsample':0.727643,\n",
    "#                'gamma':1.396417},\n",
    "# #               score -0.544077      \n",
    "#               {'max_depth':3,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.828660,\n",
    "#                'subsample':0.821156,\n",
    "#                'gamma':1.951036}\n",
    "# #               score -0.544450\n",
    "#              ]\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n",
    "\n",
    "# print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "# print (np.mean(best_rounds_xgb_le,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54393943  0.54340168  0.54486921  0.54463413  0.5455042 ]\n",
      "4024.2\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_xgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05068891,  0.47914108,  0.47017003],\n",
       "       [ 0.00355554,  0.03459964,  0.96184484],\n",
       "       [ 0.09190651,  0.43008367,  0.47800982],\n",
       "       ..., \n",
       "       [ 0.04040814,  0.38965375,  0.5699381 ],\n",
       "       [ 0.06617295,  0.5109495 ,  0.42287757],\n",
       "       [ 0.0445355 ,  0.33600247,  0.61946204]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb[:,:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
