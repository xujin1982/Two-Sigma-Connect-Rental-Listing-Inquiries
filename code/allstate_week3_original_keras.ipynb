{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allstate week 3\n",
    "\n",
    "This week we will learn how to:\n",
    "\n",
    "* tune LightGBM\n",
    "* create Neural Networks with Keras (Theano or Tensorflow backend)\n",
    "* tune Neural Networks\n",
    "* create a simple ensemble of XGBoost, LightGBM and Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# from pylightgbm.models import GBMRegressor\n",
    "import gc\n",
    "\n",
    "from scipy import sparse\n",
    "%matplotlib inline\n",
    "\n",
    "# import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logregobj(labels, preds):\n",
    "    con = 2\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess \n",
    "\n",
    "def log_mae(labels,preds,lift=200):\n",
    "    return mean_absolute_error(np.exp(labels)-lift, np.exp(preds)-lift)\n",
    "\n",
    "log_mae_scorer = metrics.make_scorer(log_mae, greater_is_better = False)\n",
    "\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = log_mae_scorer,\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain, lift=200):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-lift, np.exp(yhat)-lift)\n",
    "\n",
    "def xgb_logregobj(preds, dtrain):\n",
    "    con = 2\n",
    "    labels = dtrain.get_label()\n",
    "    x =preds-labels\n",
    "    grad =con*x / (np.abs(x)+con)\n",
    "    hess =con**2 / (np.abs(x)+con)**2\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "def search_model_mae (train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'neg_mean_absolute_error',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data finished in 2.765s\n",
      "Loading test data finished in 4.567s\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "start = time.time() \n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_size=train_data.shape[0]\n",
    "print (\"Loading train data finished in %0.3fs\" % (time.time() - start))        \n",
    "del (train_data)\n",
    "gc.collect()\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "print (\"Loading test data finished in %0.3fs\" % (time.time() - start))        \n",
    "\n",
    "# full_data = pd.read_csv('../input/FE_data_COMB.csv.tar.gz',nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FE_data_COMB.csv</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat100</th>\n",
       "      <th>cat101</th>\n",
       "      <th>cat102</th>\n",
       "      <th>cat103</th>\n",
       "      <th>cat104</th>\n",
       "      <th>cat105</th>\n",
       "      <th>cat106</th>\n",
       "      <th>cat107</th>\n",
       "      <th>...</th>\n",
       "      <th>cat14_cat38</th>\n",
       "      <th>cat14_cat24</th>\n",
       "      <th>cat14_cat82</th>\n",
       "      <th>cat14_cat25</th>\n",
       "      <th>cat38_cat24</th>\n",
       "      <th>cat38_cat82</th>\n",
       "      <th>cat38_cat25</th>\n",
       "      <th>cat24_cat82</th>\n",
       "      <th>cat24_cat25</th>\n",
       "      <th>cat82_cat25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  FE_data_COMB.csv  cat10  cat100  cat101  cat102  cat103  cat104  cat105  \\\n",
       "0                A      1       2       7       1       1       9       5   \n",
       "1                A      2      12       6       1       1       5       5   \n",
       "\n",
       "   cat106  cat107     ...       cat14_cat38  cat14_cat24  cat14_cat82  \\\n",
       "0       7      10     ...                27           27           28   \n",
       "1       9      11     ...                27           27           27   \n",
       "\n",
       "   cat14_cat25  cat38_cat24  cat38_cat82  cat38_cat25  cat24_cat82  \\\n",
       "0           27           27           28           27           28   \n",
       "1           27           27           27           27           27   \n",
       "\n",
       "   cat24_cat25  cat82_cat25  \n",
       "0           27           53  \n",
       "1           27           27  \n",
       "\n",
       "[2 rows x 727 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data1 = pd.read_csv('../input/FE_data_COMB.csv',nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat10</th>\n",
       "      <th>cat100</th>\n",
       "      <th>cat101</th>\n",
       "      <th>cat102</th>\n",
       "      <th>cat103</th>\n",
       "      <th>cat104</th>\n",
       "      <th>cat105</th>\n",
       "      <th>cat106</th>\n",
       "      <th>cat107</th>\n",
       "      <th>...</th>\n",
       "      <th>cat14_cat38</th>\n",
       "      <th>cat14_cat24</th>\n",
       "      <th>cat14_cat82</th>\n",
       "      <th>cat14_cat25</th>\n",
       "      <th>cat38_cat24</th>\n",
       "      <th>cat38_cat82</th>\n",
       "      <th>cat38_cat25</th>\n",
       "      <th>cat24_cat82</th>\n",
       "      <th>cat24_cat25</th>\n",
       "      <th>cat82_cat25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat1  cat10  cat100  cat101  cat102  cat103  cat104  cat105  cat106  cat107  \\\n",
       "0    A      1       2       7       1       1       9       5       7      10   \n",
       "1    A      2      12       6       1       1       5       5       9      11   \n",
       "\n",
       "      ...       cat14_cat38  cat14_cat24  cat14_cat82  cat14_cat25  \\\n",
       "0     ...                27           27           28           27   \n",
       "1     ...                27           27           27           27   \n",
       "\n",
       "   cat38_cat24  cat38_cat82  cat38_cat25  cat24_cat82  cat24_cat25  \\\n",
       "0           27           28           27           28           27   \n",
       "1           27           27           27           27           27   \n",
       "\n",
       "   cat82_cat25  \n",
       "0           53  \n",
       "1           27  \n",
       "\n",
       "[2 rows x 727 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge train and test\n",
    "\n",
    "This will save our time on duplicating logics for train and test and will also ensure the transformations applied on train and test are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data set created.\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_data\n",
    "                       ,test_data])\n",
    "del( train_data, test_data)\n",
    "print (\"Full Data set created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group features\n",
    "\n",
    "In this step we will group the features into different groups so we can preprocess them seperately afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Categorical features:', ['cat1', 'cat10', 'cat100', 'cat101', 'cat102', 'cat103', 'cat104', 'cat105', 'cat106', 'cat107', 'cat108', 'cat109', 'cat11', 'cat110', 'cat111', 'cat112', 'cat113', 'cat114', 'cat115', 'cat116', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cat19', 'cat2', 'cat20', 'cat21', 'cat22', 'cat23', 'cat24', 'cat25', 'cat26', 'cat27', 'cat28', 'cat29', 'cat3', 'cat30', 'cat31', 'cat32', 'cat33', 'cat34', 'cat35', 'cat36', 'cat37', 'cat38', 'cat39', 'cat4', 'cat40', 'cat41', 'cat42', 'cat43', 'cat44', 'cat45', 'cat46', 'cat47', 'cat48', 'cat49', 'cat5', 'cat50', 'cat51', 'cat52', 'cat53', 'cat54', 'cat55', 'cat56', 'cat57', 'cat58', 'cat59', 'cat6', 'cat60', 'cat61', 'cat62', 'cat63', 'cat64', 'cat65', 'cat66', 'cat67', 'cat68', 'cat69', 'cat7', 'cat70', 'cat71', 'cat72', 'cat73', 'cat74', 'cat75', 'cat76', 'cat77', 'cat78', 'cat79', 'cat8', 'cat80', 'cat81', 'cat82', 'cat83', 'cat84', 'cat85', 'cat86', 'cat87', 'cat88', 'cat89', 'cat9', 'cat90', 'cat91', 'cat92', 'cat93', 'cat94', 'cat95', 'cat96', 'cat97', 'cat98', 'cat99'])\n",
      "('Numerica features:', ['cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9'])\n",
      "ID: id, target: loss\n"
     ]
    }
   ],
   "source": [
    "data_types = full_data.dtypes  \n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "\n",
    "id_col = 'id'\n",
    "target_col = 'loss'\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('loss')\n",
    "\n",
    "print (\"Categorical features:\", cat_cols)\n",
    "print ( \"Numerica features:\", num_cols)\n",
    "print ( \"ID: %s, target: %s\" %( id_col, target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features \n",
    "### 1. Label Encoding (Factorizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label enconding finished in 35.289642 seconds\n"
     ]
    }
   ],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "start=time.time()\n",
    "for cat_col in cat_cols:\n",
    "#     print (\"Factorize feature %s\" % (cat))\n",
    "    full_data[cat_col] = LBL.fit_transform(full_data[cat_col])\n",
    "print ('Label enconding finished in %f seconds' % (time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One Hot Encoding (get dummies)\n",
    "\n",
    "OHE can be done by either Pandas' get_dummies() or SK Learn's OneHotEncoder. \n",
    "\n",
    "* get_dummies is easier to implement (can be used directly on raw categorical features, i.e. strings, but it takes longer time and is not memory efficient.\n",
    "\n",
    "* OneHotEncoder requires the features being converted to numeric, which has already been done by LabelEncoder in previous step, and is much more efficient (7x faster).\n",
    "\n",
    "* We will convert the OHE's results to a sparse matrix which uses way less memory as compared to dense matrix. However, not all algorithms and packagers support sparse matrix, e.g. Keras. In that case, we'll need to use other tricks to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding finished in 10.169861 seconds\n",
      "(313864, 1176)\n"
     ]
    }
   ],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "full_data_sparse=OHE.fit_transform(full_data[cat_cols])\n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "print (full_data_sparse.shape)\n",
    "\n",
    "## it should be (313864, 1176)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Leave-one-out Encoding\n",
    "\n",
    "This is a very useful trick that has been used by many Kaggle winning solutions. It's particularly effective for high cardinality categorical features, postal code for instance. However, it doesn't seem to help a lot for this competition and the following code is just FYI. Feel free to skip it as it may take long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start=time.time()\n",
    "# loo_cols =[]\n",
    "# for col in cat_cols:\n",
    "#     print (\"Leave-One-Out Encoding  %s\" % (col))\n",
    "#     print (\"Leave-one-out encoding column %s for %s......\" % (col, target_col))\n",
    "#     aggr=full_data.groupby(col)[target_col].agg([np.mean]).join(full_data[:train_size].groupby(col)[target_col].agg([np.sum,np.size]),how='left')        \n",
    "#     meanTagetAggr = np.mean(aggr['mean'].values)\n",
    "#     aggr=full_data.join(aggr,how='left', on=col)[list(aggr.columns)+[target_col]]\n",
    "#     loo_col = 'MEAN_BY_'+col+'_'+target_col\n",
    "#     full_data[loo_col] = \\\n",
    "#     aggr.apply(lambda row: row['mean'] if math.isnan(row[target_col]) \n",
    "#                                                        else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(0.95, 1.05) , axis=1)\n",
    "#     loo_cols.append(loo_col)\n",
    "#     print (\"New feature %s created.\" % (loo_col))\n",
    "# print ('Leave-one-out enconding finished in %f seconds' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "We will apply two preprocessings on numeric features:\n",
    "\n",
    "1. Apply box-cox transformations for skewed numeric features.\n",
    "\n",
    "2. Scale numeric features so they will fall in the range between 0 and 1.\n",
    "\n",
    "Please be advised that these preprocessings are not necessary for tree-based models, e.g. XGBoost. However, linear or linear-based models, which will be dicussed in following weeks, may benefit from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculate skewness of each numeric features: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont2    -0.311146\n",
      "cont3    -0.007023\n",
      "cont14    0.250673\n",
      "cont11    0.281139\n",
      "cont12    0.291997\n",
      "cont10    0.352116\n",
      "cont13    0.376138\n",
      "cont4     0.417559\n",
      "cont6     0.458413\n",
      "cont1     0.513205\n",
      "cont8     0.673237\n",
      "cont5     0.679610\n",
      "cont7     0.825889\n",
      "cont9     1.067247\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew, boxcox\n",
    "skewed_cols = full_data[num_cols].apply(lambda x: skew(x.dropna()))\n",
    "print (skewed_cols.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply box-cox transformations: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply Standard Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSL = preprocessing.StandardScaler()\n",
    "for num_col in num_cols:\n",
    "    full_data[num_col] = SSL.fit_transform(full_data[num_col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: LBL and OHE are likely exclusive so we will use one of them at a time combined with numeric features. In the following steps we will use OHE + Numeric to tune XGBoost models and you can apply the same process with OHE + Numeric features. Averaging results from two different models will likely generate better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 1190)\n",
      "(188318, 1190)\n"
     ]
    }
   ],
   "source": [
    "lift = 200\n",
    "\n",
    "full_data_sparse = sparse.hstack((full_data_sparse\n",
    "                                  ,full_data[num_cols])\n",
    "                                 , format='csr'\n",
    "                                 )\n",
    "print full_data_sparse.shape\n",
    "train_x = full_data_sparse[:train_size]\n",
    "test_x = full_data_sparse[train_size:]\n",
    "train_y = np.log(full_data[:train_size].loss.values + lift)\n",
    "ID = full_data.id[:train_size].values\n",
    "print train_x.shape\n",
    "# xgtrain = xgb.DMatrix(train_x, label=train_y,missing=np.nan) #used for Bayersian Optimization\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kearas\n",
    "\n",
    "https://keras.io\n",
    "\n",
    "Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,ParametricSoftplus,ThresholdedReLU,SReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "\n",
    "## Comment out following lines if you are using Theano as backend\n",
    "# import tensorflow as tf\n",
    "# tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom metric function for Keras\n",
    "\n",
    "def mae_log(y_true, y_pred): \n",
    "    return K.mean(K.abs((K.exp(y_pred)-200) - (K.exp(y_true)-200)))\n",
    "\n",
    "\n",
    "# Keras deosn't support sparse matrix. \n",
    "# The following functions are useful to split a large sparse matrix into smaller batches so they can be loaded into mem.\n",
    "\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras starter\n",
    "\n",
    "Below is a quick starter example for creating a neural networks model using Keras. It covers the following aspects:\n",
    "1. multiple layers: 1 input, 1 hidden and 1 output\n",
    "2. normalization.\n",
    "3. dropout regularization.\n",
    "4. early stopping\n",
    "5. activate function\n",
    "6. optimizer\n",
    "6. batch training\n",
    "\n",
    "Advanced optimizers, activations and dropout regularization are the key characteristics that differentiate modern Neural Networks from conventional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 2.8647 - mae_log: 1661047.9946Epoch 00000: val_mae_log improved from inf to 1581.37705, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 57s - loss: 2.8636 - mae_log: 1659924.4908 - val_loss: 0.5415 - val_mae_log: 1581.3771\n",
      "Epoch 2/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 1.0919 - mae_log: 3140765368.5883Epoch 00001: val_mae_log improved from 1581.37705 to 1365.50953, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 58s - loss: 1.0917 - mae_log: 3138631698.7883 - val_loss: 0.4380 - val_mae_log: 1365.5095\n",
      "Epoch 3/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.9290 - mae_log: 7880879668.7797Epoch 00002: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 58s - loss: 0.9289 - mae_log: 7875525813.4140 - val_loss: 0.4433 - val_mae_log: 1395.5397\n",
      "Epoch 4/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.8543 - mae_log: 7806717.4280Epoch 00003: val_mae_log improved from 1365.50953 to 1247.81024, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 59s - loss: 0.8543 - mae_log: 7801416.0210 - val_loss: 0.3987 - val_mae_log: 1247.8102\n",
      "Epoch 5/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.7935 - mae_log: 12315.2813Epoch 00004: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 59s - loss: 0.7934 - mae_log: 12309.0016 - val_loss: 0.4168 - val_mae_log: 1301.4009\n",
      "Epoch 6/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.7417 - mae_log: 3083.0691Epoch 00005: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 60s - loss: 0.7416 - mae_log: 3082.1862 - val_loss: 0.3992 - val_mae_log: 1250.9254\n",
      "Epoch 7/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.6968 - mae_log: 2464.7204Epoch 00006: val_mae_log improved from 1247.81024 to 1218.22215, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 60s - loss: 0.6968 - mae_log: 2464.4472 - val_loss: 0.3906 - val_mae_log: 1218.2221\n",
      "Epoch 8/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.6561 - mae_log: 2202.8276Epoch 00007: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 60s - loss: 0.6561 - mae_log: 2202.8845 - val_loss: 0.3974 - val_mae_log: 1241.0124\n",
      "Epoch 9/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.6226 - mae_log: 2060.3716Epoch 00008: val_mae_log improved from 1218.22215 to 1214.24458, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 60s - loss: 0.6226 - mae_log: 2060.2295 - val_loss: 0.3887 - val_mae_log: 1214.2446\n",
      "Epoch 10/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.5889 - mae_log: 1934.8879Epoch 00009: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 61s - loss: 0.5889 - mae_log: 1934.9585 - val_loss: 0.3910 - val_mae_log: 1219.5320\n",
      "Epoch 11/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.5613 - mae_log: 1832.3863Epoch 00010: val_mae_log improved from 1214.24458 to 1188.49871, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 61s - loss: 0.5613 - mae_log: 1832.4189 - val_loss: 0.3835 - val_mae_log: 1188.4987\n",
      "Epoch 12/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.5369 - mae_log: 1739.8245Epoch 00011: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 60s - loss: 0.5369 - mae_log: 1739.8336 - val_loss: 0.3828 - val_mae_log: 1189.5311\n",
      "Epoch 13/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.5154 - mae_log: 1671.4392Epoch 00012: val_mae_log improved from 1188.49871 to 1184.44667, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 60s - loss: 0.5154 - mae_log: 1671.3450 - val_loss: 0.3819 - val_mae_log: 1184.4467\n",
      "Epoch 14/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4946 - mae_log: 1602.0981Epoch 00013: val_mae_log improved from 1184.44667 to 1177.31551, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 54s - loss: 0.4946 - mae_log: 1602.2445 - val_loss: 0.3803 - val_mae_log: 1177.3155\n",
      "Epoch 15/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4765 - mae_log: 1540.2842Epoch 00014: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 54s - loss: 0.4765 - mae_log: 1540.0846 - val_loss: 0.3893 - val_mae_log: 1209.4164\n",
      "Epoch 16/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4607 - mae_log: 1488.3319Epoch 00015: val_mae_log improved from 1177.31551 to 1174.00714, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 54s - loss: 0.4607 - mae_log: 1488.1942 - val_loss: 0.3794 - val_mae_log: 1174.0071\n",
      "Epoch 17/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4469 - mae_log: 1443.0670Epoch 00016: val_mae_log improved from 1174.00714 to 1168.20851, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 55s - loss: 0.4469 - mae_log: 1443.1610 - val_loss: 0.3805 - val_mae_log: 1168.2085\n",
      "Epoch 18/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4366 - mae_log: 1408.2891Epoch 00017: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 55s - loss: 0.4366 - mae_log: 1408.1150 - val_loss: 0.3835 - val_mae_log: 1197.4785\n",
      "Epoch 19/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4266 - mae_log: 1371.3518Epoch 00018: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.4267 - mae_log: 1371.4003 - val_loss: 0.3785 - val_mae_log: 1173.0203\n",
      "Epoch 20/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4182 - mae_log: 1341.9640Epoch 00019: val_mae_log improved from 1168.20851 to 1162.96352, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 55s - loss: 0.4182 - mae_log: 1341.9696 - val_loss: 0.3762 - val_mae_log: 1162.9635\n",
      "Epoch 21/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4119 - mae_log: 1325.5963Epoch 00020: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 55s - loss: 0.4119 - mae_log: 1325.4732 - val_loss: 0.3787 - val_mae_log: 1167.5962\n",
      "Epoch 22/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4066 - mae_log: 1301.0709Epoch 00021: val_mae_log improved from 1162.96352 to 1158.57778, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 55s - loss: 0.4065 - mae_log: 1301.0136 - val_loss: 0.3753 - val_mae_log: 1158.5778\n",
      "Epoch 23/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.4015 - mae_log: 1283.7675Epoch 00022: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.4015 - mae_log: 1283.8196 - val_loss: 0.3797 - val_mae_log: 1181.9134\n",
      "Epoch 24/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3990 - mae_log: 1275.1244Epoch 00023: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 55s - loss: 0.3989 - mae_log: 1275.0051 - val_loss: 0.3768 - val_mae_log: 1169.0227\n",
      "Epoch 25/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3951 - mae_log: 1261.6973Epoch 00024: val_mae_log improved from 1158.57778 to 1156.79681, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3951 - mae_log: 1261.8480 - val_loss: 0.3746 - val_mae_log: 1156.7968\n",
      "Epoch 26/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3930 - mae_log: 1252.6771Epoch 00025: val_mae_log improved from 1156.79681 to 1154.82964, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3930 - mae_log: 1252.7126 - val_loss: 0.3742 - val_mae_log: 1154.8296\n",
      "Epoch 27/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3914 - mae_log: 1244.5735Epoch 00026: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 55s - loss: 0.3914 - mae_log: 1244.5012 - val_loss: 0.3776 - val_mae_log: 1174.9193\n",
      "Epoch 28/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3894 - mae_log: 1239.6694Epoch 00027: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3894 - mae_log: 1239.7129 - val_loss: 0.3739 - val_mae_log: 1155.0148\n",
      "Epoch 29/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3875 - mae_log: 1231.5219Epoch 00028: val_mae_log improved from 1154.82964 to 1154.51840, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3875 - mae_log: 1231.4775 - val_loss: 0.3741 - val_mae_log: 1154.5184\n",
      "Epoch 30/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3878 - mae_log: 1230.6232Epoch 00029: val_mae_log improved from 1154.51840 to 1153.00880, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3878 - mae_log: 1230.6625 - val_loss: 0.3737 - val_mae_log: 1153.0088\n",
      "Epoch 31/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3867 - mae_log: 1228.3109Epoch 00030: val_mae_log improved from 1153.00880 to 1152.96260, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3866 - mae_log: 1228.1718 - val_loss: 0.3737 - val_mae_log: 1152.9626\n",
      "Epoch 32/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3848 - mae_log: 1217.2369Epoch 00031: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 55s - loss: 0.3848 - mae_log: 1217.1838 - val_loss: 0.3757 - val_mae_log: 1163.6005\n",
      "Epoch 33/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3829 - mae_log: 1211.2407Epoch 00032: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3829 - mae_log: 1211.4331 - val_loss: 0.3736 - val_mae_log: 1153.6981\n",
      "Epoch 34/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3833 - mae_log: 1210.7105Epoch 00033: val_mae_log improved from 1152.96260 to 1152.25520, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3834 - mae_log: 1210.9115 - val_loss: 0.3730 - val_mae_log: 1152.2552\n",
      "Epoch 35/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3816 - mae_log: 1207.1958Epoch 00034: val_mae_log improved from 1152.25520 to 1148.54210, saving model to weights.hdf5\n",
      "188416/188318 [==============================] - 56s - loss: 0.3817 - mae_log: 1207.3175 - val_loss: 0.3726 - val_mae_log: 1148.5421\n",
      "Epoch 36/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3814 - mae_log: 1205.1458Epoch 00035: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3814 - mae_log: 1205.2030 - val_loss: 0.3729 - val_mae_log: 1150.0956\n",
      "Epoch 37/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3808 - mae_log: 1204.7262Epoch 00036: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3808 - mae_log: 1204.9210 - val_loss: 0.3731 - val_mae_log: 1152.4804\n",
      "Epoch 38/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3799 - mae_log: 1194.5476Epoch 00037: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 57s - loss: 0.3799 - mae_log: 1194.5765 - val_loss: 0.3725 - val_mae_log: 1150.5144\n",
      "Epoch 39/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3801 - mae_log: 1199.8763Epoch 00038: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 57s - loss: 0.3801 - mae_log: 1199.9459 - val_loss: 0.3726 - val_mae_log: 1150.6445\n",
      "Epoch 40/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3793 - mae_log: 1195.7449Epoch 00039: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3793 - mae_log: 1195.5885 - val_loss: 0.3723 - val_mae_log: 1148.8574\n",
      "Epoch 41/1000\n",
      "188288/188318 [============================>.] - ETA: 0s - loss: 0.3797 - mae_log: 1195.5429Epoch 00040: val_mae_log did not improve\n",
      "188416/188318 [==============================] - 56s - loss: 0.3796 - mae_log: 1195.4506 - val_loss: 0.3728 - val_mae_log: 1152.0318\n",
      "aa 1148.54210492\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_mae_log', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_mae_log', \n",
    "                               verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_normal'\n",
    "    \n",
    "    \n",
    "    model.add(Dense(400, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim, # fixed length: number of columns of X\n",
    "                    init=init\n",
    "                   ))\n",
    "    \n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(200,init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(50,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(1)) # 1 for regression \n",
    "    model.compile(loss = 'mae',\n",
    "                  metrics=[mae_log],\n",
    "                  optimizer = 'Adadelta' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )\n",
    "\n",
    "print min(fit.history['val_mae_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout 400 200 50\n",
    "# 0.0 0.0 0.0 1173.22430653\n",
    "# 0.1 0.1 0.1 1159.12022951\n",
    "# 0.2 0.2 0.2 1155.23839803\n",
    "# 0.3 0.3 0.3 1149.65308519\n",
    "# 0.4 0.4 0.4 1149.41400722\n",
    "# 0.5 0.5 0.5 1150.80265431\n",
    "# 0.6 0.6 0.6 1162.70112915\n",
    "# 0.4 0.5 0.5 1148.54210492\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout 200 100\n",
    "# 0.0 0.0 1171.12563576\n",
    "# 0.1 0.1  1160.17143049\n",
    "# 0.2 0.2 1156.6883911\n",
    "# 0.3 0.3 1153.0245463\n",
    "# 0.4 0.4 1169.62663742\n",
    "# 0.5 0.5 1149.56123014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# activation 0.4 0.2\n",
    "# LeakyReLU 1156.6654871\n",
    "# PReLU 1149.37891714\n",
    "# ELU 1155.62879652\n",
    "# ParametricSoftplus 1159.36182408\n",
    "# ThresholdedReLU inf\n",
    "# SReLU 1153.34983381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializations\n",
    "# uniform  1155.03357597\n",
    "# lecun_uniform 1152.83709172\n",
    "# normal 1154.07652823\n",
    "# zero 1818.27969088\n",
    "# glorot_normal 1151.56500457\n",
    "# glorot_uniform 1151.98137925\n",
    "# he_normal 1153.37595458\n",
    "# he_uniform 1164.39926316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "# 'SGD' 1177.38471356\n",
    "# 'SGD' lr = 0.1 1178.63960903\n",
    "# 'RMSprop' 1161.1068612\n",
    "# 'Adagrad' 1176.88879581\n",
    "# 'Adadelta' 1150.60310845\n",
    "# 'Adam' 1153.64722551\n",
    "# 'Adamax' 1151.59510605\n",
    "# 'Nadam' 1155.9498374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.12092417909"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [400 0.4 200 0.2] 1247.1040 1150.9042\n",
    "# [600 0.4 200 0.2] 1311.5099 1155.55158192\n",
    "# [800 0.4 200 0.2] 1280.1025 1151.91463396\n",
    "# [1000 0.4 200 0.2] 1273.5918 1153.3740\n",
    "# [800 0.4 400 0.2] 1379.2316 1164.4119\n",
    "# [800 0.2 400 0.2] 1249.6743 1160.2573\n",
    "# [900 0.2 300 0.2] 1281.4110 1158.5914\n",
    "# [400 0.2 200 0.2] 1300.3075 1158.4239\n",
    "# [400 0.6 200 0.6 100 0.6] 1259.5767 1156.3421\n",
    "# [400 0.4 200 0.2 100 0.2] 1367.5554 1153.8893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and ...\n",
    "\n",
    "The following sample shows how to do cross validation for Keras with early stopping and much more. NN is time consuming, not to mention cross validation. In fact we can leverage every minutes we spent on training NN and make good use of them.\n",
    "\n",
    "we'll first create the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_mae_log', patience=5, verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='val_mae_log', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['input_size'], input_dim = params['input_dim']))\n",
    "\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['input_drop_out']))\n",
    "        \n",
    "    model.add(Dense(params['hidden_size']))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out']))\n",
    "    \n",
    "    \n",
    "#     nadam = Nadam(lr=1e-4)\n",
    "    nadam = Nadam(lr=params['learning_rate'])\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=0, batch_size=128):\n",
    "    print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = list(KFold(len(train_y), fold))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(parameters)))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(parameters)))\n",
    "    scores = np.zeros ((len(skf),len(parameters)))\n",
    "    best_rounds = np.zeros ((len(skf),len(parameters)))\n",
    " \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "        print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], len(skf)))\n",
    "        for i, (train, val) in enumerate(skf):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train]\n",
    "            train_y_fold = train_y[train]\n",
    "            val_x_fold = train_x[val]\n",
    "            val_y_fold = train_y[val]\n",
    "\n",
    "            # early stopping\n",
    "            model = nn_model(nn_params)\n",
    "            print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=60,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     callbacks=[\n",
    "                                                EarlyStopping(monitor='val_mae_log'\n",
    "                                                              , patience=early_stopping_rounds, verbose=0, mode='auto'),\n",
    "                                                ModelCheckpoint(filepath=\"weights.hdf5\"\n",
    "                                                                , monitor='val_mae_log', \n",
    "                                                                verbose=1, save_best_only=True, mode='min')\n",
    "                                                ]\n",
    "                                     )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')\n",
    "\n",
    "         \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_generator(generator=batch_generatorp(val_x_fold, batch_size, True),\n",
    "                                        val_samples=val_x_fold.shape[0]\n",
    "                                     )\n",
    "            \n",
    "            score = log_mae(val_y_fold, val_y_predict_fold,200)\n",
    "            print (\"Score: \", score, mean_absolute_error(val_y_fold, val_y_predict_fold))\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val, j] = val_y_predict_fold.reshape(val_y_predict_fold.shape[0])\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'mae', metrics=[mae_log], optimizer = 'adadelta')            \n",
    "            test_blend_x_j[:,i] = model.predict_generator( which).reshape(test_x.shape[0])\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "   \n",
    "        test_blend_x[:,j] = test_blend_x_j.mean(1)\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a list of parameters that we thought might be working for NN, and cross validate each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_parameters = [\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :450 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.4 ,\n",
    "     'hidden_size' : 250 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1},\n",
    "    { 'input_size' :400 ,\n",
    "     'input_dim' : train_x.shape[1],\n",
    "     'input_drop_out' : 0.5 ,\n",
    "     'hidden_size' : 200 ,\n",
    "     'hidden_drop_out' :0.2,\n",
    "     'learning_rate': 0.1}\n",
    "\n",
    "]\n",
    "\n",
    "(train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_x, train_y, test_x,\n",
    "                                                         4,\n",
    "                                                         5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create two submissions: \n",
    "\n",
    "* one is from the best CV score, the fourth in my case\n",
    "* another is the average of all four\n",
    "\n",
    "You can submit both and see if averaging helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = np.exp(test_blend_x[:,3:4]) - 200 # the forth column of test_blend_x\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_starter.csv\", index=False)\n",
    "print (\"Submission created.\")\n",
    "\n",
    "pred_y = np.exp(np.mean(test_blend_x,axis=1)) - 200\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results['id'] = full_data[train_size:].id\n",
    "results['loss'] = pred_y\n",
    "results.to_csv(\"../output/sub_keras_mean.csv\", index=False)\n",
    "print (\"Submission created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow up questions\n",
    "* So far we've already create five models/ submissions:\n",
    "    * XGBoost with LE\n",
    "    * XGBoost with OHE\n",
    "    * LightGBM with LE\n",
    "    * LightGBM with OHE\n",
    "    * Keras\n",
    "    \n",
    "  Now let's create another submission, or more, by avaraging them or with whatever weights working for you. It should yield better results.\n",
    "  \n",
    "    \n",
    "* Is there a way to ensemble the models even more effectively? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n",
      "        p0       p1       p2       p3       p4       p5       p6       p7  \\\n",
      "0  7.44263  7.45627  7.43563  7.42914  7.41162  7.45465  7.47006  7.44173   \n",
      "1  7.68540  7.69088  7.66513  7.70157  7.66829  7.65112  7.69641  7.66304   \n",
      "2  9.09098  9.09288  9.01976  9.06774  9.04892  9.08354  9.07739  9.05232   \n",
      "3  8.73863  8.75128  8.71120  8.70794  8.74043  8.68432  8.73725  8.76205   \n",
      "4  6.92609  6.91564  6.90176  6.92359  6.90748  6.92546  6.92156  6.88970   \n",
      "\n",
      "        p8       p9  \n",
      "0  7.44343  7.49335  \n",
      "1  7.61425  7.68182  \n",
      "2  9.06537  9.10046  \n",
      "3  8.70831  8.76852  \n",
      "4  6.94481  6.87771  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n",
    "allpredictions = pd.DataFrame()\n",
    "kfolds = 10  # 10 folds is better!\n",
    "kf = KFold(train_x.shape[0], n_folds=kfolds)\n",
    "seed = 42\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('Fold {0}'.format(i + 1))\n",
    "    dtrain, dvalid = train_x[train_index], train_x[test_index]\n",
    "    ytrain, yvalid = train_y[train_index], train_y[test_index]\n",
    "\n",
    "    gbmr = GBMRegressor(\n",
    "        num_threads=4,\n",
    "        num_iterations=20000,\n",
    "        learning_rate=0.005,\n",
    "        num_leaves=68,\n",
    "        max_bin = 526,\n",
    "        min_data_in_leaf=127,\n",
    "        metric='l1',\n",
    "        feature_fraction=0.218683,\n",
    "        bagging_fraction=0.961961,\n",
    "        bagging_freq=1,\n",
    "        early_stopping_round=600,\n",
    "        verbose= False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    gbmr.fit(dtrain, ytrain,test_data=[(dvalid, yvalid)])\n",
    "    del dtrain\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "    \n",
    "    allpredictions['p' + str(i)] = gbmr.predict(test_x)\n",
    "    del gbmr\n",
    "    gc.collect()\n",
    "    \n",
    "print(allpredictions.head())\n",
    "    \n",
    "submission = pd.read_csv('../output/sample_submission.csv')\n",
    "submission.iloc[:, 1] = np.exp(allpredictions.mean(axis=1).values) - lift\n",
    "submission.to_csv('pyLightGBMmeansubmission_10Kfold.csv', index=None)\n",
    "submission.iloc[:, 1] = np.exp(allpredictions.median(axis=1).values) - lift\n",
    "submission.to_csv('pyLightGBMmediansubmission_10Kfold.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
