{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from scipy import sparse\n",
    "from scipy.stats.mstats import gmean\n",
    "from datetime import datetime\n",
    "# from sklearn import preprocessing\n",
    "# from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = np.ravel(pd.read_csv('../input/' + 'labels_BrandenMurray.csv'))\n",
    "\n",
    "names = ['low_0','medium_0','high_0',\n",
    "        'low_1','medium_1','high_1',\n",
    "        'low_2','medium_2','high_2',\n",
    "        'low_3','medium_3','high_3',\n",
    "        'low_4','medium_4','high_4',\n",
    "        'low_5','medium_5','high_5',\n",
    "        'low_6','medium_6','high_6',\n",
    "        'low_7','medium_7','high_7',\n",
    "        'low_8','medium_8','high_8',\n",
    "        'low_9','medium_9','high_9']\n",
    "\n",
    "data_path = \"../2ndlast/\"\n",
    "total_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.355361      0.544070    0.100569\n",
      "1   0.508303      0.446720    0.044978\n",
      "2   0.603091      0.349880    0.047029\n",
      "3   0.616221      0.328639    0.055140\n",
      "4   0.947230      0.049863    0.002907\n",
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.288217      0.532268    0.179515\n",
      "1   0.970891      0.025801    0.003308\n",
      "2   0.908912      0.078535    0.012553\n",
      "3   0.400539      0.476918    0.122542\n",
      "4   0.700470      0.269945    0.029586\n"
     ]
    }
   ],
   "source": [
    "# RFC 1st level \n",
    "file_train      = 'train_blend_RFC_entropy_last_2017-04-21-11-06' + '.csv'\n",
    "file_test_mean  = 'test_blend_RFC_entropy_mean_last_2017-04-21-11-06' + '.csv'\n",
    "\n",
    "train_rfc = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_rfc  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_rfc.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_rfc.columns = ['rfc_' + x for x in names[:n_column]]\n",
    "test_rfc.columns  = ['rfc_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_rfc.iloc[:5,:3]\n",
    "\n",
    "print test_rfc.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.259359     0.652577   0.088065\n",
      "1  0.738646     0.234238   0.027115\n",
      "2  0.396965     0.512991   0.090045\n",
      "3  0.647052     0.312537   0.040412\n",
      "4  0.923203     0.072255   0.004543\n",
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.282172     0.549423   0.168405\n",
      "1  0.955532     0.040091   0.004377\n",
      "2  0.925692     0.065197   0.009112\n",
      "3  0.631038     0.309690   0.059272\n",
      "4  0.803117     0.187725   0.009158\n"
     ]
    }
   ],
   "source": [
    "# LR 1st level\n",
    "file_train      = 'train_blend_LR_last_2017-04-21-11-16' + '.csv'\n",
    "file_test_mean  = 'test_blend_LR_mean_last_2017-04-21-11-16' + '.csv'\n",
    "\n",
    "train_LR = pd.read_csv(data_path + file_train, header = None)\n",
    "test_LR  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_LR.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_LR.columns = ['LR_' + x for x in names[:n_column]]\n",
    "test_LR.columns  = ['LR_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_LR.iloc[:5,:3]\n",
    "print test_LR.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.332903     0.538085   0.129012\n",
      "1  0.471780     0.454812   0.073408\n",
      "2  0.582223     0.383893   0.033884\n",
      "3  0.622462     0.328557   0.048981\n",
      "4  0.926402     0.064996   0.008602\n",
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.309822     0.527181   0.162997\n",
      "1  0.984336     0.014059   0.001605\n",
      "2  0.956579     0.038080   0.005341\n",
      "3  0.518490     0.384524   0.096986\n",
      "4  0.759357     0.210049   0.030594\n"
     ]
    }
   ],
   "source": [
    "# ET 1st level\n",
    "file_train      = 'train_blend_ET_entropy_last_2017-04-21-11-48' + '.csv'\n",
    "file_test_mean  = 'test_blend_ET_entropy_mean_last_2017-04-21-11-48' + '.csv'\n",
    "\n",
    "train_ET = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_ET  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_ET.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ET.columns = ['ET_' + x for x in names[:n_column]]\n",
    "test_ET.columns  = ['ET_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ET.iloc[:5,:3]\n",
    "print test_ET.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.507812              0.390625            0.101562\n",
      "1           0.531250              0.359375            0.109375\n",
      "2           0.671875              0.273438            0.054688\n",
      "3           0.609375              0.250000            0.140625\n",
      "4           0.843750              0.140625            0.015625\n",
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.381250              0.457813            0.160938\n",
      "1           0.968750              0.031250            0.000000\n",
      "2           0.970313              0.029687            0.000000\n",
      "3           0.693750              0.259375            0.046875\n",
      "4           0.612500              0.321875            0.065625\n"
     ]
    }
   ],
   "source": [
    "# KNN 1st level\n",
    "file_train      = 'train_blend_KNN_uniform_last_2017-04-21-13-53' + '.csv'\n",
    "file_test_mean  = 'test_blend_KNN_uniform_mean_last_2017-04-21-13-53' + '.csv'\n",
    "\n",
    "\n",
    "train_KNN = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_KNN  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_KNN.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_KNN.columns      = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "test_KNN.columns  = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_KNN.iloc[:5,:3]\n",
    "print test_KNN.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.460187          0.436036        0.103776\n",
      "1       0.268598          0.571916        0.159486\n",
      "2       0.724799          0.239351        0.035851\n",
      "3       0.669683          0.286716        0.043600\n",
      "4       0.917878          0.073469        0.008653\n",
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.449136          0.411890        0.138974\n",
      "1       0.971615          0.020319        0.008066\n",
      "2       0.909864          0.074040        0.016096\n",
      "3       0.661175          0.269026        0.069799\n",
      "4       0.705851          0.263069        0.031080\n"
     ]
    }
   ],
   "source": [
    "# TFFM 1st level 0322\n",
    "file_train      = 'train_blend_FM_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "file_test_mean  = 'test_blend_FM_mean_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "\n",
    "train_FM_0322      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_FM_mean_0322  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_FM_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_FM_0322.columns      = ['FM_0322_' + x for x in names[:n_column]]\n",
    "test_FM_mean_0322.columns  = ['FM_0322_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_FM_0322.iloc[:5,:3]\n",
    "print test_FM_mean_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.216985      0.579031    0.203985\n",
      "1   0.560375      0.382293    0.057331\n",
      "2   0.621019      0.332482    0.046499\n",
      "3   0.312441      0.345115    0.342444\n",
      "4   0.901678      0.090035    0.008287\n",
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.218456      0.591575    0.189969\n",
      "1   0.992103      0.007102    0.000795\n",
      "2   0.971220      0.025168    0.003612\n",
      "3   0.501988      0.411198    0.086813\n",
      "4   0.754496      0.213158    0.032347\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes 1st level\n",
    "file_train      = 'train_blend_MNB_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "file_test_mean  = 'test_blend_MNB_mean_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "\n",
    "\n",
    "train_MNB      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_MNB_mean  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_MNB.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_MNB.columns      = ['MNB_' + x for x in names[:n_column]]\n",
    "test_MNB_mean.columns  = ['MNB_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_MNB.iloc[:5,:3]\n",
    "print test_MNB_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tsne_0     tsne_1    tsne_2\n",
      "0  -8.398991  -2.415894 -3.602143\n",
      "1   0.698237   0.335786  8.884257\n",
      "2  -5.811380 -16.669975  7.145837\n",
      "3  -0.371861 -25.894747 -2.076309\n",
      "4 -15.371799   9.656209  5.813590\n",
      "      tsne_0     tsne_1     tsne_2\n",
      "0  -5.176846  -0.768422  -2.339259\n",
      "1   9.003089  13.250301  -0.707032\n",
      "2   4.188036  14.397186   4.573307\n",
      "3  10.890132 -12.660774 -13.414140\n",
      "4   6.011381   5.177731  15.669250\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level\n",
    "\n",
    "file_train = 'X_train_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "\n",
    "train_tsne = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "\n",
    "n_column = train_tsne.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne.columns = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "test_tsne.columns  = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "\n",
    "\n",
    "print train_tsne.iloc[:5,:3]\n",
    "print test_tsne.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -6.649132    13.028168     8.329733\n",
      "1     7.615566     0.067456   -14.932181\n",
      "2     8.333528     8.561174   -13.536297\n",
      "3    12.819587   -20.027314     0.661660\n",
      "4    -5.513088    -5.609218    17.130673\n",
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -5.721674     7.011411    -6.499047\n",
      "1     8.238390    -8.589710    13.771045\n",
      "2   -11.383577   -16.071395    15.083511\n",
      "3    -6.111491     6.348311   -10.222012\n",
      "4     4.426022    15.553415    11.315777\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level 0322\n",
    "\n",
    "file_train = 'X_train_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "\n",
    "train_tsne_0322 = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne_0322  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "n_column = train_tsne_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne_0322.columns = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "test_tsne_0322.columns  = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "\n",
    "print train_tsne_0322.iloc[:5,:3]\n",
    "print test_tsne_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.338520      0.631375    0.030105\n",
      "1   0.563156      0.390819    0.046025\n",
      "2   0.462242      0.498429    0.039328\n",
      "3   0.930174      0.067369    0.002458\n",
      "4   0.892201      0.106567    0.001232\n",
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.178073      0.621011    0.200916\n",
      "1   0.978270      0.012231    0.009499\n",
      "2   0.939539      0.055440    0.005021\n",
      "3   0.151284      0.616193    0.232523\n",
      "4   0.698756      0.292996    0.008247\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb*2\n",
    "tmp_test  = test_xgb_mean*2\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb      = (tmp_train + train_xgb*20) / 22.0\n",
    "test_xgb_mean  = (tmp_test + test_xgb_mean*20) / 22.0\n",
    "\n",
    "n_column = train_xgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "test_xgb_mean.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb.iloc[:5,:3]\n",
    "print test_xgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.309433             0.660416           0.030151\n",
      "1          0.565861             0.382300           0.051838\n",
      "2          0.401785             0.566253           0.031961\n",
      "3          0.931191             0.066339           0.002470\n",
      "4          0.881444             0.117002           0.001555\n",
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.174706             0.640575           0.184719\n",
      "1          0.977405             0.013037           0.009558\n",
      "2          0.944000             0.051268           0.004732\n",
      "3          0.145425             0.592400           0.262175\n",
      "4          0.670348             0.322061           0.007591\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "file_test_mean  = 'test_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_30fold      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_30fold.columns      = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_30fold.columns  = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_30fold.iloc[:5,:3]\n",
    "print test_xgb_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.313141          0.658802        0.028057\n",
      "1       0.491897          0.450891        0.057212\n",
      "2       0.486618          0.486574        0.026809\n",
      "3       0.925839          0.071295        0.002866\n",
      "4       0.863436          0.134349        0.002214\n",
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.167358          0.646104        0.186538\n",
      "1       0.981203          0.010042        0.008756\n",
      "2       0.906588          0.088855        0.004558\n",
      "3       0.167107          0.602472        0.230421\n",
      "4       0.716008          0.274903        0.009089\n"
     ]
    }
   ],
   "source": [
    "# XGB one vs rest 1st level\n",
    "\n",
    "file_train      = 'train_blend_xgb_ovr_last_2017-04-21-10-09' + '.csv'\n",
    "file_test_mean  = 'test_blend_xgb_ovr_mean_last_2017-04-21-10-09' + '.csv'\n",
    "\n",
    "train_xgb_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_ovr.columns      = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_ovr.columns  = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_xgb_ovr,axis=1)\n",
    "sum_test  = np.sum(test_xgb_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_xgb_ovr.columns.values:\n",
    "    train_xgb_ovr[col] = train_xgb_ovr[col] / sum_train\n",
    "    test_xgb_mean_ovr[col] = test_xgb_mean_ovr[col] / sum_test\n",
    "\n",
    "\n",
    "print train_xgb_ovr.iloc[:5,:3]\n",
    "print test_xgb_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.317617            0.650112          0.032271\n",
      "1         0.597390            0.386899          0.015711\n",
      "2         0.450143            0.517132          0.032726\n",
      "3         0.898037            0.100391          0.001572\n",
      "4         0.880435            0.118283          0.001282\n",
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.186506            0.567313          0.246180\n",
      "1         0.964578            0.025110          0.010312\n",
      "2         0.913688            0.080543          0.005769\n",
      "3         0.114846            0.657136          0.228019\n",
      "4         0.650263            0.344835          0.004902\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 1st level\n",
    "\n",
    "file_train      = 'train_blend_LightGBM_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_LightGBM_mean_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "\n",
    "\n",
    "train_lgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_lgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_lgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_lgb.columns      = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "test_lgb_mean.columns  = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_lgb.iloc[:5,:3]\n",
    "print test_lgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.335217     0.604643   0.060140\n",
      "1  0.772956     0.207974   0.019070\n",
      "2  0.529658     0.446699   0.023643\n",
      "3  0.956680     0.042453   0.000867\n",
      "4  0.938238     0.060709   0.001053\n",
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.276664     0.582024   0.141312\n",
      "1  0.995422     0.004223   0.000356\n",
      "2  0.979827     0.019278   0.000895\n",
      "3  0.362736     0.446379   0.190885\n",
      "4  0.729782     0.259918   0.010300\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level No.1\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-21-23' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-21-23' + '.csv'\n",
    "\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "test_nn_mean = tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-22-05' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-22-05' + '.csv'\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = (train_nn + tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "test_nn_mean = (test_nn_mean + tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn.columns      = ['nn_' + x for x in names[:n_column]]\n",
    "test_nn_mean.columns  = ['nn_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn.iloc[:5,:3]\n",
    "print test_nn_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.378595            0.564656          0.056749\n",
      "1         0.763784            0.215613          0.020603\n",
      "2         0.577165            0.398481          0.024354\n",
      "3         0.949192            0.049937          0.000871\n",
      "4         0.958236            0.040963          0.000801\n",
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.286080            0.567262          0.146658\n",
      "1         0.996091            0.003613          0.000296\n",
      "2         0.986770            0.012566          0.000664\n",
      "3         0.370175            0.443003          0.186822\n",
      "4         0.738394            0.250685          0.010921\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "\n",
    "train_nn_30fold     = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_30fold.columns      = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "test_nn_mean_30fold.columns  = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_30fold.iloc[:5,:3]\n",
    "print test_nn_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.414059         0.533141       0.052800\n",
      "1      0.752638         0.210038       0.037324\n",
      "2      0.515976         0.440243       0.043781\n",
      "3      0.780622         0.219167       0.000211\n",
      "4      0.965501         0.032244       0.002255\n",
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.227279         0.679449       0.093272\n",
      "1      0.976299         0.014297       0.009403\n",
      "2      0.919964         0.066669       0.013367\n",
      "3      0.266264         0.389498       0.344238\n",
      "4      0.874583         0.119637       0.005780\n"
     ]
    }
   ],
   "source": [
    "# Keras one vs rest 1st level\n",
    "file_train      = 'train_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "\n",
    "train_nn_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_ovr.columns      = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "test_nn_mean_ovr.columns  = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_nn_ovr,axis=1)\n",
    "sum_test  = np.sum(test_nn_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_nn_ovr.columns.values:\n",
    "    train_nn_ovr[col] = train_nn_ovr[col] / sum_train\n",
    "    test_nn_mean_ovr[col] = test_nn_mean_ovr[col] / sum_test \n",
    "\n",
    "print train_nn_ovr.iloc[:5,:3]\n",
    "print test_nn_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.445882            0.488624          0.065494\n",
      "1         0.761473            0.222332          0.016195\n",
      "2         0.542404            0.415798          0.041798\n",
      "3         0.956819            0.041689          0.001492\n",
      "4         0.950208            0.048009          0.001783\n",
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.300305            0.540791          0.158904\n",
      "1         0.995423            0.004364          0.000213\n",
      "2         0.984619            0.014709          0.000672\n",
      "3         0.376123            0.446293          0.177584\n",
      "4         0.713087            0.267086          0.019827\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 3layer 20 bagging\n",
    "file_train      = 'train_blend_Keras_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "\n",
    "train_nn_3layer      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_3layer  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_3layer.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_3layer.columns      = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "test_nn_mean_3layer.columns  = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_nn_3layer.iloc[:5,:3]\n",
    "print test_nn_mean_3layer.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print total_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2nd: (49352, 48)\t test_2nd_mean:(74659, 48)\n"
     ]
    }
   ],
   "source": [
    "train_2nd      = pd.concat([train_rfc, train_LR, train_ET, train_KNN, train_FM_0322,    train_MNB,     train_tsne,\n",
    "                            train_tsne_0322, train_xgb,     train_xgb_30fold,     train_xgb_ovr, \n",
    "                            train_nn,     train_nn_30fold,     train_nn_ovr,     train_nn_3layer,\n",
    "                            train_lgb\n",
    "#                             train_gp\n",
    "                           ], axis = 1)\n",
    "\n",
    "test_2nd_mean  = pd.concat([test_rfc,  test_LR,  test_ET,  test_KNN, test_FM_mean_0322, test_MNB_mean, test_tsne, \n",
    "                            test_tsne_0322,  test_xgb_mean, test_xgb_mean_30fold, test_xgb_mean_ovr,\n",
    "                            test_nn_mean, test_nn_mean_30fold, test_nn_mean_ovr, test_nn_mean_3layer,\n",
    "                            test_lgb_mean\n",
    "#                             test_gp\n",
    "                           ], axis = 1)\n",
    "\n",
    "print 'train_2nd: {}\\t test_2nd_mean:{}'.\\\n",
    "            format(train_2nd.shape,test_2nd_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.49766325 -0.48445834 -0.49563023 -0.51699695 -0.5375578 ]\n",
      "-0.506461312127\n",
      "Spend:  158.536000013\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "\n",
    "rgr = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=0.1,\n",
    "                         n_estimators=2000)\n",
    "score = cross_val_score(rgr,train_2nd,train_y, scoring = 'neg_log_loss', cv = 5)\n",
    "print score\n",
    "print score.mean()\n",
    "\n",
    "print \"Spend: \", time.time() - now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-27decc5d6388>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-27decc5d6388>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1200 0.1 1200\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1200 0.1 1200\n",
    "[-0.49816686 -0.48150246 -0.49625859 -0.51722592 -0.5287729 ]\n",
    "-0.504385344887\n",
    "Spend:  154.522000074\n",
    "\n",
    "    \n",
    "    \n",
    "1200 0.5 800\n",
    "[-0.49190641 -0.48459669 -0.49568783 -0.51640558 -0.53744722]\n",
    "-0.505208746775\n",
    "Spend:  149.413000107\n",
    "\n",
    "1200 1.0 800\n",
    "\n",
    "[-0.49488315 -0.48164879 -0.49825789 -0.51687499 -0.53480645]\n",
    "-0.505294252919\n",
    "Spend:  150.413000107\n",
    "\n",
    "\n",
    "1200 500 \n",
    "[-0.49273048 -0.48431558 -0.49576742 -0.51722963 -0.53711058]\n",
    "-0.505430738097\n",
    "Spend:  147.505000114\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ET_blend(est, train_x, train_y, test_x, fold,randomseed):\n",
    "    N_params = len(est)\n",
    "#     print \"Blend %d estimators for %d folds\" % (N_params, fold)\n",
    "    skf = KFold(n_splits=fold,shuffle=True,random_state=randomseed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "    fold_start = time.time() \n",
    "    \n",
    "    for j, ester in enumerate(est):\n",
    "#         print \"Model %d:\" %(j+1)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "\n",
    "            \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "#             print \"Model %d fold %d\" %(j+1,i+1)\n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]            \n",
    "            \n",
    "\n",
    "            ester.fit(train_x_fold,train_y_fold)\n",
    "            \n",
    "            val_y_predict_fold = ester.predict_proba(val_x_fold)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "#             print \"Score: \", score\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = ester.predict_proba(test_x)\n",
    "            \n",
    "#             print \"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start)            \n",
    "\n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "\n",
    "            \n",
    "#         print \"Score for model %d is %f\" % (j+1,np.mean(scores[:,j]))\n",
    "    print \"Score for blended models is %f in %0.3fm\" % (np.mean(scores), (time.time() - fold_start)/60)\n",
    "    return (train_blend_x, test_blend_x_mean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for blended models is 0.505084 in 3.292m\n",
      "Score for blended models is 0.505087 in 3.276m\n",
      "Score for blended models is 0.507045 in 3.327m\n",
      "Score for blended models is 0.503338 in 3.392m\n",
      "Score for blended models is 0.506134 in 3.384m\n",
      "Score for blended models is 0.505427 in 3.448m\n",
      "Score for blended models is 0.505096 in 3.373m\n",
      "Score for blended models is 0.505757 in 3.444m\n",
      "Score for blended models is 0.502703 in 3.436m\n",
      "Score for blended models is 0.505647 in 3.332m\n",
      "Score for blended models is 0.504198 in 3.396m\n",
      "Score for blended models is 0.504667 in 3.358m\n",
      "Score for blended models is 0.507143 in 3.328m\n",
      "Score for blended models is 0.505686 in 3.342m\n",
      "Score for blended models is 0.504679 in 3.331m\n",
      "Score for blended models is 0.504898 in 3.326m\n",
      "Score for blended models is 0.504318 in 3.311m\n",
      "Score for blended models is 0.506135 in 3.478m\n",
      "Score for blended models is 0.504611 in 3.359m\n",
      "Score for blended models is 0.505322 in 3.378m\n",
      "Score for blended models is 0.504853 in 3.476m\n",
      "Score for blended models is 0.504953 in 3.380m\n",
      "Score for blended models is 0.506300 in 3.342m\n",
      "Score for blended models is 0.505032 in 3.381m\n",
      "Score for blended models is 0.506136 in 3.321m\n",
      "Score for blended models is 0.506373 in 3.318m\n",
      "Score for blended models is 0.503851 in 3.340m\n",
      "Score for blended models is 0.504324 in 3.359m\n",
      "Score for blended models is 0.504893 in 3.373m\n",
      "Score for blended models is 0.505608 in 3.413m\n",
      "Score for blended models is 0.505384 in 3.342m\n",
      "Score for blended models is 0.504511 in 3.385m\n",
      "Score for blended models is 0.505444 in 3.391m\n",
      "Score for blended models is 0.505489 in 3.326m\n",
      "Score for blended models is 0.503834 in 3.375m\n",
      "Score for blended models is 0.505742 in 3.385m\n",
      "Score for blended models is 0.502975 in 3.327m\n",
      "Score for blended models is 0.504270 in 3.406m\n",
      "Score for blended models is 0.503990 in 3.368m\n",
      "Score for blended models is 0.504492 in 3.321m\n",
      "Score for blended models is 0.504443 in 3.325m\n",
      "Score for blended models is 0.505148 in 3.337m\n",
      "Score for blended models is 0.504938 in 3.339m\n",
      "Score for blended models is 0.504550 in 3.326m\n",
      "Score for blended models is 0.504639 in 3.338m\n",
      "Score for blended models is 0.503505 in 3.324m\n",
      "Score for blended models is 0.505044 in 3.409m\n",
      "Score for blended models is 0.503219 in 3.335m\n",
      "Score for blended models is 0.506308 in 3.325m\n",
      "Score for blended models is 0.505365 in 3.420m\n",
      "Score for blended models is 0.506215 in 3.341m\n",
      "Score for blended models is 0.507483 in 3.337m\n",
      "Score for blended models is 0.503701 in 3.423m\n",
      "Score for blended models is 0.501831 in 3.346m\n",
      "Score for blended models is 0.503506 in 3.326m\n",
      "Score for blended models is 0.505385 in 3.430m\n",
      "Score for blended models is 0.503758 in 3.316m\n",
      "Score for blended models is 0.505719 in 3.324m\n",
      "Score for blended models is 0.504420 in 3.409m\n",
      "Score for blended models is 0.504700 in 3.317m\n",
      "Score for blended models is 0.506720 in 3.329m\n",
      "Score for blended models is 0.505314 in 3.428m\n",
      "Score for blended models is 0.503596 in 3.323m\n",
      "Score for blended models is 0.503882 in 3.316m\n",
      "Score for blended models is 0.505059 in 3.328m\n",
      "Score for blended models is 0.505686 in 3.329m\n",
      "Score for blended models is 0.503879 in 3.315m\n",
      "Score for blended models is 0.504704 in 3.325m\n",
      "Score for blended models is 0.504002 in 3.323m\n",
      "Score for blended models is 0.504358 in 3.317m\n",
      "Score for blended models is 0.503622 in 3.433m\n",
      "Score for blended models is 0.503876 in 3.332m\n",
      "Score for blended models is 0.504796 in 3.319m\n",
      "Score for blended models is 0.504736 in 3.426m\n",
      "Score for blended models is 0.504649 in 3.324m\n",
      "Score for blended models is 0.504226 in 3.316m\n",
      "Score for blended models is 0.503959 in 3.412m\n",
      "Score for blended models is 0.504138 in 3.321m\n",
      "Score for blended models is 0.505692 in 3.304m\n",
      "Score for blended models is 0.503405 in 3.433m\n",
      "Score for blended models is 0.506110 in 3.326m\n",
      "Score for blended models is 0.507197 in 3.325m\n",
      "Score for blended models is 0.503433 in 3.328m\n",
      "Score for blended models is 0.505528 in 3.300m\n",
      "Score for blended models is 0.502502 in 3.316m\n",
      "Score for blended models is 0.503905 in 3.331m\n",
      "Score for blended models is 0.507305 in 3.328m\n",
      "Score for blended models is 0.502883 in 3.329m\n",
      "Score for blended models is 0.506471 in 3.324m\n",
      "Score for blended models is 0.504972 in 3.323m\n",
      "Score for blended models is 0.505188 in 3.327m\n",
      "Score for blended models is 0.505827 in 3.416m\n",
      "Score for blended models is 0.505343 in 3.332m\n",
      "Score for blended models is 0.503721 in 3.327m\n",
      "Score for blended models is 0.506175 in 3.337m\n",
      "Score for blended models is 0.504890 in 3.337m\n",
      "Score for blended models is 0.503483 in 3.336m\n",
      "Score for blended models is 0.504611 in 3.452m\n",
      "Score for blended models is 0.507296 in 3.338m\n",
      "Score for blended models is 0.503903 in 3.342m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_2nd.shape[0], 3))\n",
    "test_total = np.zeros((test_2nd_mean.shape[0], 3))\n",
    "name_train_blend = '../tmp/train_adet.csv'\n",
    "name_test_blend = '../tmp/test_adet.csv'\n",
    "score_total = 0\n",
    "count = 100\n",
    "for n in range(count):\n",
    "    randomseed = n + 42745\n",
    "    est = [AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=0.8, n_estimators=800)]\n",
    "    (train_blend_2nd_ADET,\n",
    "     test_blend_2nd_ADET,\n",
    "     blend_scores_2nd_ADET,\n",
    "     best_rounds_2nd_ADET) = ET_blend(est,\n",
    "                                 train_2nd,train_y,\n",
    "                                 test_2nd_mean,\n",
    "                                 5,randomseed)\n",
    "    train_total += train_blend_2nd_ADET\n",
    "    test_total += test_blend_2nd_ADET\n",
    "    score_total += np.mean(blend_scores_2nd_ADET)\n",
    "    \n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504863505877\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_2ndADET_100bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_2ndADET_100bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print score_total\n",
    "# print (np.mean(best_rounds_RFC,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for blended models is 0.505238 in 3.471m\n",
      "Score for blended models is 0.506248 in 3.343m\n",
      "Score for blended models is 0.503919 in 3.364m\n",
      "Score for blended models is 0.505090 in 3.351m\n",
      "Score for blended models is 0.503822 in 3.341m\n",
      "Score for blended models is 0.503052 in 3.372m\n",
      "Score for blended models is 0.504551 in 3.492m\n",
      "Score for blended models is 0.506366 in 3.332m\n",
      "Score for blended models is 0.503320 in 3.415m\n",
      "Score for blended models is 0.502503 in 3.433m\n",
      "Score for blended models is 0.506673 in 3.338m\n",
      "Score for blended models is 0.504518 in 3.360m\n",
      "Score for blended models is 0.508884 in 3.406m\n",
      "Score for blended models is 0.505212 in 3.327m\n",
      "Score for blended models is 0.505656 in 3.332m\n",
      "Score for blended models is 0.505542 in 3.342m\n",
      "Score for blended models is 0.505740 in 3.332m\n",
      "Score for blended models is 0.503763 in 3.370m\n",
      "Score for blended models is 0.501977 in 3.382m\n",
      "Score for blended models is 0.504938 in 3.317m\n",
      "Score for blended models is 0.503504 in 3.316m\n",
      "Score for blended models is 0.504173 in 3.332m\n",
      "Score for blended models is 0.505782 in 3.327m\n",
      "Score for blended models is 0.504677 in 3.371m\n",
      "Score for blended models is 0.503892 in 3.376m\n",
      "Score for blended models is 0.504979 in 3.334m\n",
      "Score for blended models is 0.502791 in 3.372m\n",
      "Score for blended models is 0.505321 in 3.367m\n",
      "Score for blended models is 0.506531 in 3.318m\n",
      "Score for blended models is 0.504751 in 3.392m\n",
      "Score for blended models is 0.504527 in 3.362m\n",
      "Score for blended models is 0.505579 in 3.330m\n",
      "Score for blended models is 0.503068 in 3.391m\n",
      "Score for blended models is 0.505702 in 3.338m\n",
      "Score for blended models is 0.505038 in 3.331m\n",
      "Score for blended models is 0.505374 in 3.324m\n",
      "Score for blended models is 0.504660 in 3.328m\n",
      "Score for blended models is 0.504789 in 3.333m\n",
      "Score for blended models is 0.506960 in 3.428m\n",
      "Score for blended models is 0.505899 in 3.329m\n",
      "Score for blended models is 0.505279 in 3.330m\n",
      "Score for blended models is 0.503709 in 3.428m\n",
      "Score for blended models is 0.503964 in 3.326m\n",
      "Score for blended models is 0.504779 in 3.324m\n",
      "Score for blended models is 0.503973 in 3.444m\n",
      "Score for blended models is 0.505864 in 3.342m\n",
      "Score for blended models is 0.507160 in 3.335m\n",
      "Score for blended models is 0.505237 in 3.427m\n",
      "Score for blended models is 0.503207 in 3.331m\n",
      "Score for blended models is 0.506472 in 3.333m\n",
      "Score for blended models is 0.505483 in 3.329m\n",
      "Score for blended models is 0.505705 in 3.330m\n",
      "Score for blended models is 0.504780 in 3.333m\n",
      "Score for blended models is 0.504302 in 3.328m\n",
      "Score for blended models is 0.504240 in 3.323m\n",
      "Score for blended models is 0.502934 in 3.340m\n",
      "Score for blended models is 0.503586 in 3.325m\n",
      "Score for blended models is 0.505029 in 3.341m\n",
      "Score for blended models is 0.504903 in 3.331m\n",
      "Score for blended models is 0.503226 in 3.430m\n",
      "Score for blended models is 0.504220 in 3.312m\n",
      "Score for blended models is 0.505516 in 3.318m\n",
      "Score for blended models is 0.504307 in 3.437m\n",
      "Score for blended models is 0.504315 in 3.337m\n",
      "Score for blended models is 0.505563 in 3.347m\n",
      "Score for blended models is 0.505214 in 3.439m\n",
      "Score for blended models is 0.506742 in 3.344m\n",
      "Score for blended models is 0.505178 in 3.342m\n",
      "Score for blended models is 0.504806 in 3.431m\n",
      "Score for blended models is 0.503505 in 3.333m\n",
      "Score for blended models is 0.506805 in 3.340m\n",
      "Score for blended models is 0.503864 in 3.329m\n",
      "Score for blended models is 0.504614 in 3.341m\n",
      "Score for blended models is 0.504357 in 3.349m\n",
      "Score for blended models is 0.504455 in 3.445m\n",
      "Score for blended models is 0.505644 in 3.350m\n",
      "Score for blended models is 0.505886 in 3.330m\n",
      "Score for blended models is 0.504505 in 3.435m\n",
      "Score for blended models is 0.506920 in 3.361m\n",
      "Score for blended models is 0.505085 in 3.346m\n",
      "Score for blended models is 0.507869 in 3.437m\n",
      "Score for blended models is 0.505433 in 3.347m\n",
      "Score for blended models is 0.506039 in 3.340m\n",
      "Score for blended models is 0.505690 in 3.433m\n",
      "Score for blended models is 0.503880 in 3.338m\n",
      "Score for blended models is 0.504475 in 3.335m\n",
      "Score for blended models is 0.505577 in 3.448m\n",
      "Score for blended models is 0.503701 in 3.350m\n",
      "Score for blended models is 0.505202 in 3.353m\n",
      "Score for blended models is 0.502479 in 3.439m\n",
      "Score for blended models is 0.504547 in 3.349m\n",
      "Score for blended models is 0.502996 in 3.368m\n",
      "Score for blended models is 0.502016 in 3.420m\n",
      "Score for blended models is 0.506172 in 3.361m\n",
      "Score for blended models is 0.504088 in 3.350m\n",
      "Score for blended models is 0.503839 in 3.349m\n",
      "Score for blended models is 0.503755 in 3.341m\n",
      "Score for blended models is 0.505161 in 3.380m\n",
      "Score for blended models is 0.504511 in 3.409m\n",
      "Score for blended models is 0.506023 in 3.346m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_2nd.shape[0], 3))\n",
    "test_total = np.zeros((test_2nd_mean.shape[0], 3))\n",
    "name_train_blend = '../tmp/train_adet_1.csv'\n",
    "name_test_blend = '../tmp/test_adet_1.csv'\n",
    "score_total = 0\n",
    "count = 100\n",
    "for n in range(count):\n",
    "    randomseed = n + 641\n",
    "    est = [AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=0.8, n_estimators=800)]\n",
    "    (train_blend_2nd_ADET,\n",
    "     test_blend_2nd_ADET,\n",
    "     blend_scores_2nd_ADET,\n",
    "     best_rounds_2nd_ADET) = ET_blend(est,\n",
    "                                 train_2nd,train_y,\n",
    "                                 test_2nd_mean,\n",
    "                                 5,randomseed)\n",
    "    train_total += train_blend_2nd_ADET\n",
    "    test_total += test_blend_2nd_ADET\n",
    "    score_total += np.mean(blend_scores_2nd_ADET)\n",
    "    \n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504837916784\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_2ndADET_100bagging_last_1_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_2ndADET_100bagging_last_1_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print score_total\n",
    "# print (np.mean(best_rounds_RFC,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for blended models is 0.504269 in 3.177m\n",
      "Score for blended models is 0.503586 in 3.219m\n",
      "Score for blended models is 0.504902 in 3.434m\n",
      "Score for blended models is 0.504372 in 3.392m\n",
      "Score for blended models is 0.505174 in 3.326m\n",
      "Score for blended models is 0.504976 in 3.514m\n",
      "Score for blended models is 0.504893 in 3.598m\n",
      "Score for blended models is 0.502550 in 3.525m\n",
      "Score for blended models is 0.505619 in 3.844m\n",
      "Score for blended models is 0.504359 in 3.630m\n",
      "Score for blended models is 0.506518 in 4.057m\n",
      "Score for blended models is 0.505075 in 3.886m\n",
      "Score for blended models is 0.506826 in 3.899m\n",
      "Score for blended models is 0.504432 in 3.915m\n",
      "Score for blended models is 0.505780 in 3.906m\n",
      "Score for blended models is 0.504274 in 3.761m\n",
      "Score for blended models is 0.504661 in 3.862m\n",
      "Score for blended models is 0.505961 in 4.047m\n",
      "Score for blended models is 0.504297 in 5.001m\n",
      "Score for blended models is 0.504014 in 5.124m\n",
      "Score for blended models is 0.503605 in 5.450m\n",
      "Score for blended models is 0.505919 in 5.283m\n",
      "Score for blended models is 0.504934 in 5.354m\n",
      "Score for blended models is 0.503485 in 5.023m\n",
      "Score for blended models is 0.502770 in 3.912m\n",
      "Score for blended models is 0.505518 in 3.745m\n",
      "Score for blended models is 0.504408 in 3.867m\n",
      "Score for blended models is 0.505938 in 3.699m\n",
      "Score for blended models is 0.506662 in 3.704m\n",
      "Score for blended models is 0.503446 in 3.740m\n",
      "Score for blended models is 0.506146 in 3.745m\n",
      "Score for blended models is 0.505491 in 3.755m\n",
      "Score for blended models is 0.504466 in 3.793m\n",
      "Score for blended models is 0.506666 in 3.721m\n",
      "Score for blended models is 0.505383 in 3.853m\n",
      "Score for blended models is 0.503855 in 3.713m\n",
      "Score for blended models is 0.504554 in 3.691m\n",
      "Score for blended models is 0.503219 in 3.850m\n",
      "Score for blended models is 0.503299 in 3.721m\n",
      "Score for blended models is 0.507119 in 3.717m\n",
      "Score for blended models is 0.504503 in 3.719m\n",
      "Score for blended models is 0.502750 in 3.687m\n",
      "Score for blended models is 0.503926 in 3.870m\n",
      "Score for blended models is 0.502077 in 3.681m\n",
      "Score for blended models is 0.506206 in 3.673m\n",
      "Score for blended models is 0.504556 in 3.732m\n",
      "Score for blended models is 0.502742 in 3.704m\n",
      "Score for blended models is 0.504790 in 3.772m\n",
      "Score for blended models is 0.506613 in 3.788m\n",
      "Score for blended models is 0.502255 in 3.741m\n",
      "Score for blended models is 0.505240 in 3.889m\n",
      "Score for blended models is 0.503056 in 3.721m\n",
      "Score for blended models is 0.504021 in 3.740m\n",
      "Score for blended models is 0.506315 in 3.852m\n",
      "Score for blended models is 0.503609 in 3.770m\n",
      "Score for blended models is 0.506717 in 4.772m\n",
      "Score for blended models is 0.503856 in 5.901m\n",
      "Score for blended models is 0.504416 in 6.118m\n",
      "Score for blended models is 0.503795 in 4.811m\n",
      "Score for blended models is 0.505952 in 5.106m\n",
      "Score for blended models is 0.503432 in 5.315m\n",
      "Score for blended models is 0.503131 in 4.362m\n",
      "Score for blended models is 0.505113 in 4.714m\n",
      "Score for blended models is 0.504025 in 5.979m\n",
      "Score for blended models is 0.505783 in 5.692m\n",
      "Score for blended models is 0.505305 in 5.443m\n",
      "Score for blended models is 0.505732 in 5.905m\n",
      "Score for blended models is 0.505384 in 6.059m\n",
      "Score for blended models is 0.506473 in 4.961m\n",
      "Score for blended models is 0.505351 in 4.313m\n",
      "Score for blended models is 0.503988 in 4.749m\n",
      "Score for blended models is 0.504026 in 3.777m\n",
      "Score for blended models is 0.502795 in 3.907m\n",
      "Score for blended models is 0.505811 in 4.129m\n",
      "Score for blended models is 0.502851 in 4.843m\n",
      "Score for blended models is 0.504894 in 4.523m\n",
      "Score for blended models is 0.504952 in 3.892m\n",
      "Score for blended models is 0.504854 in 4.168m\n",
      "Score for blended models is 0.506342 in 5.024m\n",
      "Score for blended models is 0.505449 in 4.341m\n",
      "Score for blended models is 0.504356 in 4.884m\n",
      "Score for blended models is 0.505565 in 4.348m\n",
      "Score for blended models is 0.505006 in 4.186m\n",
      "Score for blended models is 0.503265 in 3.646m\n",
      "Score for blended models is 0.504583 in 3.615m\n",
      "Score for blended models is 0.502899 in 3.621m\n",
      "Score for blended models is 0.505070 in 4.028m\n",
      "Score for blended models is 0.506105 in 4.632m\n",
      "Score for blended models is 0.505432 in 3.716m\n",
      "Score for blended models is 0.505026 in 3.765m\n",
      "Score for blended models is 0.504282 in 3.647m\n",
      "Score for blended models is 0.506986 in 3.637m\n",
      "Score for blended models is 0.504557 in 3.957m\n",
      "Score for blended models is 0.504641 in 4.059m\n",
      "Score for blended models is 0.505202 in 4.784m\n",
      "Score for blended models is 0.506069 in 3.918m\n",
      "Score for blended models is 0.505360 in 3.830m\n",
      "Score for blended models is 0.504715 in 3.834m\n",
      "Score for blended models is 0.506234 in 3.875m\n",
      "Score for blended models is 0.506101 in 4.546m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_2nd.shape[0], 3))\n",
    "test_total = np.zeros((test_2nd_mean.shape[0], 3))\n",
    "name_train_blend = '../tmp/train_adet_2.csv'\n",
    "name_test_blend = '../tmp/test_adet_2.csv'\n",
    "score_total = 0\n",
    "count = 100\n",
    "for n in range(count):\n",
    "    randomseed = n + 987654\n",
    "    est = [AdaBoostClassifier(ExtraTreesClassifier(n_estimators=1200,n_jobs= -1),learning_rate=0.8, n_estimators=800)]\n",
    "    (train_blend_2nd_ADET,\n",
    "     test_blend_2nd_ADET,\n",
    "     blend_scores_2nd_ADET,\n",
    "     best_rounds_2nd_ADET) = ET_blend(est,\n",
    "                                 train_2nd,train_y,\n",
    "                                 test_2nd_mean,\n",
    "                                 5,randomseed)\n",
    "    train_total += train_blend_2nd_ADET\n",
    "    test_total += test_blend_2nd_ADET\n",
    "    score_total += np.mean(blend_scores_2nd_ADET)\n",
    "    \n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504779591906\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../blend/train_blend_2ndADET_100bagging_last_2_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../blend/test_blend_2ndADET_100bagging_last_2_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print score_total\n",
    "# print (np.mean(best_rounds_RFC,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(\"../input/\" + 'test_BM_MB_add03052240.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_2ndADET_100bagging_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_total)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
