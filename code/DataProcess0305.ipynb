{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import re, nltk      \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "######## \n",
    "\n",
    "\n",
    "def description_sentiment(sentences):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    return pd.DataFrame(result).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 16)\n",
      "(74659, 15)\n",
      "49352\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file).reset_index()\n",
    "test_df = pd.read_json(test_file).reset_index()\n",
    "ntrain = train_df.shape[0]\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print ntrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sc_price\n",
    "tmp = pd.concat([train_df['price'],test_df['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "llimit = np.percentile(tmp.values, 1)\n",
    "\n",
    "train_df.loc[:,'sc_price'] = train_df['price'].values.reshape(-1, 1)\n",
    "test_df.loc[:,'sc_price'] = test_df['price'].values.reshape(-1, 1)\n",
    "\n",
    "train_df['price_outlier'] = 0\n",
    "test_df['price_outlier'] = 0\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['price_outlier']] = 1\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['price_outlier']] = 1\n",
    "train_df.loc[train_df['sc_price']<llimit, ['price_outlier']] = 1\n",
    "test_df.loc[test_df['sc_price']<llimit, ['price_outlier']] = 1\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "train_df.loc[train_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "test_df.loc[test_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "\n",
    "\n",
    "\n",
    "features_to_use  = [ \"sc_price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# price per bathrooms\n",
    "inx_train = train_df['bathrooms'] == 0\n",
    "inx_test = test_df['bathrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_ba_price'] = train_df.loc[non0_inx_train,'sc_price']\\\n",
    "                                                /train_df.loc[non0_inx_train,'bathrooms']\n",
    "test_df.loc[non0_inx_test,'sc_ba_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bathrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_ba_price'] = 0\n",
    "test_df.loc[inx_test,'sc_ba_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bathrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bathrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bathrooms0'] = 0\n",
    "test_df.loc[inx_test,'bathrooms0'] = 0\n",
    "\n",
    "# price per bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_be_price'] = train_df.loc[non0_inx_train,'sc_price'] \\\n",
    "                                                /train_df.loc[non0_inx_train,'bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_be_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_be_price'] = 0\n",
    "test_df.loc[inx_test,'sc_be_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bedrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bedrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bedrooms0'] = 0\n",
    "test_df.loc[inx_test,'bedrooms0'] = 0\n",
    "\n",
    "features_to_use.extend([\"sc_ba_price\", \"sc_be_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bathrooms\n",
    "\n",
    "ulimit = 5\n",
    "\n",
    "train_df['sc_bathrooms']=train_df['bathrooms']\n",
    "test_df['sc_bathrooms']=test_df['bathrooms']\n",
    "\n",
    "train_df['bathrooms_outlier'] = 0\n",
    "test_df['bathrooms_outlier'] = 0\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit, ['bathrooms_outlier']] = 1\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit, ['bathrooms_outlier']] = 1\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "\n",
    "# bedrooms\n",
    "\n",
    "ulimit = 8\n",
    "\n",
    "\n",
    "train_df['sc_bedrooms']=train_df['bedrooms']\n",
    "test_df['sc_bedrooms']=test_df['bedrooms']\n",
    "\n",
    "train_df['bedrooms_outlier'] = 0\n",
    "test_df['bedrooms_outlier'] = 0\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['bedrooms_outlier']] = 1\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit, ['bedrooms_outlier']] = 1\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['sc_bedrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit,['sc_bedrooms']] = ulimit\n",
    "\n",
    "# bathrooms / bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_babe'] = train_df.loc[non0_inx_train,'sc_bathrooms'] \\\n",
    "                                                /train_df.loc[non0_inx_train,'sc_bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_babe'] = test_df.loc[non0_inx_test,'sc_bathrooms']\\\n",
    "                                                /test_df.loc[non0_inx_test,'sc_bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_babe'] = 0\n",
    "test_df.loc[inx_test,'sc_babe'] = 0\n",
    "\n",
    "\n",
    "features_to_use.extend([\"sc_bathrooms\",  'sc_bedrooms', 'sc_babe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# longitude\n",
    "\n",
    "tmp = pd.concat([train_df['longitude'],test_df['longitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_longitude']=train_df['longitude']\n",
    "test_df['sc_longitude']=test_df['longitude']\n",
    "\n",
    "train_df['longitude_outlier'] = 0\n",
    "test_df['longitude_outlier'] = 0\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['longitude_outlier']] = 1\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['longitude_outlier']] = 1\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['longitude_outlier']] = 1\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['longitude_outlier']] = 1\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "\n",
    "# latitude\n",
    "\n",
    "tmp = pd.concat([train_df['latitude'],test_df['latitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_latitude']=train_df['latitude']\n",
    "test_df['sc_latitude']=test_df['latitude']\n",
    "\n",
    "train_df['latitude_outlier'] = 0\n",
    "test_df['latitude_outlier'] = 0\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['latitude_outlier']] = 1\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['latitude_outlier']] = 1\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['latitude_outlier']] = 1\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['latitude_outlier']] = 1\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "\n",
    "\n",
    "features_to_use.extend(['sc_longitude', \"sc_latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like weekday, month, day, hour from date columns #\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"created_month\", \"created_day\", \"created_hour\", 'created_weekday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count of words present in description column #\n",
    "train_df['clean_description'] = train_df['description'].apply(lambda x: x.replace('<p>', ''))\n",
    "train_df['clean_description'] = train_df['clean_description'].apply(lambda x: x.replace('<a', ''))\n",
    "train_df['clean_description'] = train_df['clean_description'].apply(lambda x: x.replace('website_redacted', '')) \n",
    "train_df['clean_description'] = train_df['clean_description'].apply(lambda x: x.replace('<br />', ''))\n",
    "train_df['clean_description'] = train_df['clean_description'].apply(lambda x: x.replace('<br/>', ''))\n",
    "train_df['clean_description'] = train_df['clean_description'].apply(lambda x: x.replace('&amp;', ''))\n",
    "\n",
    "test_df['clean_description'] = test_df['description'].apply(lambda x: x.replace('<p>', ''))\n",
    "test_df['clean_description'] = test_df['clean_description'].apply(lambda x: x.replace('<a', ''))\n",
    "test_df['clean_description'] = test_df['clean_description'].apply(lambda x: x.replace('website_redacted', '')) \n",
    "test_df['clean_description'] = test_df['clean_description'].apply(lambda x: x.replace('<br />', ''))\n",
    "test_df['clean_description'] = test_df['clean_description'].apply(lambda x: x.replace('<br/>', ''))\n",
    "test_df['clean_description'] = test_df['clean_description'].apply(lambda x: x.replace('&amp;', ''))\n",
    "\n",
    "test_df.loc[27992,'clean_description'] = 'This beatiful one bedroom is in a well maintained. \\\n",
    "                                            pet friendly pre-war walk-up building is located \\\n",
    "                                            on the border of Chelsea, the West Village, and \\\n",
    "                                            the trendy Meat Packing District and convenient to \\\n",
    "                                            everything in the city. Apartment Features: Private Balcony,  \\\n",
    "                                            Granite Counters, Stainless Steal Kitchen Appliances with Dishwasher.  \\\n",
    "                                            Spacious Bedroom (can fit a queen size bed). Nice Size Living \\\n",
    "                                            Room Building. Laundry in Building. Super lives in the \\\n",
    "                                            Area Pet Friendly Walk-up Pre-war Building Location:On the \\\n",
    "                                            Same Block as the A, C, E, and L Trains Steps from the Meat \\\n",
    "                                            Packing District and Highline Park. Trendy Bars, Restaurants, \\\n",
    "                                            and Night Life Activities Boutiques Convenient to the West Side \\\n",
    "                                            Highway and Holland Tunnel Historic Landmarks Cultural Activities.'\n",
    "\n",
    "train_df['description_tokens'] = train_df['clean_description'].apply(sent_tokenize)\n",
    "train_df['num_description_sent'] = train_df['description_tokens'].apply(len)\n",
    "test_df['description_tokens'] = test_df['clean_description'].apply(sent_tokenize)\n",
    "test_df['num_description_sent'] = test_df['description_tokens'].apply(len)\n",
    "\n",
    "train_df['num_description_words'] = train_df['clean_description']\\\n",
    "                                        .apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "test_df['num_description_words'] = test_df['clean_description']\\\n",
    "                                        .apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n",
    "\n",
    "# train_df['word_sent_desc'] = train_df['num_description_words'] / train_df['num_description_sent']\n",
    "# test_df['word_sent_desc'] = test_df['num_description_words'] / test_df['num_description_sent']\n",
    "\n",
    "train_df = pd.concat([train_df,train_df['description_tokens']\\\n",
    "                      .apply(description_sentiment)],axis=1)\n",
    "test_df = pd.concat([test_df,test_df['description_tokens']\\\n",
    "                     .apply(description_sentiment)],axis=1)\n",
    "\n",
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "features_to_use.extend([\"num_description_words\", 'num_description_sent', 'compound', 'neg', 'neu', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\", \"listing_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f+'_lbl'] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f+'_lbl'] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f+'_lbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc_price \t0.34122013083\n",
      "sc_ba_price \t0.668968094532\n",
      "sc_be_price \t0.462918736223\n",
      "sc_bathrooms \t-0.974792935849\n",
      "sc_bedrooms \t0.474243433469\n",
      "sc_longitude \t-12.8479106804\n",
      "num_photos \t0.475560723219\n",
      "num_features \t0.359523382085\n",
      "created_hour \t-0.389913999668\n",
      "num_description_words \t0.561581571914\n",
      "num_description_sent \t0.177791228437\n",
      "compound \t-0.239175562191\n",
      "neg \t-36.55973962\n",
      "pos \t-2.19234913267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_df[features_to_use],test_df[features_to_use]])\n",
    "skewed_cols = full_data[features_to_use].apply(lambda x: skew(x.dropna()))\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] - full_data[skewed_col].min() + 1)\n",
    "    print skewed_col, '\\t', lam\n",
    "    \n",
    "for col in features_to_use:\n",
    "    full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1))\n",
    "    train_df[col] = full_data.iloc[:ntrain][col]\n",
    "    test_df[col] = full_data.iloc[ntrain:][col]\n",
    "\n",
    "    \n",
    "del full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(['price_outlier',\"bedrooms0\",'bathrooms0',\"bathrooms_outlier\", 'bedrooms_outlier',\n",
    "                        'latitude_outlier', \"longitude_outlier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['address1'] = train_df['display_address']\n",
    "train_df['address1'] = train_df['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "test_df['address1'] = test_df['display_address']\n",
    "test_df['address1'] = test_df['address1'].apply(lambda x: x.lower())\n",
    "\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "\n",
    "train_df['address1'] = train_df['address1'].apply(lambda x: address_map_func(x))\n",
    "test_df['address1'] = test_df['address1'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n",
    "\n",
    "for col in new_cols:\n",
    "    train_df[col] = train_df['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "    test_df[col] = test_df['address1'].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "train_df['other_address'] = train_df[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "test_df['other_address'] = test_df[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "features_to_use.extend(['street', 'avenue', 'east', 'west', 'north', 'south','other_address'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train_df['manager_id_lbl'],test_df['manager_id_lbl']])\n",
    "managers_count = tmp.value_counts()\n",
    "\n",
    "train_df['top_10_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "train_df['top_25_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "train_df['top_5_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "train_df['top_50_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "train_df['top_1_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "train_df['top_2_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "train_df['top_15_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "train_df['top_20_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "train_df['top_30_manager'] = train_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)\n",
    "\n",
    "test_df['top_10_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "test_df['top_25_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "test_df['top_5_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "test_df['top_50_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "test_df['top_1_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "test_df['top_2_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "test_df['top_15_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "test_df['top_20_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "test_df['top_30_manager'] = test_df['manager_id_lbl'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)\n",
    "\n",
    "\n",
    "features_to_use.extend(['top_10_manager','top_25_manager','top_5_manager','top_50_manager','top_1_manager',\n",
    "                       'top_2_manager','top_15_manager','top_20_manager','top_30_manager'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['Zero_building_id'] = train_df['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "test_df['Zero_building_id'] = test_df['building_id'].apply(lambda x: 1 if x == '0' else 0)\n",
    "features_to_use.append('Zero_building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.concat([train_df['building_id_lbl'],test_df['building_id_lbl']])\n",
    "buildings_count = tmp.value_counts()\n",
    "\n",
    "train_df['top_10_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_df['top_25_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_df['top_5_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_df['top_50_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_df['top_1_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_df['top_2_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_df['top_15_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_df['top_20_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_df['top_30_building'] = train_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n",
    "\n",
    "test_df['top_10_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "test_df['top_25_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "test_df['top_5_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "test_df['top_50_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "test_df['top_1_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "test_df['top_2_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "test_df['top_15_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "test_df['top_20_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "test_df['top_30_building'] = test_df['building_id_lbl'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n",
    "\n",
    "\n",
    "features_to_use.extend(['top_10_building','top_25_building','top_5_building','top_50_building','top_1_building',\n",
    "                       'top_2_building','top_15_building','top_20_building','top_30_building'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 76) (74659, 75)\n"
     ]
    }
   ],
   "source": [
    "print train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "train_y = train_df['interest_level'].apply(lambda x: y_map[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 200\n",
    ")\n",
    "\n",
    "vectorizer.fit(pd.concat([train_df['clean_description'],test_df['clean_description']]))\n",
    "\n",
    "tr_desc_sparse = vectorizer.transform(train_df[\"clean_description\"])\n",
    "te_desc_sparse = vectorizer.transform(test_df[\"clean_description\"])\n",
    "desc_sparse_cols = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 200\n",
    ")\n",
    "\n",
    "train_df['clean_features'] = train_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['clean_features'] = test_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "vectorizer.fit(pd.concat([train_df['clean_features'],test_df['clean_features']]))\n",
    "\n",
    "tr_feat_sparse = vectorizer.transform(train_df[\"clean_features\"])\n",
    "te_feat_sparse = vectorizer.transform(test_df[\"clean_features\"])\n",
    "feat_sparse_cols = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_feautres = '../input/featurestouse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_train = '../input/train_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_test = '../input/test_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_y = '../input/y_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "\n",
    "name_tr_desc_sparse = '../input/tr_desc_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_tr_feat_sparse = '../input/tr_feat_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_te_desc_sparse = '../input/te_desc_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_te_feat_sparse = '../input/te_feat_sparse_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "\n",
    "name_desc_sparse_cols = '../input/desc_sparse_cols_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "name_feat_sparse_cols = '../input/feat_sparse_cols_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.pkl'\n",
    "\n",
    "\n",
    "pd.to_pickle(features_to_use,name_feautres)\n",
    "pd.to_pickle(train_df, name_train)\n",
    "pd.to_pickle(test_df, name_test)\n",
    "pd.to_pickle(train_y, name_y)\n",
    "\n",
    "pd.to_pickle(tr_desc_sparse, name_tr_desc_sparse)\n",
    "pd.to_pickle(tr_feat_sparse, name_tr_feat_sparse)\n",
    "pd.to_pickle(te_desc_sparse, name_te_desc_sparse)\n",
    "pd.to_pickle(te_feat_sparse, name_te_feat_sparse)\n",
    "\n",
    "pd.to_pickle(desc_sparse_cols,name_desc_sparse_cols)\n",
    "pd.to_pickle(feat_sparse_cols,name_feat_sparse_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df['features'] = train_df[\"features\"]\\\n",
    "#                         .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "#                         .apply(lambda x: x.lower())\n",
    "# test_df['features'] = test_df[\"features\"]\\\n",
    "#                         .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "#                         .apply(lambda x: x.lower())\n",
    "\n",
    "# print(train_df[\"features\"].head())\n",
    "# tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "# tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "# te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "# sparse_features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 220) (74659, 220)\n"
     ]
    }
   ],
   "source": [
    "# train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "# test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "# target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "# weight_num_map = {'high':1, 'medium':1, 'low':1}\n",
    "# train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "# W_train = np.array(train_df['interest_level'].apply(lambda x: weight_num_map[x]))\n",
    "\n",
    "# all_features = features_to_use + sparse_features\n",
    "# print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
