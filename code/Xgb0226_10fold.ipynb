{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 16)\n",
      "(74659, 15)\n",
      "49352\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file).reset_index()\n",
    "test_df = pd.read_json(test_file).reset_index()\n",
    "ntrain = train_df.shape[0]\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sc_price\n",
    "tmp = pd.concat([train_df['price'],test_df['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "llimit = np.percentile(tmp.values, 1)\n",
    "\n",
    "train_df.loc[:,'sc_price'] = train_df['price'].values.reshape(-1, 1)\n",
    "test_df.loc[:,'sc_price'] = test_df['price'].values.reshape(-1, 1)\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "train_df.loc[train_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "test_df.loc[test_df['sc_price']<llimit, ['sc_price']] = llimit\n",
    "\n",
    "\n",
    "\n",
    "# sc_ba_price\n",
    "inx_train = train_df['bathrooms'] == 0\n",
    "inx_test = test_df['bathrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_ba_price'] = train_df.loc[non0_inx_train,'sc_price']\\\n",
    "                                                /train_df.loc[non0_inx_train,'bathrooms']\n",
    "test_df.loc[non0_inx_test,'sc_ba_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bathrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_ba_price'] = 0\n",
    "test_df.loc[inx_test,'sc_ba_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bathrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bathrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bathrooms0'] = 0\n",
    "test_df.loc[inx_test,'bathrooms0'] = 0\n",
    "\n",
    "# price per bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "\n",
    "train_df.loc[non0_inx_train,'sc_be_price'] = train_df.loc[non0_inx_train,'sc_price'] \\\n",
    "                                                /train_df.loc[non0_inx_train,'bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_be_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_be_price'] = 0\n",
    "test_df.loc[inx_test,'sc_be_price'] = 0\n",
    "\n",
    "train_df.loc[non0_inx_train,'bedrooms0'] = 1\n",
    "test_df.loc[non0_inx_test,'bedrooms0'] = 1\n",
    "\n",
    "train_df.loc[inx_train,'bedrooms0'] = 0\n",
    "test_df.loc[inx_test,'bedrooms0'] = 0\n",
    "# bathrooms\n",
    "\n",
    "ulimit = 5\n",
    "\n",
    "train_df['sc_bathrooms']=train_df['bathrooms']\n",
    "test_df['sc_bathrooms']=test_df['bathrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "\n",
    "# bedrooms\n",
    "\n",
    "ulimit = 8\n",
    "\n",
    "train_df['sc_bedrooms']=train_df['bedrooms']\n",
    "test_df['sc_bedrooms']=test_df['bedrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['sc_bedrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit,['sc_bedrooms']] = ulimit\n",
    "\n",
    "# longitude\n",
    "\n",
    "tmp = pd.concat([train_df['longitude'],test_df['longitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_longitude']=train_df['longitude']\n",
    "test_df['sc_longitude']=test_df['longitude']\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "\n",
    "# latitude\n",
    "\n",
    "tmp = pd.concat([train_df['latitude'],test_df['latitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_latitude']=train_df['latitude']\n",
    "test_df['sc_latitude']=test_df['latitude']\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "\n",
    "\n",
    "features_to_use  = [\"sc_bathrooms\", \"sc_bedrooms\", \"sc_latitude\", \"sc_longitude\",\n",
    "                    \"sc_price\", \"sc_ba_price\", \"sc_be_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \n",
    "                        \"created_day\", \"created_hour\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc_bathrooms \t-0.974792935849\n",
      "sc_bedrooms \t0.474243433469\n",
      "sc_longitude \t-12.8479106804\n",
      "sc_price \t0.34122013083\n",
      "sc_ba_price \t0.668968094532\n",
      "sc_be_price \t0.462918736223\n",
      "num_photos \t0.475560723219\n",
      "num_features \t0.359523382085\n",
      "num_description_words \t0.543711145224\n",
      "created_hour \t-0.389913999668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_df,test_df])\n",
    "\n",
    "# SSL = preprocessing.StandardScaler()\n",
    "# for col in features_to_use:\n",
    "#     full_data[col], lam = boxcox(full_data[col] - full_data[col].min() + 1)\n",
    "#     full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1)) \n",
    "skewed_cols = full_data[features_to_use].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] - full_data[skewed_col].min() + 1)\n",
    "    print skewed_col, '\\t', lam\n",
    "for col in features_to_use:\n",
    "    full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1))\n",
    "    train_df[col] = full_data.iloc[:ntrain][col]\n",
    "    test_df[col] = full_data.iloc[ntrain:][col]\n",
    "\n",
    "    \n",
    "del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig = train_df[features_to_use].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fig = test_df[features_to_use].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_df.sc_longitude.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend([\"listing_id\",\"bedrooms0\",'bathrooms0'])\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    doorman elevator fitness_center cats_allowed d...\n",
      "2    laundry_in_building dishwasher hardwood_floors...\n",
      "3                               hardwood_floors no_fee\n",
      "4                                              pre-war\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "test_df['features'] = test_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "sparse_features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 220) (74659, 220)\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "weight_num_map = {'high':1, 'medium':1, 'low':1}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "W_train = np.array(train_df['interest_level'].apply(lambda x: weight_num_map[x]))\n",
    "\n",
    "all_features = features_to_use + sparse_features\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_dataset = xgb.DMatrix(train_X, label = train_y,\n",
    "#                          weight = W_train,\n",
    "#                          feature_names = all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44416, 218)\n",
      "(4936, 218)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.90, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rgr = xgb.XGBClassifier(objective = 'multi:softprob',\n",
    "#                        learning_rate = 0.1,\n",
    "#                        n_estimators = 10000,\n",
    "#                        nthread = -1)\n",
    "\n",
    "# rgr.fit(X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "# #         num_class = 3,\n",
    "#         early_stopping_rounds=20,\n",
    "#         verbose=False\n",
    "#        )\n",
    "\n",
    "# tmp1 = rgr.predict_proba(test_X)\n",
    "# tmp1=pd.DataFrame(tmp1)\n",
    "# tmp1.columns = ['high','medium','low']\n",
    "# plt.scatter(range(74659),tmp1.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.556399\n",
      "4 \t0.556165\n",
      "5 \t0.553328\n",
      "6 \t0.552869\n",
      "7 \t0.557626\n",
      "8 \t0.556641\n",
      "9 \t0.559613\n",
      "10 \t0.562173\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7,8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = train_param\n",
    "print train_param\n",
    "# 3 \t0.550966\n",
    "# 4 \t0.546335\n",
    "# 5 \t0.546948\n",
    "# 6 \t0.548233\n",
    "# 7 \t0.548868\n",
    "# 8 \t0.551074\n",
    "# 9 \t0.553485\n",
    "# 10 \t0.557234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t0.552672\n",
      "10 \t0.552702\n",
      "20 \t0.554569\n",
      "50 \t0.555714\n",
      "80 \t0.558212\n",
      "120 \t0.556721\n",
      "180 \t0.556979\n",
      "240 \t0.563283\n",
      "300 \t0.565675\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [5,10,20,50,80,120,180,240,300]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.544995\n",
      "0.4 \t0.548923\n",
      "0.5 \t0.547074\n",
      "0.6 \t0.546579\n",
      "0.7 \t0.551113\n",
      "0.8 \t0.550493\n",
      "0.9 \t0.551921\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = 0.6\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.551334\n",
      "0.6 \t0.55245\n",
      "0.7 \t0.54911\n",
      "0.8 \t0.550137\n",
      "0.9 \t0.548913\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t0.548913\n",
      "0.3 \t0.549075\n",
      "0.6 \t0.547017\n",
      "0.9 \t0.546771\n",
      "1.2 \t0.549405\n",
      "1.5 \t0.546167\n",
      "1.8 \t0.548024\n",
      "2.1 \t0.548317\n",
      "2.4 \t0.546834\n",
      "2.7 \t0.549393\n",
      "3.0 \t0.545459\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1093]\ttrain-mlogloss:0.401341+0.00132894\ttest-mlogloss:0.542346+0.00573164\n",
      "\n",
      "    1 | 08m25s | \u001b[35m  -0.54235\u001b[0m | \u001b[32m            0.8668\u001b[0m | \u001b[32m   1.9502\u001b[0m | \u001b[32m     4.7133\u001b[0m | \u001b[32m            2.6959\u001b[0m | \u001b[32m     0.8435\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[566]\ttrain-mlogloss:0.420788+0.00165193\ttest-mlogloss:0.547142+0.00599595\n",
      "\n",
      "    2 | 05m31s |   -0.54714 |             0.8646 |    0.3752 |      5.0573 |            39.3628 |      0.7776 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[548]\ttrain-mlogloss:0.422935+0.00117962\ttest-mlogloss:0.548102+0.00674611\n",
      "\n",
      "    3 | 05m43s |   -0.54810 |             0.9447 |    0.7051 |      5.2825 |            39.3566 |      0.7530 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1571]\ttrain-mlogloss:0.445406+0.00118658\ttest-mlogloss:0.547158+0.00519372\n",
      "\n",
      "    4 | 09m18s |   -0.54716 |             0.8470 |    1.3909 |      3.0581 |            32.1040 |      0.8681 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mlogloss:0.443695+0.00105395\ttest-mlogloss:0.548051+0.00615713\n",
      "\n",
      "    5 | 09m02s |   -0.54805 |             0.8186 |    0.5498 |      3.8086 |            36.3637 |      0.9341 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2128]\ttrain-mlogloss:0.432914+0.00127989\ttest-mlogloss:0.545702+0.00554118\n",
      "\n",
      "    6 | 13m25s |   -0.54570 |             0.9751 |    1.8401 |      3.6989 |            19.7273 |      0.9287 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1363]\ttrain-mlogloss:0.444654+0.00120094\ttest-mlogloss:0.547281+0.00587757\n",
      "\n",
      "    7 | 09m00s |   -0.54728 |             0.9991 |    1.0376 |      3.6066 |            25.2410 |      0.7373 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[573]\ttrain-mlogloss:0.399571+0.00146509\ttest-mlogloss:0.54511+0.00536791\n",
      "\n",
      "    8 | 05m52s |   -0.54511 |             0.9329 |    1.0003 |      5.7522 |            16.6616 |      0.9303 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[589]\ttrain-mlogloss:0.405063+0.00194765\ttest-mlogloss:0.545551+0.0058321\n",
      "\n",
      "    9 | 05m18s |   -0.54555 |             0.7554 |    0.6974 |      5.8611 |            23.5101 |      0.7794 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[698]\ttrain-mlogloss:0.410685+0.000737741\ttest-mlogloss:0.547187+0.00594919\n",
      "\n",
      "   10 | 06m23s |   -0.54719 |             0.7976 |    0.6546 |      5.6029 |            48.3485 |      0.9046 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[877]\ttrain-mlogloss:0.39596+0.000707632\ttest-mlogloss:0.544678+0.00640045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/helpers.py:95: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  z = (mean - y_max - xi)/std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 07m58s |   -0.54468 |             0.9993 |    0.5129 |      4.0034 |             2.0828 |      0.8933 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1153]\ttrain-mlogloss:0.407059+0.00179506\ttest-mlogloss:0.54332+0.00567749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.30528409e-05]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00028024]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.26466873e-05]), 'nit': 6, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 08m23s |   -0.54332 |             0.7552 |    2.0722 |      4.9775 |             7.0850 |      0.8323 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[587]\ttrain-mlogloss:0.377684+0.00163147\ttest-mlogloss:0.544077+0.00571811\n",
      "\n",
      "   13 | 05m32s |   -0.54408 |             0.7474 |    1.3964 |      5.3378 |             2.3705 |      0.7276 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2063]\ttrain-mlogloss:0.420522+0.00145732\ttest-mlogloss:0.54445+0.00571767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00010604]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 12m13s |   -0.54445 |             0.8287 |    1.9510 |      3.8450 |             2.0672 |      0.8212 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1380]\ttrain-mlogloss:0.454491+0.00136536\ttest-mlogloss:0.549283+0.00592362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00033041]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 08m33s |   -0.54928 |             0.8541 |    0.5258 |      3.0379 |            46.5676 |      0.8193 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[642]\ttrain-mlogloss:0.38395+0.00211097\ttest-mlogloss:0.545517+0.00634387\n",
      "\n",
      "   16 | 05m55s |   -0.54552 |             0.7739 |    0.6402 |      5.7887 |             7.8536 |      0.9899 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[962]\ttrain-mlogloss:0.400896+0.00119462\ttest-mlogloss:0.54542+0.00588859\n",
      "\n",
      "   17 | 08m32s |   -0.54542 |             0.9962 |    1.9861 |      4.1640 |             3.6469 |      0.7160 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[427]\ttrain-mlogloss:0.474899+0.0099761\ttest-mlogloss:0.552888+0.00619794\n",
      "\n",
      "   18 | 04m41s |   -0.55289 |             1.0000 |    2.1000 |      5.8205 |            11.1328 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[590]\ttrain-mlogloss:0.411467+0.00095941\ttest-mlogloss:0.546195+0.00616944\n",
      "\n",
      "   19 | 05m42s |   -0.54619 |             0.8154 |    0.3837 |      5.6261 |            27.4351 |      0.9711 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3748]\ttrain-mlogloss:0.461767+0.005936\ttest-mlogloss:0.547081+0.00592719\n",
      "\n",
      "   20 | 22m11s |   -0.54708 |             0.9338 |    1.6663 |      3.6007 |             3.3060 |      0.9941 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[847]\ttrain-mlogloss:0.40373+0.000762137\ttest-mlogloss:0.543742+0.00553166\n",
      "\n",
      "   21 | 07m31s |   -0.54374 |             0.9760 |    1.6020 |      4.8312 |             1.9888 |      0.7914 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[605]\ttrain-mlogloss:0.399078+0.00168164\ttest-mlogloss:0.546069+0.00628979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00016017]), 'nit': 6, 'funcalls': 62}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 05m40s |   -0.54607 |             0.7401 |    0.7955 |      5.6141 |            19.4741 |      0.7050 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[744]\ttrain-mlogloss:0.384721+0.00152581\ttest-mlogloss:0.543492+0.00623367\n",
      "\n",
      "   23 | 07m05s |   -0.54349 |             0.7787 |    2.0531 |      5.2944 |             6.4305 |      0.7946 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[883]\ttrain-mlogloss:0.414706+0.000975753\ttest-mlogloss:0.545368+0.00585206\n",
      "\n",
      "   24 | 07m04s |   -0.54537 |             0.8570 |    0.7085 |      4.1269 |            16.1609 |      0.9140 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[752]\ttrain-mlogloss:0.405134+0.00251367\ttest-mlogloss:0.546578+0.00601038\n",
      "\n",
      "   25 | 06m32s |   -0.54658 |             0.7148 |    0.8454 |      5.5625 |            32.6769 |      0.9972 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[932]\ttrain-mlogloss:0.413185+0.00117827\ttest-mlogloss:0.545989+0.0061361\n",
      "\n",
      "   26 | 08m56s |   -0.54599 |             0.8596 |    2.0998 |      5.9371 |            45.5257 |      0.8906 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[731]\ttrain-mlogloss:0.417904+0.000829278\ttest-mlogloss:0.546723+0.00597709\n",
      "\n",
      "   27 | 06m27s |   -0.54672 |             0.7069 |    1.2359 |      5.0935 |            49.8025 |      0.9456 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[824]\ttrain-mlogloss:0.406877+0.00216968\ttest-mlogloss:0.545291+0.00652732\n",
      "\n",
      "   28 | 08m21s |   -0.54529 |             0.9324 |    2.0653 |      5.3196 |            25.2849 |      0.9494 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1044]\ttrain-mlogloss:0.425248+0.00144535\ttest-mlogloss:0.546766+0.00613904\n",
      "\n",
      "   29 | 07m29s |   -0.54677 |             0.7403 |    1.0093 |      4.7279 |            42.8639 |      0.9455 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[695]\ttrain-mlogloss:0.410234+0.0011553\ttest-mlogloss:0.545407+0.00522425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00029022]), 'nit': 4, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  4.00588069e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 06m29s |   -0.54541 |             0.7716 |    1.8068 |      5.8513 |            30.7775 |      0.7993 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1502]\ttrain-mlogloss:0.445836+0.00144625\ttest-mlogloss:0.54758+0.00574224\n",
      "\n",
      "   31 | 08m50s |   -0.54758 |             0.7966 |    1.4983 |      3.5242 |            28.6523 |      0.7825 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1763]\ttrain-mlogloss:0.426849+0.00122329\ttest-mlogloss:0.545375+0.00575754\n",
      "\n",
      "   32 | 11m55s |   -0.54538 |             0.7091 |    1.5837 |      3.6244 |             8.1661 |      0.8457 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1469]\ttrain-mlogloss:0.436492+0.00114213\ttest-mlogloss:0.546639+0.0056767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  3.21109546e-05]), 'nit': 4, 'funcalls': 54}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 10m05s |   -0.54664 |             0.8338 |    0.6415 |      3.6306 |            18.9455 |      0.8684 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1701]\ttrain-mlogloss:0.444753+0.00124344\ttest-mlogloss:0.546655+0.00574181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.81156142e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 09m20s |   -0.54665 |             0.7142 |    1.6389 |      3.1005 |            24.5330 |      0.8899 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1491]\ttrain-mlogloss:0.452802+0.00108144\ttest-mlogloss:0.54934+0.00599488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.83255179e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 09m18s |   -0.54934 |             0.8792 |    1.6112 |      3.0527 |            42.9291 |      0.7541 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[884]\ttrain-mlogloss:0.402449+0.0015482\ttest-mlogloss:0.545158+0.00689407\n",
      "\n",
      "   36 | 07m17s |   -0.54516 |             0.9046 |    0.3665 |      4.6079 |             5.7827 |      0.9339 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1518]\ttrain-mlogloss:0.429723+0.00119856\ttest-mlogloss:0.546094+0.00582152\n",
      "\n",
      "   37 | 09m33s |   -0.54609 |             0.9016 |    0.7308 |      3.1628 |            12.6465 |      0.8824 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1578]\ttrain-mlogloss:0.436717+0.00119455\ttest-mlogloss:0.545849+0.0058345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00014088]), 'nit': 4, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 | 09m09s |   -0.54585 |             0.7854 |    1.6182 |      3.4167 |            13.7739 |      0.8322 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[732]\ttrain-mlogloss:0.450238+0.0010233\ttest-mlogloss:0.549448+0.00548907\n",
      "\n",
      "   39 | 05m38s |   -0.54945 |             0.7964 |    0.5751 |      4.9842 |            44.9441 |      0.7086 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1147]\ttrain-mlogloss:0.415803+0.00220998\ttest-mlogloss:0.545731+0.00633034\n",
      "\n",
      "   40 | 09m36s |   -0.54573 |             0.7258 |    1.9244 |      5.7463 |            46.7330 |      0.9780 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2241]\ttrain-mlogloss:0.45034+0.00213986\ttest-mlogloss:0.548627+0.00610175\n",
      "\n",
      "   41 | 13m44s |   -0.54863 |             0.9314 |    1.3956 |      3.6243 |            49.0671 |      0.9729 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[616]\ttrain-mlogloss:0.426598+0.00159515\ttest-mlogloss:0.546703+0.00572046\n",
      "\n",
      "   42 | 06m28s |   -0.54670 |             0.9371 |    2.0940 |      5.7122 |            36.6892 |      0.7947 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[894]\ttrain-mlogloss:0.411074+0.00132805\ttest-mlogloss:0.545699+0.00594749\n",
      "\n",
      "   43 | 06m45s |   -0.54570 |             0.7832 |    0.3476 |      4.9597 |            15.4925 |      0.7352 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[813]\ttrain-mlogloss:0.424286+0.00119387\ttest-mlogloss:0.546357+0.00613976\n",
      "\n",
      "   44 | 06m37s |   -0.54636 |             0.8694 |    0.4712 |      4.2705 |            22.7683 |      0.7880 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1491]\ttrain-mlogloss:0.453876+0.0012675\ttest-mlogloss:0.54816+0.00608291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00019838]), 'nit': 4, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00015251]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 | 09m10s |   -0.54816 |             0.8511 |    1.7619 |      3.7501 |            38.0995 |      0.7443 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[603]\ttrain-mlogloss:0.421017+0.00124178\ttest-mlogloss:0.547274+0.00539585\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00156888]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 | 05m30s |   -0.54727 |             0.7354 |    1.4790 |      5.6728 |            34.0076 |      0.7098 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[788]\ttrain-mlogloss:0.416902+0.00171272\ttest-mlogloss:0.546026+0.00653838\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00232891]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00010399]), 'nit': 7, 'funcalls': 65}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47 | 08m04s |   -0.54603 |             0.7913 |    2.0545 |      5.5018 |            39.8571 |      0.8385 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1046]\ttrain-mlogloss:0.425592+0.00155637\ttest-mlogloss:0.546166+0.00545797\n",
      "\n",
      "   48 | 11m04s |   -0.54617 |             0.7892 |    2.0132 |      4.8927 |            26.4855 |      0.7682 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[629]\ttrain-mlogloss:0.390545+0.00148152\ttest-mlogloss:0.545658+0.00690329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00094195]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   49 | 09m36s |   -0.54566 |             0.9852 |    0.3648 |      5.9029 |            22.6354 |      0.8157 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[897]\ttrain-mlogloss:0.420927+0.00130185\ttest-mlogloss:0.546387+0.00584617\n",
      "\n",
      "   50 | 07m26s |   -0.54639 |             0.8990 |    0.4912 |      4.2103 |            30.6949 |      0.8513 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=10,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (3,6),\n",
    "        'min_child_weight': (1,50),\n",
    "        'colsample_bytree': (0.7,1),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0.3,2.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.977517</td>\n",
       "      <td>7.085047</td>\n",
       "      <td>0.755216</td>\n",
       "      <td>0.832299</td>\n",
       "      <td>2.072250</td>\n",
       "      <td>-0.543320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.294358</td>\n",
       "      <td>6.430458</td>\n",
       "      <td>0.778670</td>\n",
       "      <td>0.794628</td>\n",
       "      <td>2.053074</td>\n",
       "      <td>-0.543492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.831209</td>\n",
       "      <td>1.988818</td>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.791368</td>\n",
       "      <td>1.601972</td>\n",
       "      <td>-0.543742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.337825</td>\n",
       "      <td>2.370541</td>\n",
       "      <td>0.747375</td>\n",
       "      <td>0.727643</td>\n",
       "      <td>1.396417</td>\n",
       "      <td>-0.544077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.844964</td>\n",
       "      <td>2.067151</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>1.951036</td>\n",
       "      <td>-0.544450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.003396</td>\n",
       "      <td>2.082787</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.893346</td>\n",
       "      <td>0.512947</td>\n",
       "      <td>-0.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.607936</td>\n",
       "      <td>5.782687</td>\n",
       "      <td>0.904582</td>\n",
       "      <td>0.933866</td>\n",
       "      <td>0.366536</td>\n",
       "      <td>-0.545158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.319597</td>\n",
       "      <td>25.284915</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.949350</td>\n",
       "      <td>2.065272</td>\n",
       "      <td>-0.545291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.126909</td>\n",
       "      <td>16.160868</td>\n",
       "      <td>0.857025</td>\n",
       "      <td>0.913964</td>\n",
       "      <td>0.708522</td>\n",
       "      <td>-0.545368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.624415</td>\n",
       "      <td>8.166120</td>\n",
       "      <td>0.709067</td>\n",
       "      <td>0.845712</td>\n",
       "      <td>1.583732</td>\n",
       "      <td>-0.545375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "1    4.977517          7.085047          0.755216   0.832299  2.072250   \n",
       "12   5.294358          6.430458          0.778670   0.794628  2.053074   \n",
       "10   4.831209          1.988818          0.976038   0.791368  1.601972   \n",
       "2    5.337825          2.370541          0.747375   0.727643  1.396417   \n",
       "3    3.844964          2.067151          0.828660   0.821156  1.951036   \n",
       "0    4.003396          2.082787          0.999265   0.893346  0.512947   \n",
       "25   4.607936          5.782687          0.904582   0.933866  0.366536   \n",
       "17   5.319597         25.284915          0.932358   0.949350  2.065272   \n",
       "13   4.126909         16.160868          0.857025   0.913964  0.708522   \n",
       "21   3.624415          8.166120          0.709067   0.845712  1.583732   \n",
       "\n",
       "       score  \n",
       "1  -0.543320  \n",
       "12 -0.543492  \n",
       "10 -0.543742  \n",
       "2  -0.544077  \n",
       "3  -0.544450  \n",
       "0  -0.544678  \n",
       "25 -0.545158  \n",
       "17 -0.545291  \n",
       "13 -0.545368  \n",
       "21 -0.545375  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.03)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]   \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds,test_blend_x_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.6,\n",
      "       gamma=3, learning_rate=0.03, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=5, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.9)\n",
      "Model 1 fold 1\n"
     ]
    }
   ],
   "source": [
    "estimators = [xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 5,\n",
    "                              colsample_bytree = 0.6,\n",
    "                              subsample = 0.9,\n",
    "                              gamma = 3),\n",
    "#              xgb.XGBClassifier(max_depth = 5,\n",
    "#                               min_child_weight = 6,\n",
    "#                               colsample_bytree = 0.778670,\n",
    "#                               subsample = 0.794628,\n",
    "#                               gamma = 2.053074)\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xgb_params = [{'max_depth':6,\n",
    "#                'min_child_weight':5,\n",
    "#                'colsample_bytree':0.6,\n",
    "#                'subsample':0.9,\n",
    "#                'gamma':3},\n",
    "# #               score -0.543320        \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':6,\n",
    "#                'colsample_bytree':0.778670,\n",
    "#                'subsample':0.794628,\n",
    "#                'gamma':2.053074},\n",
    "#               score -0.543492\n",
    "#               {'max_depth':4,\n",
    "#                'min_child_weight':1,\n",
    "#                'colsample_bytree':0.976038,\n",
    "#                'subsample':0.791368,\n",
    "#                'gamma':1.601972},\n",
    "# #               score -0.543742  \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.747375,\n",
    "#                'subsample':0.727643,\n",
    "#                'gamma':1.396417},\n",
    "# #               score -0.544077      \n",
    "#               {'max_depth':3,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.828660,\n",
    "#                'subsample':0.821156,\n",
    "#                'gamma':1.951036}\n",
    "# #               score -0.544450\n",
    "#              ]\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb,tmp) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n",
    "\n",
    "# print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "# print (np.mean(best_rounds_xgb_le,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54393943  0.54340168  0.54486921  0.54463413  0.5455042 ]\n",
      "4024.2\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_xgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05068891,  0.47914108,  0.47017003],\n",
       "       [ 0.00355554,  0.03459964,  0.96184484],\n",
       "       [ 0.09190651,  0.43008367,  0.47800982],\n",
       "       ..., \n",
       "       [ 0.04040814,  0.38965375,  0.5699381 ],\n",
       "       [ 0.06617295,  0.5109495 ,  0.42287757],\n",
       "       [ 0.0445355 ,  0.33600247,  0.61946204]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb[:,:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
