{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU,ParametricSoftplus,ThresholdedReLU,SReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.regularizers import WeightRegularizer, ActivityRegularizer,l2, activity_l2\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.ravel(pd.read_csv('../input/' + 'labels_BrandenMurray.csv'))\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "names = ['low_0','medium_0','high_0',\n",
    "        'low_1','medium_1','high_1',\n",
    "        'low_2','medium_2','high_2',\n",
    "        'low_3','medium_3','high_3',\n",
    "        'low_4','medium_4','high_4',\n",
    "        'low_5','medium_5','high_5',\n",
    "        'low_6','medium_6','high_6',\n",
    "        'low_7','medium_7','high_7',\n",
    "        'low_8','medium_8','high_8',\n",
    "        'low_9','medium_9','high_9']\n",
    "\n",
    "data_path = \"../2ndlast/\"\n",
    "total_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.355361      0.544070    0.100569\n",
      "1   0.508303      0.446720    0.044978\n",
      "2   0.603091      0.349880    0.047029\n",
      "3   0.616221      0.328639    0.055140\n",
      "4   0.947230      0.049863    0.002907\n",
      "   rfc_low_0  rfc_medium_0  rfc_high_0\n",
      "0   0.288217      0.532268    0.179515\n",
      "1   0.970891      0.025801    0.003308\n",
      "2   0.908912      0.078535    0.012553\n",
      "3   0.400539      0.476918    0.122542\n",
      "4   0.700470      0.269945    0.029586\n"
     ]
    }
   ],
   "source": [
    "# RFC 1st level \n",
    "file_train      = 'train_blend_RFC_entropy_last_2017-04-21-11-06' + '.csv'\n",
    "file_test_mean  = 'test_blend_RFC_entropy_mean_last_2017-04-21-11-06' + '.csv'\n",
    "\n",
    "train_rfc = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_rfc  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_rfc.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_rfc.columns = ['rfc_' + x for x in names[:n_column]]\n",
    "test_rfc.columns  = ['rfc_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_rfc.iloc[:5,:3]\n",
    "\n",
    "print test_rfc.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.259359     0.652577   0.088065\n",
      "1  0.738646     0.234238   0.027115\n",
      "2  0.396965     0.512991   0.090045\n",
      "3  0.647052     0.312537   0.040412\n",
      "4  0.923203     0.072255   0.004543\n",
      "   LR_low_0  LR_medium_0  LR_high_0\n",
      "0  0.282172     0.549423   0.168405\n",
      "1  0.955532     0.040091   0.004377\n",
      "2  0.925692     0.065197   0.009112\n",
      "3  0.631038     0.309690   0.059272\n",
      "4  0.803117     0.187725   0.009158\n"
     ]
    }
   ],
   "source": [
    "# LR 1st level\n",
    "file_train      = 'train_blend_LR_last_2017-04-21-11-16' + '.csv'\n",
    "file_test_mean  = 'test_blend_LR_mean_last_2017-04-21-11-16' + '.csv'\n",
    "\n",
    "train_LR = pd.read_csv(data_path + file_train, header = None)\n",
    "test_LR  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_LR.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_LR.columns = ['LR_' + x for x in names[:n_column]]\n",
    "test_LR.columns  = ['LR_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_LR.iloc[:5,:3]\n",
    "print test_LR.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.332903     0.538085   0.129012\n",
      "1  0.471780     0.454812   0.073408\n",
      "2  0.582223     0.383893   0.033884\n",
      "3  0.622462     0.328557   0.048981\n",
      "4  0.926402     0.064996   0.008602\n",
      "   ET_low_0  ET_medium_0  ET_high_0\n",
      "0  0.309822     0.527181   0.162997\n",
      "1  0.984336     0.014059   0.001605\n",
      "2  0.956579     0.038080   0.005341\n",
      "3  0.518490     0.384524   0.096986\n",
      "4  0.759357     0.210049   0.030594\n"
     ]
    }
   ],
   "source": [
    "# ET 1st level\n",
    "file_train      = 'train_blend_ET_entropy_last_2017-04-21-11-48' + '.csv'\n",
    "file_test_mean  = 'test_blend_ET_entropy_mean_last_2017-04-21-11-48' + '.csv'\n",
    "\n",
    "train_ET = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_ET  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_ET.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ET.columns = ['ET_' + x for x in names[:n_column]]\n",
    "test_ET.columns  = ['ET_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ET.iloc[:5,:3]\n",
    "print test_ET.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.507812              0.390625            0.101562\n",
      "1           0.531250              0.359375            0.109375\n",
      "2           0.671875              0.273438            0.054688\n",
      "3           0.609375              0.250000            0.140625\n",
      "4           0.843750              0.140625            0.015625\n",
      "   KNN_uniform_low_0  KNN_uniform_medium_0  KNN_uniform_high_0\n",
      "0           0.381250              0.457813            0.160938\n",
      "1           0.968750              0.031250            0.000000\n",
      "2           0.970313              0.029687            0.000000\n",
      "3           0.693750              0.259375            0.046875\n",
      "4           0.612500              0.321875            0.065625\n"
     ]
    }
   ],
   "source": [
    "# KNN 1st level\n",
    "file_train      = 'train_blend_KNN_uniform_last_2017-04-21-13-53' + '.csv'\n",
    "file_test_mean  = 'test_blend_KNN_uniform_mean_last_2017-04-21-13-53' + '.csv'\n",
    "\n",
    "\n",
    "train_KNN = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_KNN  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_KNN.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_KNN.columns      = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "test_KNN.columns  = ['KNN_uniform_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_KNN.iloc[:5,:3]\n",
    "print test_KNN.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.460187          0.436036        0.103776\n",
      "1       0.268598          0.571916        0.159486\n",
      "2       0.724799          0.239351        0.035851\n",
      "3       0.669683          0.286716        0.043600\n",
      "4       0.917878          0.073469        0.008653\n",
      "   FM_0322_low_0  FM_0322_medium_0  FM_0322_high_0\n",
      "0       0.449136          0.411890        0.138974\n",
      "1       0.971615          0.020319        0.008066\n",
      "2       0.909864          0.074040        0.016096\n",
      "3       0.661175          0.269026        0.069799\n",
      "4       0.705851          0.263069        0.031080\n"
     ]
    }
   ],
   "source": [
    "# TFFM 1st level 0322\n",
    "file_train      = 'train_blend_FM_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "file_test_mean  = 'test_blend_FM_mean_BM_0322_2017-03-27-04-35' + '.csv'\n",
    "\n",
    "train_FM_0322      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_FM_mean_0322  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "n_column = train_FM_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_FM_0322.columns      = ['FM_0322_' + x for x in names[:n_column]]\n",
    "test_FM_mean_0322.columns  = ['FM_0322_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_FM_0322.iloc[:5,:3]\n",
    "print test_FM_mean_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.216985      0.579031    0.203985\n",
      "1   0.560375      0.382293    0.057331\n",
      "2   0.621019      0.332482    0.046499\n",
      "3   0.312441      0.345115    0.342444\n",
      "4   0.901678      0.090035    0.008287\n",
      "   MNB_low_0  MNB_medium_0  MNB_high_0\n",
      "0   0.218456      0.591575    0.189969\n",
      "1   0.992103      0.007102    0.000795\n",
      "2   0.971220      0.025168    0.003612\n",
      "3   0.501988      0.411198    0.086813\n",
      "4   0.754496      0.213158    0.032347\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes 1st level\n",
    "file_train      = 'train_blend_MNB_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "file_test_mean  = 'test_blend_MNB_mean_BM_MB_last_2017-04-21-14-02' + '.csv'\n",
    "\n",
    "\n",
    "train_MNB      = pd.read_csv(data_path + file_train,      header = None)\n",
    "test_MNB_mean  = pd.read_csv(data_path + file_test_mean,  header = None)\n",
    "\n",
    "\n",
    "n_column = train_MNB.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_MNB.columns      = ['MNB_' + x for x in names[:n_column]]\n",
    "test_MNB_mean.columns  = ['MNB_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_MNB.iloc[:5,:3]\n",
    "print test_MNB_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tsne_0     tsne_1    tsne_2\n",
      "0  -8.398991  -2.415894 -3.602143\n",
      "1   0.698237   0.335786  8.884257\n",
      "2  -5.811380 -16.669975  7.145837\n",
      "3  -0.371861 -25.894747 -2.076309\n",
      "4 -15.371799   9.656209  5.813590\n",
      "      tsne_0     tsne_1     tsne_2\n",
      "0  -5.176846  -0.768422  -2.339259\n",
      "1   9.003089  13.250301  -0.707032\n",
      "2   4.188036  14.397186   4.573307\n",
      "3  10.890132 -12.660774 -13.414140\n",
      "4   6.011381   5.177731  15.669250\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level\n",
    "\n",
    "file_train = 'X_train_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_MB_add_desc_2017-03-18-17-14' + '.csv'\n",
    "\n",
    "train_tsne = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "\n",
    "n_column = train_tsne.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne.columns = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "test_tsne.columns  = ['tsne_0', 'tsne_1', 'tsne_2']\n",
    "\n",
    "\n",
    "print train_tsne.iloc[:5,:3]\n",
    "print test_tsne.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -6.649132    13.028168     8.329733\n",
      "1     7.615566     0.067456   -14.932181\n",
      "2     8.333528     8.561174   -13.536297\n",
      "3    12.819587   -20.027314     0.661660\n",
      "4    -5.513088    -5.609218    17.130673\n",
      "   tsne_0_0322  tsne_1_0322  tsne_2_0322\n",
      "0    -5.721674     7.011411    -6.499047\n",
      "1     8.238390    -8.589710    13.771045\n",
      "2   -11.383577   -16.071395    15.083511\n",
      "3    -6.111491     6.348311   -10.222012\n",
      "4     4.426022    15.553415    11.315777\n"
     ]
    }
   ],
   "source": [
    "# TSNE 1st level 0322\n",
    "\n",
    "file_train = 'X_train_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "file_test  = 'X_test_tsne_BM_0322_2017-03-26-16-33' + '.csv'\n",
    "\n",
    "train_tsne_0322 = pd.read_csv(data_path + file_train, header = None)\n",
    "test_tsne_0322  = pd.read_csv(data_path + file_test, header = None)\n",
    "\n",
    "n_column = train_tsne_0322.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_tsne_0322.columns = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "test_tsne_0322.columns  = ['tsne_0_0322', 'tsne_1_0322', 'tsne_2_0322']\n",
    "\n",
    "print train_tsne_0322.iloc[:5,:3]\n",
    "print test_tsne_0322.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.338520      0.631375    0.030105\n",
      "1   0.563156      0.390819    0.046025\n",
      "2   0.462242      0.498429    0.039328\n",
      "3   0.930174      0.067369    0.002458\n",
      "4   0.892201      0.106567    0.001232\n",
      "   xgb_low_0  xgb_medium_0  xgb_high_0\n",
      "0   0.178073      0.621011    0.200916\n",
      "1   0.978270      0.012231    0.009499\n",
      "2   0.939539      0.055440    0.005021\n",
      "3   0.151284      0.616193    0.232523\n",
      "4   0.698756      0.292996    0.008247\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_2bagging_CV_MS_52571_2017-04-20-23-06' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb*2\n",
    "tmp_test  = test_xgb_mean*2\n",
    "\n",
    "file_train     = 'train_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "file_test_mean = 'test_blend_XGB_BM_20bagging_last_2017-04-21-19-08' + '.csv'\n",
    "\n",
    "train_xgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb      = (tmp_train + train_xgb*20) / 22.0\n",
    "test_xgb_mean  = (tmp_test + test_xgb_mean*20) / 22.0\n",
    "\n",
    "n_column = train_xgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "test_xgb_mean.columns = ['xgb_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb.iloc[:5,:3]\n",
    "print test_xgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.309433             0.660416           0.030151\n",
      "1          0.565861             0.382300           0.051838\n",
      "2          0.401785             0.566253           0.031961\n",
      "3          0.931191             0.066339           0.002470\n",
      "4          0.881444             0.117002           0.001555\n",
      "   xgb_30fold_low_0  xgb_30fold_medium_0  xgb_30fold_high_0\n",
      "0          0.174706             0.640575           0.184719\n",
      "1          0.977405             0.013037           0.009558\n",
      "2          0.944000             0.051268           0.004732\n",
      "3          0.145425             0.592400           0.262175\n",
      "4          0.670348             0.322061           0.007591\n"
     ]
    }
   ],
   "source": [
    "# XGB 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "file_test_mean  = 'test_blend_XGB_last_30fold_2017-04-21-12-57' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_30fold      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_30fold.columns      = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_30fold.columns  = ['xgb_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_30fold.iloc[:5,:3]\n",
    "print test_xgb_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.313141          0.658802        0.028057\n",
      "1       0.491897          0.450891        0.057212\n",
      "2       0.486618          0.486574        0.026809\n",
      "3       0.925839          0.071295        0.002866\n",
      "4       0.863436          0.134349        0.002214\n",
      "   xgb_ovr_low_0  xgb_ovr_medium_0  xgb_ovr_high_0\n",
      "0       0.167358          0.646104        0.186538\n",
      "1       0.981203          0.010042        0.008756\n",
      "2       0.906588          0.088855        0.004558\n",
      "3       0.167107          0.602472        0.230421\n",
      "4       0.716008          0.274903        0.009089\n"
     ]
    }
   ],
   "source": [
    "# XGB one vs rest 1st level\n",
    "\n",
    "file_train      = 'train_blend_xgb_ovr_last_2017-04-21-10-09' + '.csv'\n",
    "file_test_mean  = 'test_blend_xgb_ovr_mean_last_2017-04-21-10-09' + '.csv'\n",
    "\n",
    "train_xgb_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "\n",
    "n_column = train_xgb_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_ovr.columns      = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_ovr.columns  = ['xgb_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_xgb_ovr,axis=1)\n",
    "sum_test  = np.sum(test_xgb_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_xgb_ovr.columns.values:\n",
    "    train_xgb_ovr[col] = train_xgb_ovr[col] / sum_train\n",
    "    test_xgb_mean_ovr[col] = test_xgb_mean_ovr[col] / sum_test\n",
    "\n",
    "\n",
    "print train_xgb_ovr.iloc[:5,:3]\n",
    "print test_xgb_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.317617            0.650112          0.032271\n",
      "1         0.597390            0.386899          0.015711\n",
      "2         0.450143            0.517132          0.032726\n",
      "3         0.898037            0.100391          0.001572\n",
      "4         0.880435            0.118283          0.001282\n",
      "   lgb_10bag_low_0  lgb_10bag_medium_0  lgb_10bag_high_0\n",
      "0         0.186506            0.567313          0.246180\n",
      "1         0.964578            0.025110          0.010312\n",
      "2         0.913688            0.080543          0.005769\n",
      "3         0.114846            0.657136          0.228019\n",
      "4         0.650263            0.344835          0.004902\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 1st level\n",
    "\n",
    "file_train      = 'train_blend_LightGBM_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_LightGBM_mean_last_10bagging_2017-04-21-21-54' + '.csv'\n",
    "\n",
    "\n",
    "train_lgb      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_lgb_mean  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_lgb.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_lgb.columns      = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "test_lgb_mean.columns  = ['lgb_10bag_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_lgb.iloc[:5,:3]\n",
    "print test_lgb_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.335217     0.604643   0.060140\n",
      "1  0.772956     0.207974   0.019070\n",
      "2  0.529658     0.446699   0.023643\n",
      "3  0.956680     0.042453   0.000867\n",
      "4  0.938238     0.060709   0.001053\n",
      "   nn_low_0  nn_medium_0  nn_high_0\n",
      "0  0.276664     0.582024   0.141312\n",
      "1  0.995422     0.004223   0.000356\n",
      "2  0.979827     0.019278   0.000895\n",
      "3  0.362736     0.446379   0.190885\n",
      "4  0.729782     0.259918   0.010300\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level No.1\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-21-23' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-21-23' + '.csv'\n",
    "\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "test_nn_mean = tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2})\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_2017-04-20-22-05' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_2017-04-20-22-05' + '.csv'\n",
    "\n",
    "tmp_train = pd.read_csv(data_path + file_train, header = None)\n",
    "tmp_test  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_nn     = (train_nn + tmp_train[[0, 1, 2]]+(tmp_train[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "test_nn_mean = (test_nn_mean + tmp_test[[0, 1, 2]]+(tmp_test[[3, 4, 5]]).rename(columns = {3:0,4:1,5:2}))/4\n",
    "\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn.columns      = ['nn_' + x for x in names[:n_column]]\n",
    "test_nn_mean.columns  = ['nn_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn.iloc[:5,:3]\n",
    "print test_nn_mean.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.378595            0.564656          0.056749\n",
      "1         0.763784            0.215613          0.020603\n",
      "2         0.577165            0.398481          0.024354\n",
      "3         0.949192            0.049937          0.000871\n",
      "4         0.958236            0.040963          0.000801\n",
      "   nn_30fold_low_0  nn_30fold_medium_0  nn_30fold_high_0\n",
      "0         0.286080            0.567262          0.146658\n",
      "1         0.996091            0.003613          0.000296\n",
      "2         0.986770            0.012566          0.000664\n",
      "3         0.370175            0.443003          0.186822\n",
      "4         0.738394            0.250685          0.010921\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 30fold\n",
    "\n",
    "file_train      = 'train_blend_Keras_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_30fold_2017-04-21-12-03' + '.csv'\n",
    "\n",
    "train_nn_30fold     = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_30fold  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_30fold.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_30fold.columns      = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "test_nn_mean_30fold.columns  = ['nn_30fold_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_30fold.iloc[:5,:3]\n",
    "print test_nn_mean_30fold.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.414059         0.533141       0.052800\n",
      "1      0.752638         0.210038       0.037324\n",
      "2      0.515976         0.440243       0.043781\n",
      "3      0.780622         0.219167       0.000211\n",
      "4      0.965501         0.032244       0.002255\n",
      "   nn_ovr_low_0  nn_ovr_medium_0  nn_ovr_high_0\n",
      "0      0.227279         0.679449       0.093272\n",
      "1      0.976299         0.014297       0.009403\n",
      "2      0.919964         0.066669       0.013367\n",
      "3      0.266264         0.389498       0.344238\n",
      "4      0.874583         0.119637       0.005780\n"
     ]
    }
   ],
   "source": [
    "# Keras one vs rest 1st level\n",
    "file_train      = 'train_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_ovr_last_2017-04-21-10-15' + '.csv'\n",
    "\n",
    "train_nn_ovr      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_ovr  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_ovr.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_ovr.columns      = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "test_nn_mean_ovr.columns  = ['nn_ovr_' + x for x in names[:n_column]]\n",
    "\n",
    "sum_train = np.sum(train_nn_ovr,axis=1)\n",
    "sum_test  = np.sum(test_nn_mean_ovr,axis=1)\n",
    "\n",
    "for col in train_nn_ovr.columns.values:\n",
    "    train_nn_ovr[col] = train_nn_ovr[col] / sum_train\n",
    "    test_nn_mean_ovr[col] = test_nn_mean_ovr[col] / sum_test \n",
    "\n",
    "print train_nn_ovr.iloc[:5,:3]\n",
    "print test_nn_mean_ovr.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.445882            0.488624          0.065494\n",
      "1         0.761473            0.222332          0.016195\n",
      "2         0.542404            0.415798          0.041798\n",
      "3         0.956819            0.041689          0.001492\n",
      "4         0.950208            0.048009          0.001783\n",
      "   nn_3layer_low_0  nn_3layer_medium_0  nn_3layer_high_0\n",
      "0         0.300305            0.540791          0.158904\n",
      "1         0.995423            0.004364          0.000213\n",
      "2         0.984619            0.014709          0.000672\n",
      "3         0.376123            0.446293          0.177584\n",
      "4         0.713087            0.267086          0.019827\n"
     ]
    }
   ],
   "source": [
    "# Keras 1st level 3layer 20 bagging\n",
    "file_train      = 'train_blend_Keras_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "file_test_mean  = 'test_blend_Keras_mean_last_3layer_20bagging_2017-04-21-20-01' + '.csv'\n",
    "\n",
    "train_nn_3layer      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_3layer  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn_3layer.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_3layer.columns      = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "test_nn_mean_3layer.columns  = ['nn_3layer_' + x for x in names[:n_column]]\n",
    "\n",
    "\n",
    "print train_nn_3layer.iloc[:5,:3]\n",
    "print test_nn_mean_3layer.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../3rd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nn_2nd_low_0  nn_2nd_medium_0  nn_2nd_high_0\n",
      "0      0.357091         0.585324       0.057586\n",
      "1      0.554172         0.404744       0.041084\n",
      "2      0.410131         0.546008       0.043861\n",
      "3      0.890533         0.106213       0.003254\n",
      "4      0.951064         0.047675       0.001261\n",
      "   nn_2nd_low_0  nn_2nd_medium_0  nn_2nd_high_0\n",
      "0      0.162490         0.584023       0.253486\n",
      "1      0.995381         0.004230       0.000389\n",
      "2      0.972846         0.025891       0.001264\n",
      "3      0.108858         0.501719       0.389423\n",
      "4      0.733884         0.257278       0.008839\n"
     ]
    }
   ],
   "source": [
    "# Keras 2nd level \n",
    "\n",
    "file_train      = 'train_blend_2ndKeras_100bagging_2017-04-22-18-54' + '.csv'\n",
    "file_test_mean  = 'test_blend_2ndKeras_100bagging_2017-04-22-18-54' + '.csv'\n",
    "\n",
    "\n",
    "train_nn_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_nn_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "n_column = train_nn.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_nn_2nd.columns      = ['nn_2nd_' + x for x in names[:n_column]]\n",
    "test_nn_mean_2nd.columns  = ['nn_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_nn_2nd.iloc[:5,:3]\n",
    "print test_nn_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   xgb_2nd_low_0  xgb_2nd_medium_0  xgb_2nd_high_0\n",
      "0       0.315073          0.651911        0.033015\n",
      "1       0.583858          0.368919        0.047223\n",
      "2       0.446948          0.505974        0.047079\n",
      "3       0.907728          0.087888        0.004384\n",
      "4       0.912005          0.085264        0.002731\n",
      "   xgb_2nd_low_0  xgb_2nd_medium_0  xgb_2nd_high_0\n",
      "0       0.171173          0.618564        0.210263\n",
      "1       0.994381          0.004820        0.000799\n",
      "2       0.944171          0.053315        0.002514\n",
      "3       0.133564          0.550379        0.316057\n",
      "4       0.750419          0.240804        0.008776\n"
     ]
    }
   ],
   "source": [
    "# XGB 2nd level\n",
    "\n",
    "file_train     = 'train_blend_2ndXGB_BM_100bagging_2017-04-22-07-18' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndXGB_BM_100bagging_2017-04-22-07-18' + '.csv'\n",
    "\n",
    "\n",
    "train_xgb_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_xgb_2nd\n",
    "tmp_test  = test_xgb_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndXGB_BM_100bagging_1_2017-04-22-14-37' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndXGB_BM_100bagging_1_2017-04-22-14-37' + '.csv'\n",
    "\n",
    "train_xgb_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_xgb_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_xgb_2nd      = (tmp_train + train_xgb_2nd) / 2.0\n",
    "test_xgb_mean_2nd  = (tmp_test + test_xgb_mean_2nd) / 2.0\n",
    "\n",
    "n_column = train_xgb_2nd.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_xgb_2nd.columns = ['xgb_2nd_' + x for x in names[:n_column]]\n",
    "test_xgb_mean_2nd.columns = ['xgb_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_xgb_2nd.iloc[:5,:3]\n",
    "print test_xgb_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ADET_2nd_low_0  ADET_2nd_medium_0  ADET_2nd_high_0\n",
      "0        0.350422           0.600100         0.049478\n",
      "1        0.457400           0.494753         0.047847\n",
      "2        0.428550           0.521861         0.049589\n",
      "3        0.840500           0.157169         0.002331\n",
      "4        0.932883           0.057958         0.009158\n",
      "   ADET_2nd_low_0  ADET_2nd_medium_0  ADET_2nd_high_0\n",
      "0        0.132605           0.644076         0.223319\n",
      "1        0.982162           0.017716         0.000123\n",
      "2        0.910824           0.088860         0.000316\n",
      "3        0.163267           0.489916         0.346817\n",
      "4        0.723374           0.258492         0.018134\n"
     ]
    }
   ],
   "source": [
    "# ADET 2nd level\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_2017-04-22-04-30' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_2017-04-22-04-30' + '.csv'\n",
    "\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = train_ADET_2nd\n",
    "tmp_test  = test_ADET_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_1_2017-04-22-10-08' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_1_2017-04-22-10-08' + '.csv'\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "tmp_train = tmp_train + train_ADET_2nd\n",
    "tmp_test  = tmp_test + test_ADET_mean_2nd\n",
    "\n",
    "file_train     = 'train_blend_2ndADET_100bagging_last_2_2017-04-22-17-43' + '.csv'\n",
    "file_test_mean = 'test_blend_2ndADET_100bagging_last_2_2017-04-22-17-43' + '.csv'\n",
    "\n",
    "train_ADET_2nd      = pd.read_csv(data_path + file_train, header = None)\n",
    "test_ADET_mean_2nd  = pd.read_csv(data_path + file_test_mean, header = None)\n",
    "\n",
    "train_ADET_2nd      = (tmp_train + train_ADET_2nd) / 3.0\n",
    "test_ADET_mean_2nd  = (tmp_test + test_ADET_mean_2nd) / 3.0\n",
    "\n",
    "n_column = train_ADET_2nd.shape[1]\n",
    "total_col += n_column\n",
    "\n",
    "train_ADET_2nd.columns = ['ADET_2nd_' + x for x in names[:n_column]]\n",
    "test_ADET_mean_2nd.columns = ['ADET_2nd_' + x for x in names[:n_column]]\n",
    "\n",
    "print train_ADET_2nd.iloc[:5,:3]\n",
    "print test_ADET_mean_2nd.iloc[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "print total_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2nd: (49352, 57)\t test_2nd_mean:(74659, 57)\n"
     ]
    }
   ],
   "source": [
    "train_2nd      = pd.concat([train_rfc, train_LR, train_ET, train_KNN, train_FM_0322,    train_MNB,     train_tsne,\n",
    "                            train_tsne_0322, train_xgb,     train_xgb_30fold,     train_xgb_ovr, \n",
    "                            train_nn,     train_nn_30fold,     train_nn_ovr,     train_nn_3layer,\n",
    "                            train_lgb,\n",
    "                            train_nn_2nd, train_xgb_2nd, train_ADET_2nd\n",
    "                           ], axis = 1)\n",
    "\n",
    "test_2nd_mean  = pd.concat([test_rfc,  test_LR,  test_ET,  test_KNN, test_FM_mean_0322, test_MNB_mean, test_tsne, \n",
    "                            test_tsne_0322,  test_xgb_mean, test_xgb_mean_30fold, test_xgb_mean_ovr,\n",
    "                            test_nn_mean, test_nn_mean_30fold, test_nn_mean_ovr, test_nn_mean_3layer,\n",
    "                            test_lgb_mean,\n",
    "                            test_nn_mean_2nd, test_xgb_mean_2nd, test_ADET_mean_2nd\n",
    "                           ], axis = 1)\n",
    "\n",
    "print 'train_2nd: {}\\t test_2nd_mean:{}'.\\\n",
    "            format(train_2nd.shape,test_2nd_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 222) (74659, 222) (49352, 3)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "\n",
    "ntrain = train_X_0322.shape[0]\n",
    "sub_id = test_X_0322.listing_id.astype('int32').values\n",
    "\n",
    "train_X = pd.read_csv(data_path + 'train_CV_MS_52571.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_CV_MS_52571.csv')\n",
    "\n",
    "train_X = train_X_0322[['listing_id']].merge(train_X,on='listing_id',how='left')\n",
    "test_X = test_X_0322[['listing_id']].merge(test_X,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 223)\n",
      "(74659, 223)\n"
     ]
    }
   ],
   "source": [
    "time_feature = pd.read_csv(data_path + 'listing_image_time.csv')\n",
    "time_feature.columns = ['listing_id','time_stamp']\n",
    "train_X = train_X.merge(time_feature,on='listing_id',how='left')\n",
    "test_X = test_X.merge(time_feature,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 280)\n",
      "(74659, 280)\n"
     ]
    }
   ],
   "source": [
    "train_X = pd.concat([train_X,train_2nd],axis=1)\n",
    "test_X = pd.concat([test_X,test_2nd_mean],axis=1)\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124011, 280)\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.concat([train_X,test_X])\n",
    "print full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 280)\n",
      "(74659, 280)\n"
     ]
    }
   ],
   "source": [
    "full_data = full_data.fillna(0)\n",
    "\n",
    "for col in full_data.columns.values:\n",
    "    full_data.loc[:,col] = (full_data[col]-full_data[col].mean())/full_data[col].std()\n",
    "train_df_nn = full_data[:ntrain]\n",
    "test_df_nn = full_data[ntrain:]\n",
    "\n",
    "train_df_nn = sparse.csr_matrix(train_df_nn)\n",
    "test_df_nn = sparse.csr_matrix(test_df_nn)\n",
    "\n",
    "\n",
    "print train_df_nn.shape\n",
    "print test_df_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df_nn, train_y, train_size=.80, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.7607 - val_loss: 0.5257\n",
      "Epoch 2/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5763 - val_loss: 0.5085\n",
      "Epoch 3/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5427 - val_loss: 0.5057\n",
      "Epoch 4/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5320 - val_loss: 0.5026\n",
      "Epoch 5/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5276 - val_loss: 0.5021\n",
      "Epoch 6/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5266 - val_loss: 0.5021\n",
      "Epoch 7/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5211 - val_loss: 0.5015\n",
      "Epoch 8/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5197 - val_loss: 0.4994\n",
      "Epoch 9/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5226 - val_loss: 0.4991\n",
      "Epoch 10/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5138 - val_loss: 0.4990\n",
      "Epoch 11/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5168 - val_loss: 0.4973\n",
      "Epoch 12/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5145 - val_loss: 0.4973\n",
      "Epoch 13/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5120 - val_loss: 0.4971\n",
      "Epoch 14/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5157 - val_loss: 0.4975\n",
      "Epoch 15/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5098 - val_loss: 0.4965\n",
      "Epoch 16/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5153 - val_loss: 0.4961\n",
      "Epoch 17/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5083 - val_loss: 0.4968\n",
      "Epoch 18/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5098 - val_loss: 0.4959\n",
      "Epoch 19/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5075 - val_loss: 0.4958\n",
      "Epoch 20/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5122 - val_loss: 0.4954\n",
      "Epoch 21/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5088 - val_loss: 0.4947\n",
      "Epoch 22/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5075 - val_loss: 0.4952\n",
      "Epoch 23/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5067 - val_loss: 0.4959\n",
      "Epoch 24/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5064 - val_loss: 0.4946\n",
      "Epoch 25/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5056 - val_loss: 0.4963\n",
      "Epoch 26/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5055 - val_loss: 0.4958\n",
      "Epoch 27/1000\n",
      "49408/49352 [==============================] - 4s - loss: 0.5041 - val_loss: 0.4959\n",
      "Epoch 28/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5039 - val_loss: 0.4959\n",
      "Epoch 29/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5062 - val_loss: 0.4961\n",
      "Epoch 30/1000\n",
      "49408/49352 [==============================] - 5s - loss: 0.5024 - val_loss: 0.4973\n",
      "0.494599252314\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=5, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    \n",
    "    model.add(Dense(70, # number of input units: needs to be tuned\n",
    "                    input_dim = input_dim, # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(20,init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "#     model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(BatchNormalization())    \n",
    "#     model.add(Dropout(0.4)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                   init = init,\n",
    "                   activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "#                   metrics=[mae_log],\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(X_train.shape[1])\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=1000,\n",
    "                         samples_per_epoch=ntrain,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop,checkpointer]\n",
    "                         )\n",
    "\n",
    "print min(fit.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def nn_model(params):\n",
    "    model = Sequential()\n",
    "    init = 'glorot_uniform'\n",
    "    \n",
    "    model.add(Dense(params['input_size'], # number of input units: needs to be tuned\n",
    "                    input_dim = params['input_dim'], # fixed length: number of columns of X\n",
    "                    init=init,\n",
    "                   ))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU()) # activation function\n",
    "    model.add(BatchNormalization()) # normalization\n",
    "    model.add(Dropout(params['input_drop_out'])) #dropout rate. needs to be tuned\n",
    "        \n",
    "    model.add(Dense(params['hidden_size'],\n",
    "                    init=init)) # number of hidden1 units. needs to be tuned.\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(params['hidden_drop_out'])) #dropout rate. needs to be tuned\n",
    "    \n",
    "#     model.add(Dense(20,init=init)) # number of hidden2 units. needs to be tuned.\n",
    "#     model.add(Activation('sigmoid'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(BatchNormalization())    \n",
    "#     model.add(Dropout(0.5)) #dropout rate. needs to be tuned\n",
    "    \n",
    "    model.add(Dense(3,\n",
    "                    init = init,\n",
    "                    activation = 'softmax')) # 1 for regression \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'Adamax' # optimizer. you may want to try different ones\n",
    "                 )\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "def nn_blend_data(parameters, train_x, train_y, test_x, fold, early_stopping_rounds=10, batch_size=128,randomseed = 1234):\n",
    "    \n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', # custom metric\n",
    "                           patience=early_stopping_rounds, #early stopping for epoch\n",
    "                           verbose=0)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", \n",
    "                               monitor='val_loss', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "\n",
    "    N_params = len(parameters)\n",
    "#     print (\"Blend %d estimators for %d folds\" % (len(parameters), fold))\n",
    "    skf = KFold(n_splits=fold, shuffle=True, random_state=randomseed)\n",
    "    N_class = train_y.shape[1]\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    fold_start = time.time() \n",
    "\n",
    "    \n",
    "    for j, nn_params in enumerate(parameters):\n",
    "#         print (\"Model %d: %s\" %(j+1, nn_params))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "#             print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "\n",
    "            model = nn_model(nn_params)\n",
    "#             print (model)\n",
    "            fit= model.fit_generator(generator=batch_generator(train_x_fold, train_y_fold, batch_size, True),\n",
    "                                     nb_epoch=70,\n",
    "                                     samples_per_epoch=train_x_fold.shape[0],\n",
    "                                     validation_data=(val_x_fold.todense(), val_y_fold),\n",
    "                                     verbose = 0,\n",
    "                                     callbacks=[early_stop, checkpointer]\n",
    "                                    )\n",
    "\n",
    "            best_round=len(fit.epoch)-early_stopping_rounds-1\n",
    "            best_rounds[i,j]=best_round\n",
    "#             print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )\n",
    "            \n",
    "            # print (mean_absolute_error(np.exp(y_val)-200, pred_y))\n",
    "            val_y_predict_fold = model.predict_proba(x=val_x_fold.toarray(),verbose=0)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "#             print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            model.load_weights(\"weights.hdf5\")\n",
    "            # Compile model (required to make predictions)\n",
    "            model.compile(loss = 'categorical_crossentropy',optimizer = 'Adamax' )            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = model.predict_proba(x=test_x.toarray(),verbose=0)\n",
    "#             print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "#         print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print \"Score for blended models is %f in %0.3fm\" % (np.mean(scores), (time.time() - fold_start)/60)\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting.........\n",
      "0\n",
      "Score for blended models is 0.486551 in 11.479m\n",
      "1\n",
      "Score for blended models is 0.494617 in 11.118m\n",
      "2\n",
      "Score for blended models is 0.489532 in 10.814m\n",
      "3\n",
      "Score for blended models is 0.487771 in 10.570m\n",
      "4\n",
      "Score for blended models is 0.492188 in 11.279m\n",
      "5\n",
      "Score for blended models is 0.489884 in 11.527m\n",
      "6\n",
      "Score for blended models is 0.479322 in 10.654m\n",
      "7\n",
      "Score for blended models is 0.488136 in 9.969m\n",
      "8\n",
      "Score for blended models is 0.488957 in 11.112m\n",
      "9\n",
      "Score for blended models is 0.485534 in 11.218m\n",
      "10\n",
      "Score for blended models is 0.483647 in 11.682m\n",
      "11\n",
      "Score for blended models is 0.483279 in 11.850m\n",
      "12\n",
      "Score for blended models is 0.494668 in 11.207m\n",
      "13\n",
      "Score for blended models is 0.493479 in 10.937m\n",
      "14\n",
      "Score for blended models is 0.486787 in 12.132m\n",
      "15\n",
      "Score for blended models is 0.486160 in 11.848m\n",
      "16\n",
      "Score for blended models is 0.496244 in 9.799m\n",
      "17\n",
      "Score for blended models is 0.488491 in 10.649m\n",
      "18\n",
      "Score for blended models is 0.491036 in 11.342m\n",
      "19\n",
      "Score for blended models is 0.485993 in 11.033m\n",
      "20\n",
      "Score for blended models is 0.487604 in 10.681m\n",
      "21\n",
      "Score for blended models is 0.487175 in 11.317m\n",
      "22\n",
      "Score for blended models is 0.490608 in 10.614m\n",
      "23\n",
      "Score for blended models is 0.495696 in 10.298m\n",
      "24\n",
      "Score for blended models is 0.499472 in 10.529m\n",
      "25\n",
      "Score for blended models is 0.497044 in 10.757m\n",
      "26\n",
      "Score for blended models is 0.490116 in 10.265m\n",
      "27\n",
      "Score for blended models is 0.484876 in 10.666m\n",
      "28\n",
      "Score for blended models is 0.484574 in 9.349m\n",
      "29\n",
      "Score for blended models is 0.489988 in 10.177m\n",
      "30\n",
      "Score for blended models is 0.486371 in 11.132m\n",
      "31\n",
      "Score for blended models is 0.488370 in 10.623m\n",
      "32\n",
      "Score for blended models is 0.485309 in 10.820m\n",
      "33\n",
      "Score for blended models is 0.492231 in 9.689m\n",
      "34\n",
      "Score for blended models is 0.487455 in 10.256m\n",
      "35\n",
      "Score for blended models is 0.489947 in 11.150m\n",
      "36\n",
      "Score for blended models is 0.492506 in 11.192m\n",
      "37\n",
      "Score for blended models is 0.495972 in 10.970m\n",
      "38\n",
      "Score for blended models is 0.487831 in 10.990m\n",
      "39\n",
      "Score for blended models is 0.487963 in 9.615m\n",
      "40\n",
      "Score for blended models is 0.488866 in 9.689m\n",
      "41\n",
      "Score for blended models is 0.485992 in 11.393m\n",
      "42\n",
      "Score for blended models is 0.487259 in 10.948m\n",
      "43\n",
      "Score for blended models is 0.488782 in 10.668m\n",
      "44\n",
      "Score for blended models is 0.494455 in 11.607m\n",
      "45\n",
      "Score for blended models is 0.491675 in 10.220m\n",
      "46\n",
      "Score for blended models is 0.492772 in 11.293m\n",
      "47\n",
      "Score for blended models is 0.493938 in 9.902m\n",
      "48\n",
      "Score for blended models is 0.482555 in 10.175m\n",
      "49\n",
      "Score for blended models is 0.491821 in 11.325m\n",
      "50\n",
      "Score for blended models is 0.496360 in 10.923m\n",
      "51\n",
      "Score for blended models is 0.491145 in 12.113m\n",
      "52\n",
      "Score for blended models is 0.479937 in 13.399m\n",
      "53\n",
      "Score for blended models is 0.497158 in 9.724m\n",
      "54\n",
      "Score for blended models is 0.492697 in 11.046m\n",
      "55\n",
      "Score for blended models is 0.492775 in 10.809m\n",
      "56\n",
      "Score for blended models is 0.488250 in 10.733m\n",
      "57\n",
      "Score for blended models is 0.483633 in 10.687m\n",
      "58\n",
      "Score for blended models is 0.496196 in 11.479m\n",
      "59\n",
      "Score for blended models is 0.490995 in 11.404m\n",
      "60\n",
      "Score for blended models is 0.491827 in 10.538m\n",
      "61\n",
      "Score for blended models is 0.490198 in 9.948m\n",
      "62\n",
      "Score for blended models is 0.481830 in 11.761m\n",
      "63\n",
      "Score for blended models is 0.484135 in 10.272m\n",
      "64\n",
      "Score for blended models is 0.486217 in 10.743m\n",
      "65\n",
      "Score for blended models is 0.486783 in 11.306m\n",
      "66\n",
      "Score for blended models is 0.488859 in 10.347m\n",
      "67\n",
      "Score for blended models is 0.486918 in 11.848m\n",
      "68\n",
      "Score for blended models is 0.489498 in 11.514m\n",
      "69\n",
      "Score for blended models is 0.491130 in 11.203m\n",
      "70\n",
      "Score for blended models is 0.490934 in 11.399m\n",
      "71\n",
      "Score for blended models is 0.488742 in 10.490m\n",
      "72\n",
      "Score for blended models is 0.492663 in 9.687m\n",
      "73\n",
      "Score for blended models is 0.488128 in 10.147m\n",
      "74\n",
      "Score for blended models is 0.489014 in 9.989m\n",
      "75\n",
      "Score for blended models is 0.489041 in 10.023m\n",
      "76\n",
      "Score for blended models is 0.479535 in 11.366m\n",
      "77\n",
      "Score for blended models is 0.491139 in 11.359m\n",
      "78\n",
      "Score for blended models is 0.494106 in 10.208m\n",
      "79\n",
      "Score for blended models is 0.490414 in 10.165m\n",
      "80\n",
      "Score for blended models is 0.489688 in 9.516m\n",
      "81\n",
      "Score for blended models is 0.495521 in 11.263m\n",
      "82\n",
      "Score for blended models is 0.486687 in 11.487m\n",
      "83\n",
      "Score for blended models is 0.491041 in 11.004m\n",
      "84\n",
      "Score for blended models is 0.496683 in 10.709m\n",
      "85\n",
      "Score for blended models is 0.486960 in 11.106m\n",
      "86\n",
      "Score for blended models is 0.488987 in 11.256m\n",
      "87\n",
      "Score for blended models is 0.491299 in 11.311m\n",
      "88\n",
      "Score for blended models is 0.493096 in 10.992m\n",
      "89\n",
      "Score for blended models is 0.488449 in 10.281m\n",
      "90\n",
      "Score for blended models is 0.483313 in 10.464m\n",
      "91\n",
      "Score for blended models is 0.486303 in 10.376m\n",
      "92\n",
      "Score for blended models is 0.491023 in 11.742m\n",
      "93\n",
      "Score for blended models is 0.491770 in 10.349m\n",
      "94\n",
      "Score for blended models is 0.487558 in 10.670m\n",
      "95\n",
      "Score for blended models is 0.488560 in 11.329m\n",
      "96\n",
      "Score for blended models is 0.487157 in 10.062m\n",
      "97\n",
      "Score for blended models is 0.486648 in 11.298m\n",
      "98\n",
      "Score for blended models is 0.495212 in 12.015m\n",
      "99\n",
      "Score for blended models is 0.487209 in 11.205m\n"
     ]
    }
   ],
   "source": [
    "train_total = np.zeros((train_df_nn.shape[0],3))\n",
    "test_total = np.zeros((test_df_nn.shape[0],3))\n",
    "score_total = 0\n",
    "name_train_blend = '../tmp/train_3rd.csv'\n",
    "name_test_blend = '../tmp/test_3rd.csv'\n",
    "count = 100\n",
    "print 'Starting.........'\n",
    "for n in range(count):\n",
    "    print n\n",
    "    nn_parameters = [\n",
    "        { 'input_size' :70 ,\n",
    "         'input_dim' : train_X.shape[1],\n",
    "         'input_drop_out' : 0.4 ,\n",
    "         'hidden_size' : 20 ,\n",
    "         'hidden_drop_out' :0.4},\n",
    "\n",
    "    ]\n",
    "\n",
    "    (train_blend_x, test_blend_x, blend_scores,best_round) = nn_blend_data(nn_parameters, train_df_nn, train_y, test_df_nn,\n",
    "                                                             5,\n",
    "                                                             10,128,n+500)\n",
    "    train_total += train_blend_x\n",
    "    test_total += test_blend_x\n",
    "    score_total += np.mean(blend_scores)\n",
    "\n",
    "\n",
    "    np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "    np.savetxt(name_test_blend,test_total, delimiter=\",\")\n",
    "    \n",
    "train_total = train_total / count\n",
    "test_total = test_total / count\n",
    "score_total = score_total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.61497001e-01,   5.79230454e-01,   2.59272542e-01],\n",
       "       [  9.95039592e-01,   4.56041498e-03,   3.99995375e-04],\n",
       "       [  9.69870075e-01,   2.86860567e-02,   1.44386714e-03],\n",
       "       ..., \n",
       "       [  9.82038900e-01,   1.65036585e-02,   1.45744027e-03],\n",
       "       [  9.58500472e-01,   3.94348309e-02,   2.06469954e-03],\n",
       "       [  5.46840004e-01,   4.07801798e-01,   4.53581978e-02]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_3rdKeras_last_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_total)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48720921]\n",
      "[ 33.4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_3rdKeras_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_3rdKeras_100bagging_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores,axis=0))\n",
    "print (np.mean(best_round,axis=0))\n",
    "np.savetxt(name_train_blend,train_total, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_total, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
