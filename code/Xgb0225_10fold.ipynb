{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 16)\n",
      "(74659, 15)\n",
      "49352\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file).reset_index()\n",
    "test_df = pd.read_json(test_file).reset_index()\n",
    "ntrain = train_df.shape[0]\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc_price\n",
    "tmp = pd.concat([train_df['price'],test_df['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "\n",
    "train_df.loc[:,'sc_price'] = train_df['price'].values.reshape(-1, 1)\n",
    "test_df.loc[:,'sc_price'] = test_df['price'].values.reshape(-1, 1)\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "\n",
    "# sc_ba_price\n",
    "inx_train = train_df['bathrooms'] == 0\n",
    "inx_test = test_df['bathrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "train_df.loc[non0_inx_train,'sc_ba_price'] = train_df.loc[non0_inx_train,'sc_price']\\\n",
    "                                                /train_df.loc[non0_inx_train,'bathrooms']\n",
    "test_df.loc[non0_inx_test,'sc_ba_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bathrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_ba_price'] = 0\n",
    "test_df.loc[inx_test,'sc_ba_price'] = 0\n",
    "\n",
    "# price per bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "train_df.loc[non0_inx_train,'sc_be_price'] = train_df.loc[non0_inx_train,'sc_price'] \\\n",
    "                                                /train_df.loc[non0_inx_train,'bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_be_price'] = test_df.loc[non0_inx_test,'sc_price']\\\n",
    "                                                /test_df.loc[non0_inx_test,'bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_be_price'] = 0\n",
    "test_df.loc[inx_test,'sc_be_price'] = 0\n",
    "\n",
    "\n",
    "# bathrooms\n",
    "\n",
    "ulimit = 5\n",
    "\n",
    "train_df['sc_bathrooms']=train_df['bathrooms']\n",
    "test_df['sc_bathrooms']=test_df['bathrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "\n",
    "# bedrooms\n",
    "\n",
    "ulimit = 8\n",
    "\n",
    "train_df['sc_bedrooms']=train_df['bedrooms']\n",
    "test_df['sc_bedrooms']=test_df['bedrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['sc_bedrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit,['sc_bedrooms']] = ulimit\n",
    "\n",
    "# longitude\n",
    "\n",
    "tmp = pd.concat([train_df['longitude'],test_df['longitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_longitude']=train_df['longitude']\n",
    "test_df['sc_longitude']=test_df['longitude']\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "\n",
    "# latitude\n",
    "\n",
    "tmp = pd.concat([train_df['latitude'],test_df['latitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_latitude']=train_df['latitude']\n",
    "test_df['sc_latitude']=test_df['latitude']\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "\n",
    "\n",
    "features_to_use  = [\"sc_bathrooms\", \"sc_bedrooms\", \"sc_latitude\", \"sc_longitude\",\n",
    "                    \"sc_price\", \"sc_ba_price\", \"sc_be_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \n",
    "                        \"created_day\", \"created_hour\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full_data=pd.concat([train_df,test_df])\n",
    "\n",
    "# SSL = preprocessing.StandardScaler()\n",
    "# for col in features_to_use:\n",
    "#     full_data[col], lam = boxcox(full_data[col] - full_data[col].min() + 1)\n",
    "#     full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1)) \n",
    "#     train_df[col] = full_data.iloc[:ntrain][col]\n",
    "#     test_df[col] = full_data.iloc[ntrain][col]\n",
    "\n",
    "# del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.append(\"listing_id\")\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    doorman elevator fitness_center cats_allowed d...\n",
      "2    laundry_in_building dishwasher hardwood_floors...\n",
      "3                               hardwood_floors no_fee\n",
      "4                                              pre-war\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "test_df['features'] = test_df[\"features\"]\\\n",
    "                        .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\\\n",
    "                        .apply(lambda x: x.lower())\n",
    "\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "sparse_features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 218) (74659, 218)\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "weight_num_map = {'high':1, 'medium':1, 'low':1}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "W_train = np.array(train_df['interest_level'].apply(lambda x: weight_num_map[x]))\n",
    "\n",
    "all_features = features_to_use + sparse_features\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_dataset = xgb.DMatrix(train_X, label = train_y,\n",
    "#                          weight = W_train,\n",
    "#                          feature_names = all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44416, 218)\n",
      "(4936, 218)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.90, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rgr = xgb.XGBClassifier(objective = 'multi:softprob',\n",
    "#                        learning_rate = 0.1,\n",
    "#                        n_estimators = 10000,\n",
    "#                        nthread = -1)\n",
    "\n",
    "# rgr.fit(X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "# #         num_class = 3,\n",
    "#         early_stopping_rounds=20,\n",
    "#         verbose=False\n",
    "#        )\n",
    "\n",
    "# tmp1 = rgr.predict_proba(test_X)\n",
    "# tmp1=pd.DataFrame(tmp1)\n",
    "# tmp1.columns = ['high','medium','low']\n",
    "# plt.scatter(range(74659),tmp1.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.556399\n",
      "4 \t0.556165\n",
      "5 \t0.553328\n",
      "6 \t0.552869\n",
      "7 \t0.557626\n",
      "8 \t0.556641\n",
      "9 \t0.559613\n",
      "10 \t0.562173\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7,8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = train_param\n",
    "print train_param\n",
    "# 3 \t0.550966\n",
    "# 4 \t0.546335\n",
    "# 5 \t0.546948\n",
    "# 6 \t0.548233\n",
    "# 7 \t0.548868\n",
    "# 8 \t0.551074\n",
    "# 9 \t0.553485\n",
    "# 10 \t0.557234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t0.552672\n",
      "10 \t0.552702\n",
      "20 \t0.554569\n",
      "50 \t0.555714\n",
      "80 \t0.558212\n",
      "120 \t0.556721\n",
      "180 \t0.556979\n",
      "240 \t0.563283\n",
      "300 \t0.565675\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [5,10,20,50,80,120,180,240,300]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.544995\n",
      "0.4 \t0.548923\n",
      "0.5 \t0.547074\n",
      "0.6 \t0.546579\n",
      "0.7 \t0.551113\n",
      "0.8 \t0.550493\n",
      "0.9 \t0.551921\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = 0.6\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.551334\n",
      "0.6 \t0.55245\n",
      "0.7 \t0.54911\n",
      "0.8 \t0.550137\n",
      "0.9 \t0.548913\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t0.548913\n",
      "0.3 \t0.549075\n",
      "0.6 \t0.547017\n",
      "0.9 \t0.546771\n",
      "1.2 \t0.549405\n",
      "1.5 \t0.546167\n",
      "1.8 \t0.548024\n",
      "2.1 \t0.548317\n",
      "2.4 \t0.546834\n",
      "2.7 \t0.549393\n",
      "3.0 \t0.545459\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [0, 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print train_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1093]\ttrain-mlogloss:0.401341+0.00132894\ttest-mlogloss:0.542346+0.00573164\n",
      "\n",
      "    1 | 08m25s | \u001b[35m  -0.54235\u001b[0m | \u001b[32m            0.8668\u001b[0m | \u001b[32m   1.9502\u001b[0m | \u001b[32m     4.7133\u001b[0m | \u001b[32m            2.6959\u001b[0m | \u001b[32m     0.8435\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[566]\ttrain-mlogloss:0.420788+0.00165193\ttest-mlogloss:0.547142+0.00599595\n",
      "\n",
      "    2 | 05m31s |   -0.54714 |             0.8646 |    0.3752 |      5.0573 |            39.3628 |      0.7776 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[548]\ttrain-mlogloss:0.422935+0.00117962\ttest-mlogloss:0.548102+0.00674611\n",
      "\n",
      "    3 | 05m43s |   -0.54810 |             0.9447 |    0.7051 |      5.2825 |            39.3566 |      0.7530 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1571]\ttrain-mlogloss:0.445406+0.00118658\ttest-mlogloss:0.547158+0.00519372\n",
      "\n",
      "    4 | 09m18s |   -0.54716 |             0.8470 |    1.3909 |      3.0581 |            32.1040 |      0.8681 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1567]\ttrain-mlogloss:0.443695+0.00105395\ttest-mlogloss:0.548051+0.00615713\n",
      "\n",
      "    5 | 09m02s |   -0.54805 |             0.8186 |    0.5498 |      3.8086 |            36.3637 |      0.9341 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2128]\ttrain-mlogloss:0.432914+0.00127989\ttest-mlogloss:0.545702+0.00554118\n",
      "\n",
      "    6 | 13m25s |   -0.54570 |             0.9751 |    1.8401 |      3.6989 |            19.7273 |      0.9287 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1363]\ttrain-mlogloss:0.444654+0.00120094\ttest-mlogloss:0.547281+0.00587757\n",
      "\n",
      "    7 | 09m00s |   -0.54728 |             0.9991 |    1.0376 |      3.6066 |            25.2410 |      0.7373 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[573]\ttrain-mlogloss:0.399571+0.00146509\ttest-mlogloss:0.54511+0.00536791\n",
      "\n",
      "    8 | 05m52s |   -0.54511 |             0.9329 |    1.0003 |      5.7522 |            16.6616 |      0.9303 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[589]\ttrain-mlogloss:0.405063+0.00194765\ttest-mlogloss:0.545551+0.0058321\n",
      "\n",
      "    9 | 05m18s |   -0.54555 |             0.7554 |    0.6974 |      5.8611 |            23.5101 |      0.7794 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[698]\ttrain-mlogloss:0.410685+0.000737741\ttest-mlogloss:0.547187+0.00594919\n",
      "\n",
      "   10 | 06m23s |   -0.54719 |             0.7976 |    0.6546 |      5.6029 |            48.3485 |      0.9046 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[877]\ttrain-mlogloss:0.39596+0.000707632\ttest-mlogloss:0.544678+0.00640045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/helpers.py:95: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  z = (mean - y_max - xi)/std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 07m58s |   -0.54468 |             0.9993 |    0.5129 |      4.0034 |             2.0828 |      0.8933 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1153]\ttrain-mlogloss:0.407059+0.00179506\ttest-mlogloss:0.54332+0.00567749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.30528409e-05]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00028024]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.26466873e-05]), 'nit': 6, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 08m23s |   -0.54332 |             0.7552 |    2.0722 |      4.9775 |             7.0850 |      0.8323 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[587]\ttrain-mlogloss:0.377684+0.00163147\ttest-mlogloss:0.544077+0.00571811\n",
      "\n",
      "   13 | 05m32s |   -0.54408 |             0.7474 |    1.3964 |      5.3378 |             2.3705 |      0.7276 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2063]\ttrain-mlogloss:0.420522+0.00145732\ttest-mlogloss:0.54445+0.00571767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00010604]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 12m13s |   -0.54445 |             0.8287 |    1.9510 |      3.8450 |             2.0672 |      0.8212 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1380]\ttrain-mlogloss:0.454491+0.00136536\ttest-mlogloss:0.549283+0.00592362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00033041]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 08m33s |   -0.54928 |             0.8541 |    0.5258 |      3.0379 |            46.5676 |      0.8193 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[642]\ttrain-mlogloss:0.38395+0.00211097\ttest-mlogloss:0.545517+0.00634387\n",
      "\n",
      "   16 | 05m55s |   -0.54552 |             0.7739 |    0.6402 |      5.7887 |             7.8536 |      0.9899 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[962]\ttrain-mlogloss:0.400896+0.00119462\ttest-mlogloss:0.54542+0.00588859\n",
      "\n",
      "   17 | 08m32s |   -0.54542 |             0.9962 |    1.9861 |      4.1640 |             3.6469 |      0.7160 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[427]\ttrain-mlogloss:0.474899+0.0099761\ttest-mlogloss:0.552888+0.00619794\n",
      "\n",
      "   18 | 04m41s |   -0.55289 |             1.0000 |    2.1000 |      5.8205 |            11.1328 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[590]\ttrain-mlogloss:0.411467+0.00095941\ttest-mlogloss:0.546195+0.00616944\n",
      "\n",
      "   19 | 05m42s |   -0.54619 |             0.8154 |    0.3837 |      5.6261 |            27.4351 |      0.9711 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3748]\ttrain-mlogloss:0.461767+0.005936\ttest-mlogloss:0.547081+0.00592719\n",
      "\n",
      "   20 | 22m11s |   -0.54708 |             0.9338 |    1.6663 |      3.6007 |             3.3060 |      0.9941 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[847]\ttrain-mlogloss:0.40373+0.000762137\ttest-mlogloss:0.543742+0.00553166\n",
      "\n",
      "   21 | 07m31s |   -0.54374 |             0.9760 |    1.6020 |      4.8312 |             1.9888 |      0.7914 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[605]\ttrain-mlogloss:0.399078+0.00168164\ttest-mlogloss:0.546069+0.00628979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00016017]), 'nit': 6, 'funcalls': 62}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 05m40s |   -0.54607 |             0.7401 |    0.7955 |      5.6141 |            19.4741 |      0.7050 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[744]\ttrain-mlogloss:0.384721+0.00152581\ttest-mlogloss:0.543492+0.00623367\n",
      "\n",
      "   23 | 07m05s |   -0.54349 |             0.7787 |    2.0531 |      5.2944 |             6.4305 |      0.7946 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[883]\ttrain-mlogloss:0.414706+0.000975753\ttest-mlogloss:0.545368+0.00585206\n",
      "\n",
      "   24 | 07m04s |   -0.54537 |             0.8570 |    0.7085 |      4.1269 |            16.1609 |      0.9140 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[752]\ttrain-mlogloss:0.405134+0.00251367\ttest-mlogloss:0.546578+0.00601038\n",
      "\n",
      "   25 | 06m32s |   -0.54658 |             0.7148 |    0.8454 |      5.5625 |            32.6769 |      0.9972 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[932]\ttrain-mlogloss:0.413185+0.00117827\ttest-mlogloss:0.545989+0.0061361\n",
      "\n",
      "   26 | 08m56s |   -0.54599 |             0.8596 |    2.0998 |      5.9371 |            45.5257 |      0.8906 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[731]\ttrain-mlogloss:0.417904+0.000829278\ttest-mlogloss:0.546723+0.00597709\n",
      "\n",
      "   27 | 06m27s |   -0.54672 |             0.7069 |    1.2359 |      5.0935 |            49.8025 |      0.9456 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[824]\ttrain-mlogloss:0.406877+0.00216968\ttest-mlogloss:0.545291+0.00652732\n",
      "\n",
      "   28 | 08m21s |   -0.54529 |             0.9324 |    2.0653 |      5.3196 |            25.2849 |      0.9494 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1044]\ttrain-mlogloss:0.425248+0.00144535\ttest-mlogloss:0.546766+0.00613904\n",
      "\n",
      "   29 | 07m29s |   -0.54677 |             0.7403 |    1.0093 |      4.7279 |            42.8639 |      0.9455 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[695]\ttrain-mlogloss:0.410234+0.0011553\ttest-mlogloss:0.545407+0.00522425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00029022]), 'nit': 4, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  4.00588069e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 06m29s |   -0.54541 |             0.7716 |    1.8068 |      5.8513 |            30.7775 |      0.7993 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1502]\ttrain-mlogloss:0.445836+0.00144625\ttest-mlogloss:0.54758+0.00574224\n",
      "\n",
      "   31 | 08m50s |   -0.54758 |             0.7966 |    1.4983 |      3.5242 |            28.6523 |      0.7825 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1763]\ttrain-mlogloss:0.426849+0.00122329\ttest-mlogloss:0.545375+0.00575754\n",
      "\n",
      "   32 | 11m55s |   -0.54538 |             0.7091 |    1.5837 |      3.6244 |             8.1661 |      0.8457 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1469]\ttrain-mlogloss:0.436492+0.00114213\ttest-mlogloss:0.546639+0.0056767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  3.21109546e-05]), 'nit': 4, 'funcalls': 54}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 10m05s |   -0.54664 |             0.8338 |    0.6415 |      3.6306 |            18.9455 |      0.8684 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1701]\ttrain-mlogloss:0.444753+0.00124344\ttest-mlogloss:0.546655+0.00574181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.81156142e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 09m20s |   -0.54665 |             0.7142 |    1.6389 |      3.1005 |            24.5330 |      0.8899 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1491]\ttrain-mlogloss:0.452802+0.00108144\ttest-mlogloss:0.54934+0.00599488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.83255179e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 09m18s |   -0.54934 |             0.8792 |    1.6112 |      3.0527 |            42.9291 |      0.7541 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[884]\ttrain-mlogloss:0.402449+0.0015482\ttest-mlogloss:0.545158+0.00689407\n",
      "\n",
      "   36 | 07m17s |   -0.54516 |             0.9046 |    0.3665 |      4.6079 |             5.7827 |      0.9339 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1518]\ttrain-mlogloss:0.429723+0.00119856\ttest-mlogloss:0.546094+0.00582152\n",
      "\n",
      "   37 | 09m33s |   -0.54609 |             0.9016 |    0.7308 |      3.1628 |            12.6465 |      0.8824 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1578]\ttrain-mlogloss:0.436717+0.00119455\ttest-mlogloss:0.545849+0.0058345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00014088]), 'nit': 4, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38 | 09m09s |   -0.54585 |             0.7854 |    1.6182 |      3.4167 |            13.7739 |      0.8322 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[732]\ttrain-mlogloss:0.450238+0.0010233\ttest-mlogloss:0.549448+0.00548907\n",
      "\n",
      "   39 | 05m38s |   -0.54945 |             0.7964 |    0.5751 |      4.9842 |            44.9441 |      0.7086 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1147]\ttrain-mlogloss:0.415803+0.00220998\ttest-mlogloss:0.545731+0.00633034\n",
      "\n",
      "   40 | 09m36s |   -0.54573 |             0.7258 |    1.9244 |      5.7463 |            46.7330 |      0.9780 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2241]\ttrain-mlogloss:0.45034+0.00213986\ttest-mlogloss:0.548627+0.00610175\n",
      "\n",
      "   41 | 13m44s |   -0.54863 |             0.9314 |    1.3956 |      3.6243 |            49.0671 |      0.9729 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[616]\ttrain-mlogloss:0.426598+0.00159515\ttest-mlogloss:0.546703+0.00572046\n",
      "\n",
      "   42 | 06m28s |   -0.54670 |             0.9371 |    2.0940 |      5.7122 |            36.6892 |      0.7947 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[894]\ttrain-mlogloss:0.411074+0.00132805\ttest-mlogloss:0.545699+0.00594749\n",
      "\n",
      "   43 | 06m45s |   -0.54570 |             0.7832 |    0.3476 |      4.9597 |            15.4925 |      0.7352 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[813]\ttrain-mlogloss:0.424286+0.00119387\ttest-mlogloss:0.546357+0.00613976\n",
      "\n",
      "   44 | 06m37s |   -0.54636 |             0.8694 |    0.4712 |      4.2705 |            22.7683 |      0.7880 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1491]\ttrain-mlogloss:0.453876+0.0012675\ttest-mlogloss:0.54816+0.00608291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00019838]), 'nit': 4, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00015251]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 | 09m10s |   -0.54816 |             0.8511 |    1.7619 |      3.7501 |            38.0995 |      0.7443 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[603]\ttrain-mlogloss:0.421017+0.00124178\ttest-mlogloss:0.547274+0.00539585\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00156888]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 | 05m30s |   -0.54727 |             0.7354 |    1.4790 |      5.6728 |            34.0076 |      0.7098 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[788]\ttrain-mlogloss:0.416902+0.00171272\ttest-mlogloss:0.546026+0.00653838\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00232891]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00010399]), 'nit': 7, 'funcalls': 65}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47 | 08m04s |   -0.54603 |             0.7913 |    2.0545 |      5.5018 |            39.8571 |      0.8385 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1046]\ttrain-mlogloss:0.425592+0.00155637\ttest-mlogloss:0.546166+0.00545797\n",
      "\n",
      "   48 | 11m04s |   -0.54617 |             0.7892 |    2.0132 |      4.8927 |            26.4855 |      0.7682 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[629]\ttrain-mlogloss:0.390545+0.00148152\ttest-mlogloss:0.545658+0.00690329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00094195]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   49 | 09m36s |   -0.54566 |             0.9852 |    0.3648 |      5.9029 |            22.6354 |      0.8157 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[897]\ttrain-mlogloss:0.420927+0.00130185\ttest-mlogloss:0.546387+0.00584617\n",
      "\n",
      "   50 | 07m26s |   -0.54639 |             0.8990 |    0.4912 |      4.2103 |            30.6949 |      0.8513 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=10,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (3,6),\n",
    "        'min_child_weight': (1,50),\n",
    "        'colsample_bytree': (0.7,1),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0.3,2.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.977517</td>\n",
       "      <td>7.085047</td>\n",
       "      <td>0.755216</td>\n",
       "      <td>0.832299</td>\n",
       "      <td>2.072250</td>\n",
       "      <td>-0.543320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.294358</td>\n",
       "      <td>6.430458</td>\n",
       "      <td>0.778670</td>\n",
       "      <td>0.794628</td>\n",
       "      <td>2.053074</td>\n",
       "      <td>-0.543492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.831209</td>\n",
       "      <td>1.988818</td>\n",
       "      <td>0.976038</td>\n",
       "      <td>0.791368</td>\n",
       "      <td>1.601972</td>\n",
       "      <td>-0.543742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.337825</td>\n",
       "      <td>2.370541</td>\n",
       "      <td>0.747375</td>\n",
       "      <td>0.727643</td>\n",
       "      <td>1.396417</td>\n",
       "      <td>-0.544077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.844964</td>\n",
       "      <td>2.067151</td>\n",
       "      <td>0.828660</td>\n",
       "      <td>0.821156</td>\n",
       "      <td>1.951036</td>\n",
       "      <td>-0.544450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.003396</td>\n",
       "      <td>2.082787</td>\n",
       "      <td>0.999265</td>\n",
       "      <td>0.893346</td>\n",
       "      <td>0.512947</td>\n",
       "      <td>-0.544678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.607936</td>\n",
       "      <td>5.782687</td>\n",
       "      <td>0.904582</td>\n",
       "      <td>0.933866</td>\n",
       "      <td>0.366536</td>\n",
       "      <td>-0.545158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.319597</td>\n",
       "      <td>25.284915</td>\n",
       "      <td>0.932358</td>\n",
       "      <td>0.949350</td>\n",
       "      <td>2.065272</td>\n",
       "      <td>-0.545291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.126909</td>\n",
       "      <td>16.160868</td>\n",
       "      <td>0.857025</td>\n",
       "      <td>0.913964</td>\n",
       "      <td>0.708522</td>\n",
       "      <td>-0.545368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.624415</td>\n",
       "      <td>8.166120</td>\n",
       "      <td>0.709067</td>\n",
       "      <td>0.845712</td>\n",
       "      <td>1.583732</td>\n",
       "      <td>-0.545375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "1    4.977517          7.085047          0.755216   0.832299  2.072250   \n",
       "12   5.294358          6.430458          0.778670   0.794628  2.053074   \n",
       "10   4.831209          1.988818          0.976038   0.791368  1.601972   \n",
       "2    5.337825          2.370541          0.747375   0.727643  1.396417   \n",
       "3    3.844964          2.067151          0.828660   0.821156  1.951036   \n",
       "0    4.003396          2.082787          0.999265   0.893346  0.512947   \n",
       "25   4.607936          5.782687          0.904582   0.933866  0.366536   \n",
       "17   5.319597         25.284915          0.932358   0.949350  2.065272   \n",
       "13   4.126909         16.160868          0.857025   0.913964  0.708522   \n",
       "21   3.624415          8.166120          0.709067   0.845712  1.583732   \n",
       "\n",
       "       score  \n",
       "1  -0.543320  \n",
       "12 -0.543492  \n",
       "10 -0.543742  \n",
       "2  -0.544077  \n",
       "3  -0.544450  \n",
       "0  -0.544678  \n",
       "25 -0.545158  \n",
       "17 -0.545291  \n",
       "13 -0.545368  \n",
       "21 -0.545375  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.03)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]   \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds,test_blend_x_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.6,\n",
      "       gamma=3, learning_rate=0.03, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=5, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.9)\n",
      "Model 1 fold 1\n",
      "best round 4905\n",
      "('Score: ', 0.54247337742848956)\n",
      "Model 1 fold 1 fitting finished in 676.769s\n",
      "Model 1 fold 2\n",
      "best round 3350\n",
      "('Score: ', 0.53380446733242892)\n",
      "Model 1 fold 2 fitting finished in 477.740s\n",
      "Model 1 fold 3\n",
      "best round 5665\n",
      "('Score: ', 0.51841402729840236)\n",
      "Model 1 fold 3 fitting finished in 778.781s\n",
      "Model 1 fold 4\n",
      "best round 4150\n",
      "('Score: ', 0.53219100574651057)\n",
      "Model 1 fold 4 fitting finished in 576.468s\n",
      "Model 1 fold 5\n",
      "best round 3523\n",
      "('Score: ', 0.52547553537656211)\n",
      "Model 1 fold 5 fitting finished in 492.912s\n",
      "Model 1 fold 6\n",
      "best round 4176\n",
      "('Score: ', 0.53556574616785135)\n",
      "Model 1 fold 6 fitting finished in 576.606s\n",
      "Model 1 fold 7\n",
      "best round 3638\n",
      "('Score: ', 0.54445061538823125)\n",
      "Model 1 fold 7 fitting finished in 504.942s\n",
      "Model 1 fold 8\n",
      "best round 4246\n",
      "('Score: ', 0.53410266267196616)\n",
      "Model 1 fold 8 fitting finished in 579.738s\n",
      "Model 1 fold 9\n",
      "best round 3255\n",
      "('Score: ', 0.54783848061990847)\n",
      "Model 1 fold 9 fitting finished in 457.126s\n",
      "Model 1 fold 10\n",
      "best round 4877\n",
      "('Score: ', 0.54727095060811171)\n",
      "Model 1 fold 10 fitting finished in 664.497s\n",
      "Score for model 1 is 0.536159\n",
      "Score for blended models is 0.536159\n"
     ]
    }
   ],
   "source": [
    "estimators = [xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 5,\n",
    "                              colsample_bytree = 0.6,\n",
    "                              subsample = 0.9,\n",
    "                              gamma = 3),\n",
    "#              xgb.XGBClassifier(max_depth = 5,\n",
    "#                               min_child_weight = 6,\n",
    "#                               colsample_bytree = 0.778670,\n",
    "#                               subsample = 0.794628,\n",
    "#                               gamma = 2.053074)\n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# xgb_params = [{'max_depth':6,\n",
    "#                'min_child_weight':5,\n",
    "#                'colsample_bytree':0.6,\n",
    "#                'subsample':0.9,\n",
    "#                'gamma':3},\n",
    "# #               score -0.543320        \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':6,\n",
    "#                'colsample_bytree':0.778670,\n",
    "#                'subsample':0.794628,\n",
    "#                'gamma':2.053074},\n",
    "#               score -0.543492\n",
    "#               {'max_depth':4,\n",
    "#                'min_child_weight':1,\n",
    "#                'colsample_bytree':0.976038,\n",
    "#                'subsample':0.791368,\n",
    "#                'gamma':1.601972},\n",
    "# #               score -0.543742  \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.747375,\n",
    "#                'subsample':0.727643,\n",
    "#                'gamma':1.396417},\n",
    "# #               score -0.544077      \n",
    "#               {'max_depth':3,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.828660,\n",
    "#                'subsample':0.821156,\n",
    "#                'gamma':1.951036}\n",
    "# #               score -0.544450\n",
    "#              ]\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb,tmp) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n",
    "\n",
    "# print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "# print (np.mean(best_rounds_xgb_le,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044566</td>\n",
       "      <td>0.460453</td>\n",
       "      <td>0.494981</td>\n",
       "      <td>0.046104</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.467042</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.451836</td>\n",
       "      <td>0.495925</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529387</td>\n",
       "      <td>0.054692</td>\n",
       "      <td>0.491957</td>\n",
       "      <td>0.453351</td>\n",
       "      <td>0.065767</td>\n",
       "      <td>0.526081</td>\n",
       "      <td>0.408152</td>\n",
       "      <td>0.042964</td>\n",
       "      <td>0.517221</td>\n",
       "      <td>0.439816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.958746</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.030283</td>\n",
       "      <td>0.967088</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.970128</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961575</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.959779</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.960791</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.964111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088891</td>\n",
       "      <td>0.428814</td>\n",
       "      <td>0.482295</td>\n",
       "      <td>0.083227</td>\n",
       "      <td>0.433731</td>\n",
       "      <td>0.483042</td>\n",
       "      <td>0.097146</td>\n",
       "      <td>0.453983</td>\n",
       "      <td>0.448872</td>\n",
       "      <td>0.112520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506227</td>\n",
       "      <td>0.080920</td>\n",
       "      <td>0.441123</td>\n",
       "      <td>0.477956</td>\n",
       "      <td>0.090725</td>\n",
       "      <td>0.442763</td>\n",
       "      <td>0.466512</td>\n",
       "      <td>0.064595</td>\n",
       "      <td>0.452286</td>\n",
       "      <td>0.483119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119714</td>\n",
       "      <td>0.510923</td>\n",
       "      <td>0.369363</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.496122</td>\n",
       "      <td>0.382139</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0.482848</td>\n",
       "      <td>0.408018</td>\n",
       "      <td>0.180610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456661</td>\n",
       "      <td>0.094824</td>\n",
       "      <td>0.473570</td>\n",
       "      <td>0.431606</td>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.507627</td>\n",
       "      <td>0.362541</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>0.523689</td>\n",
       "      <td>0.368335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.246655</td>\n",
       "      <td>0.522666</td>\n",
       "      <td>0.230679</td>\n",
       "      <td>0.170953</td>\n",
       "      <td>0.618990</td>\n",
       "      <td>0.210057</td>\n",
       "      <td>0.254950</td>\n",
       "      <td>0.569017</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.222274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>0.269260</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.182475</td>\n",
       "      <td>0.250829</td>\n",
       "      <td>0.560539</td>\n",
       "      <td>0.188633</td>\n",
       "      <td>0.211703</td>\n",
       "      <td>0.554986</td>\n",
       "      <td>0.233311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.160027</td>\n",
       "      <td>0.495432</td>\n",
       "      <td>0.344541</td>\n",
       "      <td>0.139873</td>\n",
       "      <td>0.486927</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.124840</td>\n",
       "      <td>0.460282</td>\n",
       "      <td>0.414879</td>\n",
       "      <td>0.167588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371646</td>\n",
       "      <td>0.153144</td>\n",
       "      <td>0.486861</td>\n",
       "      <td>0.359995</td>\n",
       "      <td>0.151369</td>\n",
       "      <td>0.547396</td>\n",
       "      <td>0.301234</td>\n",
       "      <td>0.153448</td>\n",
       "      <td>0.461628</td>\n",
       "      <td>0.384924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.311935</td>\n",
       "      <td>0.665378</td>\n",
       "      <td>0.026476</td>\n",
       "      <td>0.368393</td>\n",
       "      <td>0.605131</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.297906</td>\n",
       "      <td>0.680196</td>\n",
       "      <td>0.031636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707958</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.340147</td>\n",
       "      <td>0.638020</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.349858</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.021646</td>\n",
       "      <td>0.308829</td>\n",
       "      <td>0.669524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.065749</td>\n",
       "      <td>0.930302</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.054972</td>\n",
       "      <td>0.941177</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.039236</td>\n",
       "      <td>0.958927</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940611</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.071063</td>\n",
       "      <td>0.924182</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.055730</td>\n",
       "      <td>0.940583</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>0.953587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.028745</td>\n",
       "      <td>0.383771</td>\n",
       "      <td>0.587485</td>\n",
       "      <td>0.031779</td>\n",
       "      <td>0.408973</td>\n",
       "      <td>0.559248</td>\n",
       "      <td>0.026125</td>\n",
       "      <td>0.392499</td>\n",
       "      <td>0.581376</td>\n",
       "      <td>0.027558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634928</td>\n",
       "      <td>0.025109</td>\n",
       "      <td>0.355646</td>\n",
       "      <td>0.619245</td>\n",
       "      <td>0.026475</td>\n",
       "      <td>0.336836</td>\n",
       "      <td>0.636689</td>\n",
       "      <td>0.027231</td>\n",
       "      <td>0.320358</td>\n",
       "      <td>0.652411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.054284</td>\n",
       "      <td>0.544205</td>\n",
       "      <td>0.401512</td>\n",
       "      <td>0.071739</td>\n",
       "      <td>0.575213</td>\n",
       "      <td>0.353048</td>\n",
       "      <td>0.062840</td>\n",
       "      <td>0.585746</td>\n",
       "      <td>0.351414</td>\n",
       "      <td>0.081150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405106</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>0.599840</td>\n",
       "      <td>0.335255</td>\n",
       "      <td>0.062128</td>\n",
       "      <td>0.563416</td>\n",
       "      <td>0.374456</td>\n",
       "      <td>0.044447</td>\n",
       "      <td>0.597871</td>\n",
       "      <td>0.357682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.042166</td>\n",
       "      <td>0.429802</td>\n",
       "      <td>0.528032</td>\n",
       "      <td>0.055513</td>\n",
       "      <td>0.433984</td>\n",
       "      <td>0.510503</td>\n",
       "      <td>0.057928</td>\n",
       "      <td>0.470640</td>\n",
       "      <td>0.471432</td>\n",
       "      <td>0.061666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502822</td>\n",
       "      <td>0.046098</td>\n",
       "      <td>0.475959</td>\n",
       "      <td>0.477943</td>\n",
       "      <td>0.046920</td>\n",
       "      <td>0.420018</td>\n",
       "      <td>0.533062</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.452691</td>\n",
       "      <td>0.506498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.427309</td>\n",
       "      <td>0.525419</td>\n",
       "      <td>0.041077</td>\n",
       "      <td>0.429253</td>\n",
       "      <td>0.529670</td>\n",
       "      <td>0.045966</td>\n",
       "      <td>0.454744</td>\n",
       "      <td>0.499291</td>\n",
       "      <td>0.063209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.042955</td>\n",
       "      <td>0.494583</td>\n",
       "      <td>0.462462</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.472614</td>\n",
       "      <td>0.478442</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.472693</td>\n",
       "      <td>0.486657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.104757</td>\n",
       "      <td>0.685763</td>\n",
       "      <td>0.209481</td>\n",
       "      <td>0.132085</td>\n",
       "      <td>0.642619</td>\n",
       "      <td>0.225296</td>\n",
       "      <td>0.086186</td>\n",
       "      <td>0.713880</td>\n",
       "      <td>0.199934</td>\n",
       "      <td>0.158341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178935</td>\n",
       "      <td>0.124008</td>\n",
       "      <td>0.660082</td>\n",
       "      <td>0.215910</td>\n",
       "      <td>0.147815</td>\n",
       "      <td>0.644569</td>\n",
       "      <td>0.207616</td>\n",
       "      <td>0.086359</td>\n",
       "      <td>0.727303</td>\n",
       "      <td>0.186338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.103578</td>\n",
       "      <td>0.887546</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.075047</td>\n",
       "      <td>0.918623</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>0.085090</td>\n",
       "      <td>0.909723</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926365</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.103368</td>\n",
       "      <td>0.888270</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.086649</td>\n",
       "      <td>0.906097</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.115490</td>\n",
       "      <td>0.879359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.057602</td>\n",
       "      <td>0.934966</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.057084</td>\n",
       "      <td>0.937482</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.052984</td>\n",
       "      <td>0.941564</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941670</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>0.934610</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.053319</td>\n",
       "      <td>0.940687</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.055074</td>\n",
       "      <td>0.940987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.020249</td>\n",
       "      <td>0.351863</td>\n",
       "      <td>0.627888</td>\n",
       "      <td>0.019128</td>\n",
       "      <td>0.384489</td>\n",
       "      <td>0.596383</td>\n",
       "      <td>0.015287</td>\n",
       "      <td>0.311260</td>\n",
       "      <td>0.673454</td>\n",
       "      <td>0.024589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651066</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.392196</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.325454</td>\n",
       "      <td>0.657015</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.643750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.088783</td>\n",
       "      <td>0.597665</td>\n",
       "      <td>0.313552</td>\n",
       "      <td>0.091671</td>\n",
       "      <td>0.615094</td>\n",
       "      <td>0.293236</td>\n",
       "      <td>0.078085</td>\n",
       "      <td>0.581246</td>\n",
       "      <td>0.340669</td>\n",
       "      <td>0.105082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324348</td>\n",
       "      <td>0.094320</td>\n",
       "      <td>0.603873</td>\n",
       "      <td>0.301807</td>\n",
       "      <td>0.077456</td>\n",
       "      <td>0.667530</td>\n",
       "      <td>0.255014</td>\n",
       "      <td>0.069120</td>\n",
       "      <td>0.623599</td>\n",
       "      <td>0.307281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.063536</td>\n",
       "      <td>0.393688</td>\n",
       "      <td>0.542776</td>\n",
       "      <td>0.082301</td>\n",
       "      <td>0.461901</td>\n",
       "      <td>0.455798</td>\n",
       "      <td>0.063453</td>\n",
       "      <td>0.443082</td>\n",
       "      <td>0.493465</td>\n",
       "      <td>0.095398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>0.073546</td>\n",
       "      <td>0.445316</td>\n",
       "      <td>0.481138</td>\n",
       "      <td>0.071652</td>\n",
       "      <td>0.436788</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>0.062834</td>\n",
       "      <td>0.484610</td>\n",
       "      <td>0.452556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.079577</td>\n",
       "      <td>0.484976</td>\n",
       "      <td>0.435448</td>\n",
       "      <td>0.119288</td>\n",
       "      <td>0.469267</td>\n",
       "      <td>0.411445</td>\n",
       "      <td>0.082312</td>\n",
       "      <td>0.407661</td>\n",
       "      <td>0.510028</td>\n",
       "      <td>0.093583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484121</td>\n",
       "      <td>0.122815</td>\n",
       "      <td>0.436286</td>\n",
       "      <td>0.440899</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>0.441752</td>\n",
       "      <td>0.453320</td>\n",
       "      <td>0.070126</td>\n",
       "      <td>0.444832</td>\n",
       "      <td>0.485042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.061267</td>\n",
       "      <td>0.521871</td>\n",
       "      <td>0.416862</td>\n",
       "      <td>0.089319</td>\n",
       "      <td>0.506330</td>\n",
       "      <td>0.404352</td>\n",
       "      <td>0.078493</td>\n",
       "      <td>0.442584</td>\n",
       "      <td>0.478923</td>\n",
       "      <td>0.075068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440366</td>\n",
       "      <td>0.062491</td>\n",
       "      <td>0.582118</td>\n",
       "      <td>0.355391</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.448136</td>\n",
       "      <td>0.459811</td>\n",
       "      <td>0.049232</td>\n",
       "      <td>0.537966</td>\n",
       "      <td>0.412802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.044566  0.460453  0.494981  0.046104  0.486853  0.467042  0.052239   \n",
       "1   0.004466  0.036788  0.958746  0.002629  0.030283  0.967088  0.003106   \n",
       "2   0.088891  0.428814  0.482295  0.083227  0.433731  0.483042  0.097146   \n",
       "3   0.119714  0.510923  0.369363  0.121739  0.496122  0.382139  0.109134   \n",
       "4   0.246655  0.522666  0.230679  0.170953  0.618990  0.210057  0.254950   \n",
       "5   0.160027  0.495432  0.344541  0.139873  0.486927  0.373200  0.124840   \n",
       "6   0.022687  0.311935  0.665378  0.026476  0.368393  0.605131  0.021898   \n",
       "7   0.003949  0.065749  0.930302  0.003851  0.054972  0.941177  0.001837   \n",
       "8   0.028745  0.383771  0.587485  0.031779  0.408973  0.559248  0.026125   \n",
       "9   0.054284  0.544205  0.401512  0.071739  0.575213  0.353048  0.062840   \n",
       "10  0.042166  0.429802  0.528032  0.055513  0.433984  0.510503  0.057928   \n",
       "11  0.047272  0.427309  0.525419  0.041077  0.429253  0.529670  0.045966   \n",
       "12  0.104757  0.685763  0.209481  0.132085  0.642619  0.225296  0.086186   \n",
       "13  0.008876  0.103578  0.887546  0.006329  0.075047  0.918623  0.005187   \n",
       "14  0.007432  0.057602  0.934966  0.005433  0.057084  0.937482  0.005453   \n",
       "15  0.020249  0.351863  0.627888  0.019128  0.384489  0.596383  0.015287   \n",
       "16  0.088783  0.597665  0.313552  0.091671  0.615094  0.293236  0.078085   \n",
       "17  0.063536  0.393688  0.542776  0.082301  0.461901  0.455798  0.063453   \n",
       "18  0.079577  0.484976  0.435448  0.119288  0.469267  0.411445  0.082312   \n",
       "19  0.061267  0.521871  0.416862  0.089319  0.506330  0.404352  0.078493   \n",
       "\n",
       "          7         8         9     ...           20        21        22  \\\n",
       "0   0.451836  0.495925  0.060600    ...     0.529387  0.054692  0.491957   \n",
       "1   0.026766  0.970128  0.004504    ...     0.961575  0.004076  0.036145   \n",
       "2   0.453983  0.448872  0.112520    ...     0.506227  0.080920  0.441123   \n",
       "3   0.482848  0.408018  0.180610    ...     0.456661  0.094824  0.473570   \n",
       "4   0.569017  0.176034  0.222274    ...     0.179533  0.269260  0.548265   \n",
       "5   0.460282  0.414879  0.167588    ...     0.371646  0.153144  0.486861   \n",
       "6   0.297906  0.680196  0.031636    ...     0.707958  0.021832  0.340147   \n",
       "7   0.039236  0.958927  0.005270    ...     0.940611  0.004755  0.071063   \n",
       "8   0.392499  0.581376  0.027558    ...     0.634928  0.025109  0.355646   \n",
       "9   0.585746  0.351414  0.081150    ...     0.405106  0.064905  0.599840   \n",
       "10  0.470640  0.471432  0.061666    ...     0.502822  0.046098  0.475959   \n",
       "11  0.454744  0.499291  0.063209    ...     0.457949  0.042955  0.494583   \n",
       "12  0.713880  0.199934  0.158341    ...     0.178935  0.124008  0.660082   \n",
       "13  0.085090  0.909723  0.005848    ...     0.926365  0.008363  0.103368   \n",
       "14  0.052984  0.941564  0.007040    ...     0.941670  0.006179  0.059211   \n",
       "15  0.311260  0.673454  0.024589    ...     0.651066  0.021372  0.392196   \n",
       "16  0.581246  0.340669  0.105082    ...     0.324348  0.094320  0.603873   \n",
       "17  0.443082  0.493465  0.095398    ...     0.484165  0.073546  0.445316   \n",
       "18  0.407661  0.510028  0.093583    ...     0.484121  0.122815  0.436286   \n",
       "19  0.442584  0.478923  0.075068    ...     0.440366  0.062491  0.582118   \n",
       "\n",
       "          23        24        25        26        27        28        29  \n",
       "0   0.453351  0.065767  0.526081  0.408152  0.042964  0.517221  0.439816  \n",
       "1   0.959779  0.003172  0.036037  0.960791  0.002761  0.033128  0.964111  \n",
       "2   0.477956  0.090725  0.442763  0.466512  0.064595  0.452286  0.483119  \n",
       "3   0.431606  0.129832  0.507627  0.362541  0.107976  0.523689  0.368335  \n",
       "4   0.182475  0.250829  0.560539  0.188633  0.211703  0.554986  0.233311  \n",
       "5   0.359995  0.151369  0.547396  0.301234  0.153448  0.461628  0.384924  \n",
       "6   0.638020  0.024242  0.349858  0.625900  0.021646  0.308829  0.669524  \n",
       "7   0.924182  0.003687  0.055730  0.940583  0.002435  0.043978  0.953587  \n",
       "8   0.619245  0.026475  0.336836  0.636689  0.027231  0.320358  0.652411  \n",
       "9   0.335255  0.062128  0.563416  0.374456  0.044447  0.597871  0.357682  \n",
       "10  0.477943  0.046920  0.420018  0.533062  0.040811  0.452691  0.506498  \n",
       "11  0.462462  0.048944  0.472614  0.478442  0.040650  0.472693  0.486657  \n",
       "12  0.215910  0.147815  0.644569  0.207616  0.086359  0.727303  0.186338  \n",
       "13  0.888270  0.007254  0.086649  0.906097  0.005151  0.115490  0.879359  \n",
       "14  0.934610  0.005994  0.053319  0.940687  0.003939  0.055074  0.940987  \n",
       "15  0.586432  0.017531  0.325454  0.657015  0.014407  0.341843  0.643750  \n",
       "16  0.301807  0.077456  0.667530  0.255014  0.069120  0.623599  0.307281  \n",
       "17  0.481138  0.071652  0.436788  0.491560  0.062834  0.484610  0.452556  \n",
       "18  0.440899  0.104928  0.441752  0.453320  0.070126  0.444832  0.485042  \n",
       "19  0.355391  0.092053  0.448136  0.459811  0.049232  0.537966  0.412802  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tmp).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050689</td>\n",
       "      <td>0.479141</td>\n",
       "      <td>0.470170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.961845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091907</td>\n",
       "      <td>0.430084</td>\n",
       "      <td>0.478010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118784</td>\n",
       "      <td>0.504385</td>\n",
       "      <td>0.376830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.243760</td>\n",
       "      <td>0.551849</td>\n",
       "      <td>0.204390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.149103</td>\n",
       "      <td>0.478667</td>\n",
       "      <td>0.372229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.024415</td>\n",
       "      <td>0.330223</td>\n",
       "      <td>0.645363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>0.938237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.027641</td>\n",
       "      <td>0.359416</td>\n",
       "      <td>0.612943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.062964</td>\n",
       "      <td>0.562003</td>\n",
       "      <td>0.375033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050612</td>\n",
       "      <td>0.440275</td>\n",
       "      <td>0.509114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.465125</td>\n",
       "      <td>0.486676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.120510</td>\n",
       "      <td>0.666354</td>\n",
       "      <td>0.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.089687</td>\n",
       "      <td>0.903818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.055927</td>\n",
       "      <td>0.938150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.343824</td>\n",
       "      <td>0.637552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.088053</td>\n",
       "      <td>0.602183</td>\n",
       "      <td>0.309763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.076711</td>\n",
       "      <td>0.432716</td>\n",
       "      <td>0.490574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.101214</td>\n",
       "      <td>0.424807</td>\n",
       "      <td>0.473979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.070080</td>\n",
       "      <td>0.496558</td>\n",
       "      <td>0.433361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.050689  0.479141  0.470170\n",
       "1   0.003556  0.034600  0.961845\n",
       "2   0.091907  0.430084  0.478010\n",
       "3   0.118784  0.504385  0.376830\n",
       "4   0.243760  0.551849  0.204390\n",
       "5   0.149103  0.478667  0.372229\n",
       "6   0.024415  0.330223  0.645363\n",
       "7   0.003765  0.057998  0.938237\n",
       "8   0.027641  0.359416  0.612943\n",
       "9   0.062964  0.562003  0.375033\n",
       "10  0.050612  0.440275  0.509114\n",
       "11  0.048200  0.465125  0.486676\n",
       "12  0.120510  0.666354  0.213135\n",
       "13  0.006495  0.089687  0.903818\n",
       "14  0.005923  0.055927  0.938150\n",
       "15  0.018624  0.343824  0.637552\n",
       "16  0.088053  0.602183  0.309763\n",
       "17  0.076711  0.432716  0.490574\n",
       "18  0.101214  0.424807  0.473979\n",
       "19  0.070080  0.496558  0.433361"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_blend_x_xgb).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.072210</td>\n",
       "      <td>0.258895</td>\n",
       "      <td>0.668895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046046</td>\n",
       "      <td>0.074063</td>\n",
       "      <td>0.879891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105463</td>\n",
       "      <td>0.350333</td>\n",
       "      <td>0.544204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078242</td>\n",
       "      <td>0.275522</td>\n",
       "      <td>0.646236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124018</td>\n",
       "      <td>0.367762</td>\n",
       "      <td>0.508220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.107140</td>\n",
       "      <td>0.374264</td>\n",
       "      <td>0.518595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.259383</td>\n",
       "      <td>0.669192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050132</td>\n",
       "      <td>0.104337</td>\n",
       "      <td>0.845532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070318</td>\n",
       "      <td>0.256687</td>\n",
       "      <td>0.672995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.071713</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>0.667845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.075188</td>\n",
       "      <td>0.273335</td>\n",
       "      <td>0.651478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.072671</td>\n",
       "      <td>0.264783</td>\n",
       "      <td>0.662546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.107579</td>\n",
       "      <td>0.356726</td>\n",
       "      <td>0.535695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.047172</td>\n",
       "      <td>0.075618</td>\n",
       "      <td>0.877210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.046893</td>\n",
       "      <td>0.074179</td>\n",
       "      <td>0.878927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.258281</td>\n",
       "      <td>0.670554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.105223</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.513776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.077122</td>\n",
       "      <td>0.274320</td>\n",
       "      <td>0.648558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.108725</td>\n",
       "      <td>0.215958</td>\n",
       "      <td>0.675317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.077963</td>\n",
       "      <td>0.289275</td>\n",
       "      <td>0.632763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.071323</td>\n",
       "      <td>0.252329</td>\n",
       "      <td>0.676347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.049804</td>\n",
       "      <td>0.098135</td>\n",
       "      <td>0.852061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.069473</td>\n",
       "      <td>0.257856</td>\n",
       "      <td>0.672671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.107513</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.511569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.049912</td>\n",
       "      <td>0.098893</td>\n",
       "      <td>0.851195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.071092</td>\n",
       "      <td>0.259481</td>\n",
       "      <td>0.669427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.072482</td>\n",
       "      <td>0.265904</td>\n",
       "      <td>0.661614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.071211</td>\n",
       "      <td>0.250039</td>\n",
       "      <td>0.678750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.106431</td>\n",
       "      <td>0.359449</td>\n",
       "      <td>0.534120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.045824</td>\n",
       "      <td>0.072379</td>\n",
       "      <td>0.881797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74629</th>\n",
       "      <td>0.079330</td>\n",
       "      <td>0.278175</td>\n",
       "      <td>0.642494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74630</th>\n",
       "      <td>0.106492</td>\n",
       "      <td>0.381427</td>\n",
       "      <td>0.512081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74631</th>\n",
       "      <td>0.046917</td>\n",
       "      <td>0.074311</td>\n",
       "      <td>0.878772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74632</th>\n",
       "      <td>0.070584</td>\n",
       "      <td>0.241576</td>\n",
       "      <td>0.687840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74633</th>\n",
       "      <td>0.071047</td>\n",
       "      <td>0.270213</td>\n",
       "      <td>0.658740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74634</th>\n",
       "      <td>0.071150</td>\n",
       "      <td>0.248639</td>\n",
       "      <td>0.680211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>0.071795</td>\n",
       "      <td>0.273016</td>\n",
       "      <td>0.655190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74636</th>\n",
       "      <td>0.070684</td>\n",
       "      <td>0.248615</td>\n",
       "      <td>0.680702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74637</th>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.109703</td>\n",
       "      <td>0.821319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74638</th>\n",
       "      <td>0.077893</td>\n",
       "      <td>0.268131</td>\n",
       "      <td>0.653976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74639</th>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.373058</td>\n",
       "      <td>0.520544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74640</th>\n",
       "      <td>0.070253</td>\n",
       "      <td>0.257014</td>\n",
       "      <td>0.672734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74641</th>\n",
       "      <td>0.072324</td>\n",
       "      <td>0.266449</td>\n",
       "      <td>0.661227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74642</th>\n",
       "      <td>0.105144</td>\n",
       "      <td>0.351725</td>\n",
       "      <td>0.543131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74643</th>\n",
       "      <td>0.071228</td>\n",
       "      <td>0.262666</td>\n",
       "      <td>0.666106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74644</th>\n",
       "      <td>0.071551</td>\n",
       "      <td>0.251076</td>\n",
       "      <td>0.677373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74645</th>\n",
       "      <td>0.071483</td>\n",
       "      <td>0.264011</td>\n",
       "      <td>0.664506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>0.070716</td>\n",
       "      <td>0.266799</td>\n",
       "      <td>0.662485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74647</th>\n",
       "      <td>0.071747</td>\n",
       "      <td>0.251681</td>\n",
       "      <td>0.676572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74648</th>\n",
       "      <td>0.105840</td>\n",
       "      <td>0.362454</td>\n",
       "      <td>0.531706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74649</th>\n",
       "      <td>0.104327</td>\n",
       "      <td>0.376822</td>\n",
       "      <td>0.518851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>0.106862</td>\n",
       "      <td>0.374171</td>\n",
       "      <td>0.518967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74651</th>\n",
       "      <td>0.105827</td>\n",
       "      <td>0.372533</td>\n",
       "      <td>0.521640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74652</th>\n",
       "      <td>0.046963</td>\n",
       "      <td>0.079294</td>\n",
       "      <td>0.873742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74653</th>\n",
       "      <td>0.047748</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>0.876651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74654</th>\n",
       "      <td>0.049066</td>\n",
       "      <td>0.096756</td>\n",
       "      <td>0.854177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74655</th>\n",
       "      <td>0.071110</td>\n",
       "      <td>0.250985</td>\n",
       "      <td>0.677906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>0.072942</td>\n",
       "      <td>0.262177</td>\n",
       "      <td>0.664881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>0.085767</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>0.622971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>0.070912</td>\n",
       "      <td>0.250066</td>\n",
       "      <td>0.679022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74659 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2\n",
       "0      0.072210  0.258895  0.668895\n",
       "1      0.046046  0.074063  0.879891\n",
       "2      0.105463  0.350333  0.544204\n",
       "3      0.078242  0.275522  0.646236\n",
       "4      0.124018  0.367762  0.508220\n",
       "5      0.107140  0.374264  0.518595\n",
       "6      0.071425  0.259383  0.669192\n",
       "7      0.050132  0.104337  0.845532\n",
       "8      0.070318  0.256687  0.672995\n",
       "9      0.071713  0.260442  0.667845\n",
       "10     0.075188  0.273335  0.651478\n",
       "11     0.072671  0.264783  0.662546\n",
       "12     0.107579  0.356726  0.535695\n",
       "13     0.047172  0.075618  0.877210\n",
       "14     0.046893  0.074179  0.878927\n",
       "15     0.071165  0.258281  0.670554\n",
       "16     0.105223  0.381002  0.513776\n",
       "17     0.077122  0.274320  0.648558\n",
       "18     0.108725  0.215958  0.675317\n",
       "19     0.077963  0.289275  0.632763\n",
       "20     0.071323  0.252329  0.676347\n",
       "21     0.049804  0.098135  0.852061\n",
       "22     0.069473  0.257856  0.672671\n",
       "23     0.107513  0.380919  0.511569\n",
       "24     0.049912  0.098893  0.851195\n",
       "25     0.071092  0.259481  0.669427\n",
       "26     0.072482  0.265904  0.661614\n",
       "27     0.071211  0.250039  0.678750\n",
       "28     0.106431  0.359449  0.534120\n",
       "29     0.045824  0.072379  0.881797\n",
       "...         ...       ...       ...\n",
       "74629  0.079330  0.278175  0.642494\n",
       "74630  0.106492  0.381427  0.512081\n",
       "74631  0.046917  0.074311  0.878772\n",
       "74632  0.070584  0.241576  0.687840\n",
       "74633  0.071047  0.270213  0.658740\n",
       "74634  0.071150  0.248639  0.680211\n",
       "74635  0.071795  0.273016  0.655190\n",
       "74636  0.070684  0.248615  0.680702\n",
       "74637  0.068978  0.109703  0.821319\n",
       "74638  0.077893  0.268131  0.653976\n",
       "74639  0.106398  0.373058  0.520544\n",
       "74640  0.070253  0.257014  0.672734\n",
       "74641  0.072324  0.266449  0.661227\n",
       "74642  0.105144  0.351725  0.543131\n",
       "74643  0.071228  0.262666  0.666106\n",
       "74644  0.071551  0.251076  0.677373\n",
       "74645  0.071483  0.264011  0.664506\n",
       "74646  0.070716  0.266799  0.662485\n",
       "74647  0.071747  0.251681  0.676572\n",
       "74648  0.105840  0.362454  0.531706\n",
       "74649  0.104327  0.376822  0.518851\n",
       "74650  0.106862  0.374171  0.518967\n",
       "74651  0.105827  0.372533  0.521640\n",
       "74652  0.046963  0.079294  0.873742\n",
       "74653  0.047748  0.075601  0.876651\n",
       "74654  0.049066  0.096756  0.854177\n",
       "74655  0.071110  0.250985  0.677906\n",
       "74656  0.072942  0.262177  0.664881\n",
       "74657  0.085767  0.291262  0.622971\n",
       "74658  0.070912  0.250066  0.679022\n",
       "\n",
       "[74659 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.stack([tmp[:,range(0,30,3)].mean(1),tmp[:,range(1,30,3)].mean(1),tmp[:,range(2,30,3)].mean(1)]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54393943  0.54340168  0.54486921  0.54463413  0.5455042 ]\n",
      "4024.2\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../output/test_blend_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_xgb, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05068891,  0.47914108,  0.47017003],\n",
       "       [ 0.00355554,  0.03459964,  0.96184484],\n",
       "       [ 0.09190651,  0.43008367,  0.47800982],\n",
       "       ..., \n",
       "       [ 0.04040814,  0.38965375,  0.5699381 ],\n",
       "       [ 0.06617295,  0.5109495 ,  0.42287757],\n",
       "       [ 0.0445355 ,  0.33600247,  0.61946204]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb[:,:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4b823a08d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAFoCAYAAACBqCu+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X90XeV54PuvJMeOAIsYQew0E980NDx0ID9sxyHTTDN3\n4E47/Gonk0xub8zMymIRFkMCcUlJStwJaWaK2zDQkDRQ8GRcLgVPWIG1CIaWtmRlZrISEhwbcEV5\nmoabOCkkOLaMDAiEJd0/9pZ1fJAsnSPZ+5yj72ctL5+z3/1uvfvxPkeP3/2+7+4aHx9HkiTpaOuu\nugGSJGlhMgmRJEmVMAmRJEmVMAmRJEmVMAmRJEmVMAmRJEmVMAmRJEmVMAmRJEmVMAmRJEmVMAmR\nJEmVWNRohYhYA3wOWAPsBz6fmdeVZWcCG4FTgV3Axsy8o6bu5cClwArgMWB9Zm4vy5YANwDnAkuA\nbwCXZObeZk9OkiS1roZ6QiJiGfAXwLcpEolfBz4SEe+LiBXAPcCNwEnAemBTRKwu654PXA1cACwH\ntgJbI6K3PPw1wCrgDOCUsm2b53R2kiSpZXU18gC7iDgH+GpmHlOz7T8Cvwn8NfD/ZOY7asq2AIOZ\neWlE3AtkZv5OWdYF/AT4beAu4OfABZl5X1kewOPA6zPzp3M7TUmS1GqaGRMyXiYQEwaBtwOrge11\n+24H1pav19SWZ+Y48EhZfjJwPLCjpjyB4bKeJEnqMI2OCfkW8ALwnyPiD4BfoBjjsQzop+jZqLUX\nOLF83U+RsExV3g+MT1E+WFNfkiR1kIaSkMzcFxG/CVwPfBQYoBi3MXELpmu6uvNUfljj4+PjXV1z\nOoQkSQvVUf8F2vDsmMz8FvCuifcR8W8pekB2U/Ro1OoHnilfT1e+syzrKt+/UFN+Qk39GXV1dTE0\nNMzo6Nhsq3Scnp5u+vp6F3wcwFhMMA4F4zDJWBSMw6SJWBxtDSUh5TTa/xu4OzOfKzf/GsVtmh3A\nhXVV1gLfKV9voxjfcVt5rG6KcSSbgCcpbr2sAX5clp8OLC7rzdro6BgHDizsiwmMQy1jUTAOBeMw\nyVgUjEN1Gu0JGaGYZvvLEfF7wFnAOuCfA08Bvx8RFwK3l2VnU0y5BbgJ2FLOmHkMuBJ4Ebg/M8ci\n4hZgQ0RsoxiQeg1wV2bunssJSpKk1tTQ7JhyRsu/A/4V8CzF4mLrMvPRMlk4D7gM2AdcV5YNlHUf\nAK4C7gT2UCQp52TmS+XhPw08BDwK/KA8/ofndHaSJKllNbROSBsYHxx8fkF3qy1a1M2yZcey0OMA\nxmKCcSgYh0nGomAcJpWxOOoDU312jCRJqoRJiCRJqoRJiCRJqoRJiCRJqoRJiCRJqoRJiCRJqoRJ\niCRJqoRJiCRJqoRJiCRJqoRJiCRJqoRJiCRJqoRJiCRJqsSiqhug1jIyMsLAwM5Dtp122ltYvHhx\nRS2SJHUqkxAdYmBgJ5+4/m6W9q8EYP+eXXzuCli1ak3FLZMkdRqTEL3C0v6VvGbFm6tuhiSpwzkm\nRJIkVcIkRJIkVcIkRJIkVcIkRJIkVcIkRJIkVcIkRJIkVcIkRJIkVcIkRJIkVaLhxcoi4u3AdcBq\nYBh4EFifmXsi4kxgI3AqsAvYmJl31NS9HLgUWAE8VtbbXpYtAW4AzgWWAN8ALsnMvU2fnSRJalkN\n9YRERA9wH/At4CTgNOC1wI0RsQK4B7ixLFsPbIqI1WXd84GrgQuA5cBWYGtE9JaHvwZYBZwBnFK2\nbfNcTk6SJLWuRm/HvK788+eZeSAzB4G7KZKHdUBm5q2ZOZKZDwJfAy4q614MbM7MbZn5EnAtMA6c\nXyY3FwKfzcynMnMfsAE4r0xuJElSh2k0CflHYAdwcUQcGxGvBd5H0auxBthet/92YG35+pDyzBwH\nHinLTwaOL489UZ4Ut3t8cpokSR2ooSSkTBzeD/wbYAh4GugBPgX0A4N1VfYCJ5avD1feT9ErUl8+\nWFNfkiR1kIYGpkbEYuBe4CsUYziOoxgDcnu5S9cMh5hr+Yx6ehb2hJ+J8282DlPV6+npZtGi9ovr\nXGPRKYxDwThMMhYF4zCpqhg0OjvmLOCNmfmp8v1zEfEZitsqf0HRo1GrH3imfL17mvKdZVlX+f6F\nmvITaurPSl9f78w7LQDNxmGqen19vSxbduxcm1QZr4mCcSgYh0nGomAcqtNoEtIDdEdEd2aOldte\nTXEr5W+AD9Xtvxb4Tvl6G8X4jtsAIqKbYprvJuBJilsva4Afl+WnA4vLerM2NDTM6OjYzDt2qJ6e\nbvr6epuOw9DQ8JTbBgefn4/mHVVzjUWnMA4F4zDJWBSMw6SJWBxtjSYh3wKeA34/Iq4BjqEYD/I/\nKZKLqyPiQorbM2cBZ1NMuQW4CdgSEVso1gi5EngRuD8zxyLiFmBDRGyjGJB6DXBXZu5upIGjo2Mc\nOLCwLyZoPg5TfRDbPabt3v75YhwKxmGSsSgYh+o0OjB1L/DrwLuBn1DcSnkB+GBm/hw4D7gM2Eex\noNm6zBwo6z4AXAXcCeyhSFLOKafrAnwaeAh4FPgB8Czw4bmcnCRJal0Nr5iamTuAM6cp+ybFmiHT\n1b0ZuHmaspcpEpjLGm2TJElqPw4JliRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJ\nkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJ\nlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlVjUyM4R8avA\nXwHjNZu7gVdlZk9EnAlsBE4FdgEbM/OOmvqXA5cCK4DHgPWZub0sWwLcAJwLLAG+AVySmXubOzVJ\nktTKGuoJycz/nZm9mXnMxB/g94GvRMQK4B7gRuAkYD2wKSJWA0TE+cDVwAXAcmArsDUiesvDXwOs\nAs4ATinbtnmuJyhJklrTnG7HRMRK4ArgE8A6IDPz1swcycwHga8BF5W7XwxszsxtmfkScC1Fj8r5\nEdEDXAh8NjOfysx9wAbgvDK5kSRJHWauY0I+C/y3zPwJsAbYXle+HVhbvj6kPDPHgUfK8pOB44Ed\nNeUJDJf1JElSh2loTEitiHgj8F7gl8pN/cCP63bbC5xYUz44TXk/Ra9IfflgTf1Z6elZ2GNtJ86/\n2ThMVa+np5tFi9ovrnONRacwDgXjMMlYFIzDpKpi0HQSAnwEuDszd9ds65qhzlzLZ9TX1zvzTgtA\ns3GYql5fXy/Llh071yZVxmuiYBwKxmGSsSgYh+rMJQl5P8V4kAm7KXo0avUDz8xQvrMs6yrfv1BT\nfkJN/VkZGhpmdHSskSodpaenm76+3qbjMDQ0POW2wcHn56N5R9VcY9EpjEPBOEwyFgXjMGkiFkdb\nU0lIRLwNWAn8dc3mbcCH6nZdC3ynpnwNcFt5jG5gNbAJeJLi1ssayls6EXE6sLisN2ujo2McOLCw\nLyZoPg5TfRDbPabt3v75YhwKxmGSsSgYh+o02xOyCtiTmc/VbLsd+ExEXFi+Pgs4m2LKLcBNwJaI\n2EKxRsiVwIvA/Zk5FhG3ABsiYhvFgNRrgLvqbvdIkqQO0exIlBXAT2s3lMnCecBlwD7gOmBdZg6U\n5Q8AVwF3AnsokpRzyum6AJ8GHgIeBX4APAt8uMn2SZKkFtdUT0hm/iHwh1Ns/yZFL8l09W4Gbp6m\n7GWKBOayZtokSZLai/OSJElSJUxCJElSJUxCJElSJUxCJElSJeayWJk0b0ZGRhgY2HnIttNOewuL\nFy+uqEWSpCPNJEQtYWBgJ5+4/m6W9q8EYP+eXXzuCli1yucXSlKnMgnRvGu2V2Np/0pes+LNR7Jp\nkqQWYhKieWevhiRpNkxCdETYqyFJmomzYyRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJ\nUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiUWNVMpIjYA\nHwGWAt8GPpyZP4qIM4GNwKnALmBjZt5RU+9y4FJgBfAYsD4zt5dlS4AbgHOBJcA3gEsyc29zpyZJ\nklpZwz0hEfER4IPAe4DXAY8Dvx0RK4B7gBuBk4D1wKaIWF3WOx+4GrgAWA5sBbZGRG956GuAVcAZ\nwCll2zY3fWaSJKmlNdMTcgVwRWb+Q/l+PUBEfBzIzLy13P5gRHwNuIii9+NiYHNmbiv3vxb4GHB+\nRNwFXAhckJlPleUbgMcjYkVm/rS505MkSa2qoSQkIn4B+EWgPyIGKHo0vk6RZKwBttdV2Q58oHy9\nBtgyUZCZ4xHxCLAWeAQ4HthRU54RMVzWu6+RdurIGRkZYWBg5yu2n3baW1i8eHEFLZIktatGe0L+\nSfn3+4EzgR7gLmATcAzw47r99wInlq/7gcFpyvuB8SnKB2vqqwUMDOzkE9ffzdL+lQe37d+zi89d\nAatWramwZZKkdtNoEtJV/v1HmfkzgIi4GvgL4K9rymeq32z5jHp6FvaEn4nzbzYOU9Xr6elm0aLJ\n4y7tX8lrVrz5sPsc7hjN/NxmzDUWncI4FIzDJGNRMA6TqopBo0nIxNiMZ2u2/ZAieXgVRY9GrX7g\nmfL17mnKd5ZlXeX7F2rKT6ipPyt9fb0z77QANBuHqer19fWybNmxhz3uTPvUljfzc+fCa6JgHArG\nYZKxKBiH6jSahPwEGALeTjGOA4oxIiPA/cB/qNt/LfCd8vU2ivEdtwFERDewmuJWzpMUt17WUN7S\niYjTgcVlvVkbGhpmdHSskSodpaenm76+3qbjMDQ0POW2wcHnpy2fzT615c383GbMNRadwjgUjMMk\nY1EwDpMmYnG0NZSEZOZoRHwZ2BAR/xvYD/wnisTi/wX+U0RcCNwOnAWcTTHlFuAmYEtEbKFYI+RK\n4EXg/swci4hbyuNuA4YppuzelZm7G2nj6OgYBw4s7IsJmo/DVB/E2mNN90GdaZ+Z2tNMndnymigY\nh4JxmGQsCsahOs1M0b2Koofiu2X9rwIfy8wXIuI84IvAlyhu06zLzAGAzHwgIq4C7qRYR+Rh4JzM\nfKk87qeB44BHKQa83ksx60ZHyFQzXTKfqKg1kqSFpuEkJDNHgMvKP/Vl36RYcGy6ujcDN09T9vJ0\nx9WRMdVMl589+TDL37S2wlZJkhaKppZtV+eon+myf0/9LGtJko4M5yVJkqRKmIRIkqRKmIRIkqRK\nOCZER9zY6IEpZ934vBlJWthMQnTEPb/vab5831Msfei5g9t83owkySRER8VUz5uRJC1sjgmRJEmV\nMAmRJEmVMAmRJEmVcExIB5vq2TDOSJEktQqTkA5W/2yYZmak1E+v9QF3kqT5YhLS4eY6K6V+eq0P\nuJMkzReTEM2oNpHxAXeSpPniwFRJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJ\nklQJkxBJklQJkxBJklQJkxBJklSJhpdtj4gx4CVgHOgq/96UmR+LiDOBjcCpwC5gY2beUVP3cuBS\nYAXwGLA+M7eXZUuAG4BzgSXAN4BLMnNv02cnSZJaVjM9IePAKZl5TGb2ln9/LCJWAPcANwInAeuB\nTRGxGiAizgeuBi4AlgNbga0R0Vse9xpgFXAGcErZts3Nn5okSWplzSQhXeWfeuuAzMxbM3MkMx8E\nvgZcVJZfDGzOzG2Z+RJwLUVCc35E9AAXAp/NzKcycx+wATivTG4kSVKHaXZMyB9FxI8iYjAi/jQi\njgXWANvr9tsOTDz3/ZDyzBwHHinLTwaOB3bUlCcwXNaTJEkdpuExIcC3gb8C/gPwJuArFLdg+oH6\n57zvBU4sX/cDg9OU91P0itSXD9bUn5WenoU91nbi/Ht6uqeMRU9PN4sWdR+y73z93EaPO1Od2vJm\n29RIezqVcSgYh0nGomAcJlUVg4aTkMx8d+3biPhd4F7gfzH1bZpacy2fUV9f78w7LQB9fb1TxqKv\nr5dly449+Ho+f16jx52pTm35XNsm4zDBOEwyFgXjUJ1mekLq/RDoAcYoejRq9QPPlK93T1O+syzr\nKt+/UFN+Qk39WRkaGmZ0dKyRKh2lp6ebvr5ehoaGGRoafkX50NAwg4PPH3w9X5o57kx1asubURsL\nrwnjYBwmGYuCcZg0EYujraEkJCLeDlyQmb9Ts/mfAi8C9wMfqquyFvhO+XobxfiO28pjdQOrgU3A\nkxS3XtZQ3tKJiNOBxWW9WRsdHePAgYV9MUERh6k+VLXxmc8PXTPHnanOfP1bek0UjEPBOEwyFgXj\nUJ1Ge0KeAS6OiGeAzwNvBD4L3Az8OXB1RFwI3A6cBZxNMeUW4CZgS0RsoVgj5ErK5CUzxyLiFmBD\nRGyjGJB6DXBXZu6ew/lJkqQW1dBIlMx8CjgH+E3g58A3KXpAPlkmC+cBlwH7gOuAdZk5UNZ9ALgK\nuBPYQ5GknFNO1wX4NPAQ8CjwA+BZ4MNzOTlJktS6mhmY+k3g3YcpW3WYujdT9JpMVfYyRQJzWaNt\nkiRJ7cd5SZIkqRImIZIkqRLzMUVXC9zY6AEynzj4vva1JEnTMQnRnD2/72m+fN9TLH3oOQB+9uTD\nLH/T2hlqSZIWOpMQzYul/St5zYo3A7B/T/3q/ZIkvZJjQiRJUiVMQiRJUiVMQiRJUiVMQiRJUiVM\nQiRJUiWcHdOmRkZGGBjY+Yrtb3vb24Bjj36DJElqkElImxoY2Mknrr+bpf0rD27bv2cX113ZzfLl\n76mwZZIkzY5JSBurXZtDkqR245gQSZJUCZMQSZJUCZMQSZJUCceEqBI+eVeSZBKiSvjkXUmSSYgq\n45N3JWlhc0yIJEmqhEmIJEmqhEmIJEmqhEmIJEmqRNMDUyPij4GPZWZ3+f5MYCNwKrAL2JiZd9Ts\nfzlwKbACeAxYn5nby7IlwA3AucAS4BvAJZm5t9n2SZKk1tZUT0hEvB3498B4+f51wD3AjcBJwHpg\nU0SsLsvPB64GLgCWA1uBrRHRWx7yGmAVcAZwStmuzc2dkiRJagcNJyER0QXcBFxXs3kdkJl5a2aO\nZOaDwNeAi8ryi4HNmbktM18CrqVIYM6PiB7gQuCzmflUZu4DNgDnRcSKps9MrzCxQNiOHd9jx47v\nuUCYJKlSzdyOuQQYBu4A/ku5bTWwvW6/7cAHytdrgC0TBZk5HhGPAGuBR4DjgR015RkRw2W9+5po\no6bgAmGSpFbSUBISEcuBzwDvqSvqB+pXm9oLnFhTPjhNeT9Fr0h9+WBNfc0TFwiTJLWKRntCrgO+\nXPZU/B91ZV0z1J1r+az09CyMCT/TnWd3d9fB8naPRU9PN4sWNX8OE+ff7nGYK+NQMA6TjEXBOEyq\nKgazTkIi4izgV4APl5tqk4bdFD0atfqBZ2Yo31mWdZXvX6gpP6Gm/qz19fXOvFMHmO48jzvu1QfL\n2z0WfX29LFt27LwcR8ZhgnGYZCwKxqE6jfSErANeC+yKCCgGtXZFxDMUPSQfrNt/LfCd8vU2ivEd\ntwFERDfFOJJNwJMUt17WUN7SiYjTgcVlvYYMDQ0zOjrWaLW2MzQ0POX255578WD5dPu0i6GhYQYH\nn2+6fk9PN319vQvmmpiOcSgYh0nGomAcJk3E4mhrJAn5beD3at6/Afg28LbyOFdFxIXA7cBZwNkU\nU26hmE2zJSK2UKwRciXwInB/Zo5FxC3AhojYRjHo9Rrgrszc3egJjY6OceBA519M031gxsbGD5a3\n+4dqvv4tF8o1MRPjUDAOk4xFwThUZ9ZJSGY+Czw78T4iXgWMZ+bT5fvzgC8CXwJ+CKzLzIGy7gMR\ncRVwJ8U6Ig8D55TTdQE+DRwHPAr0APdSLGwmSZI6VNMrpmbmjygShon336RYcGy6/W8Gbp6m7GXg\nsvKPJElaABwSLEmSKmESIkmSKtH07RipaiMjIwwM7Dxk22mnvYXFixdX1CJJUiNMQtS2BgZ28onr\n72Zp/0oA9u/ZxeeugFWr1lTcMknSbJiEqG3U93xkPnHIMvSSpPZiEqK2Ud/z4QP4JKm9mYSorfgA\nPknqHM6OkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJlTAJkSRJ\nlXDF1A4yNnqAJ574O/r6ehkaGibziaqb1LSx0QOvaH87n48k6ZVMQjrI8/ueZtO9T/E/vrUfaO9n\nqzy/72m+fN9TLH3ouYPb2vl8JEmvZBLSYTrp2Sr1T8ht9/ORJB3KMSGSJKkSJiGSJKkS3o5Rx6gf\nzNrT001fXy8rV/4S3d1e6pLUavxmVseYajDr/j27uO7K9/PWt66qsGWSpKmYhKij1A9mlSS1roaT\nkIh4G3Ad8A5gGPifwOWZ+UxEnAlsBE4FdgEbM/OOmrqXA5cCK4DHgPWZub0sWwLcAJwLLAG+AVyS\nmXubPjtJktSyGhqYGhGLgQeArwMnAacDy4GbImIFcA9wY1m2HtgUEavLuucDVwMXlHW2Alsjorc8\n/DXAKuAM4JSybZvncnKSJKl1NTo75hjgU8AfZubLmbkHuJsiGVkHZGbempkjmfkg8DXgorLuxcDm\nzNyWmS8B1wLjwPkR0QNcCHw2M5/KzH3ABuC8MrmRJEkdpqEkJDP3ZeZ/z8wxgIgI4EPAV4A1wPa6\nKtuBiSUuDynPzHHgkbL8ZOB4YEdNeVLc7lnTSBslSVJ7aGpgakSsBL4P9AC3AJ8B/gKoX9JyL3Bi\n+bofGJymvJ+iV6S+fLCm/qz09HTm0icjIyP87d/uPPj++9/PClvTXrq7u1i0qDOvi9mY+Ex06mdj\ntozDJGNRMA6TqopBU0lIZu4ClkTEyRRJyG1lUdcMVedaPqO+vt6Zd2pDDz/8OB+/9qss7V8J+ByV\nRhx33KtZtuzYqptRuU79bDTKOEwyFgXjUJ05TdHNzB9ExAbgW8B9FD0atfqBZ8rXu6cp31mWdZXv\nX6gpP6Gm/qwMDQ0zOjrWSJW2MDQ03FHPhTmannvuRQYHn6+6GZWZWLStUz8bs2UcJhmLgnGYNBGL\no62hJCQi/iVwU2aeWrN5vPzzXeD9dVXWAt8pX2+jGN9xW3msbmA1sAl4kuLWyxrKWzoRcTqwuKw3\na6OjYxw40HkX00L/gMzF2Nh4R14TjerUz0ajjMMkY1EwDtVptCfke0BfRPwRxTiQ4yim3f4v4Cbg\n4xFxIXA7cBZwNsWUW8ryLRGxhWKNkCuBF4H7M3MsIm4BNkTENooBqdcAd2Xm7jmcnyRJalGNzo4Z\nAv4V8E6KWyg7gX3ABzPz58B5wGXltuuAdZk5UNZ9ALgKuBPYQ5GknFNO1wX4NPAQ8CjwA+BZ4MNz\nOTlJktS6Gh4TUiYV/3Kasm9SLDg2Xd2bgZunKXuZIoG5rNE2SZKk9uO8JEmSVAmTEEmSVAmTEEmS\nVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmTEEmSVAmT\nEEmSVAmTEEmSVAmTEEmSVIlFVTdAajUjIyMMDOw8ZNtpp72FxYsXV9QiSepMJiFSnYGBnXzi+rtZ\n2r8SgP17dvG5K2DVqjUVt0ySOotJiDSFpf0rec2KN1fdDEnqaI4JkSRJlbAnRAvaVOM/Mp84Isd1\nXIkkHcokRAta/fgPgJ89+TDL37R2Xo/ruBJJeiWTEC149eM/9u/58RE5riTpUI4JkSRJlWi4JyQi\nVgKfB94DvAz8JfCxzByKiDOBjcCpwC5gY2beUVP3cuBSYAXwGLA+M7eXZUuAG4BzgSXAN4BLMnNv\n02cnSZJaVjM9IfcCe4E3AGuA04D/GhErgHuAG4GTgPXApohYDRAR5wNXAxcAy4GtwNaI6C2Pew2w\nCjgDOKVs2+bmTkuSJLW6hpKQiDgeeBi4KjOHM/Mp4FaKXpF1QGbmrZk5kpkPAl8DLiqrXwxszsxt\nmfkScC0wDpwfET3AhcBnM/OpzNwHbADOK5MbSZLUYRpKQjLz2cy8KDN312x+A/CPFL0i2+uqbAcm\nphkcUp6Z48AjZfnJwPHAjpryBIbLepIkqcPMaXZMRLwD+CjwG8AngfppBXuBE8vX/cDgNOX9FL0i\n9eWDNfVnpaenM8fadup5HQ3d3V0sWjR1/GYb156e7mmPMdvjNnqM+TLRloV+DRmHScaiYBwmVRWD\nppOQiHg3xe2WT2bm1yPik0DXDNXmWj6jvr7emXdqQ516XkfDcce9mmXLjp2ybLZx7evrnfYYsz1u\no8eYb15DBeMwyVgUjEN1mkpCykGmtwEfyczby827KXo0avUDz8xQvrMs6yrfv1BTfkJN/VkZGhpm\ndHSskSptYWhouOomtK3nnnuRwcHnpyybbVyHhoanPcZsj9voMeZLT083fX29HfvZmC3jMMlYFIzD\npIlYHG3NTNH9FeDPgPeVg08nbAM+VLf7WuA7NeVrKJIXIqIbWA1sAp6kuPWyhvKWTkScDiwu683a\n6OgYBw503sW00D8gczE2Nn7wmqhfTn22S7TXXldTLckOhy7LPtW/V9XXZtU/v1UYh0nGomAcqtNQ\nElLOYtlEcQvmwbri24HPRMSF5euzgLMpptwC3ARsiYgtFGuEXAm8CNyfmWMRcQuwISK2UQxIvQa4\nq24QrDQn9cupN7NE+1RLvbssuyQ1rtGekH9GsRDZFyLiixSDSbvKvwM4D/gi8CXgh8C6zBwAyMwH\nIuIq4E6KdUQeBs4pp+sCfBo4DngU6KFYj+TSps9MmkbtcurNLtHukuySNHcNJSGZ+U2KBGE6P6ZY\ncGy6+jcDN09T9jJwWflHkiR1OOclSZKkSpiESJKkSpiESJKkSpiESJKkSpiESJKkSpiESJKkSpiE\nSJKkSpiESJKkSjT9FF2pHYyNHuCJJ/7u4LNcZvusGEnSkWcSoo72/L6n2XTvUyzt3w8096wYSdKR\nYRKijjcfz4qRJM0/x4RIkqRKmIRIkqRKeDtGmsHY6IFDBrQ6uFWS5odJSIsaGRlhYGDnwff+4qvO\n8/ue5st6h0+QAAAPXklEQVT3PcXSh54D2mtw68jICI89NkBfXy9DQ8MHZwmddtpbWLx4ccWtk7TQ\nmYS0qIGBnXzi+rtZ2r8SaK9ffJ2oXQe31l9HAPv37OJzV8CqVWsqbJkkmYS0tHb9xafWUnsdSVIr\nMQmRJOkoqb/VPmGh3iI1CWkBU12UjgHpLPWDW2HhfulIC5m3SA9lEtICprooHQPSWeoHt87mS2eq\n5NTERWp/3iKdZBLSIuovSseAdJ5Gv3jqk9OF/L8lSZ3JJERqYf6PSVInc8VUSZJUCZMQSZJUiYZv\nx0TErwO3Al/PzA/WlZ0JbAROBXYBGzPzjpryy4FLgRXAY8D6zNxeli0BbgDOBZYA3wAuycy9jZ+W\nJElqdQ31hETElcDngb+fomwFcA9wI3ASsB7YFBGry/LzgauBC4DlwFZga0T0loe4BlgFnAGcUrZt\nc+OnJB19E1Nwd+z4Hjt2fM8p1pI0C432hAwD7wS+QNFbUWsdkJl5a/n+wYj4GnARRe/HxcDmzNwG\nEBHXAh8Dzo+Iu4ALgQsy86myfAPweESsyMyfNn5q0tHTzs+XkaSqNJSEZOafAETEVMVrgO1127YD\nH6gp31JzrPGIeARYCzwCHA/sqCnPiBgu693XSDulKnTKMvuu6CjpaJnPKbr9QP03717gxJrywWnK\n+4HxKcoHa+rPSk9P+421bcc2a+56erpZtGj6f/uprouZ6szmGDMd57HHBqZc0fG6K7tZvbp91yiZ\niIWfN2MxoYo4NPOZPBqquhbme52QriNcPqO+vt6Zd2ox7dhmzV1fXy/Llh178P3IyAiPPvrowfc/\n+cn/N2Od2fyM2fzs+rKp1idp9Ge3Kj9vk4xF4WjGoZnPZCebzyRkN0WPRq1+4JkZyneWZV3l+xdq\nyk+oqT8rQ0PDjI6ONVKlckNDw1U3QRUYGhpmcPD5g++3b/8eH7/2qwd7IOrHlYyNHuC7393+iuvl\n9NOnv00y3bVV/7PnWqcd9PR009fX25bfEfPNWBSqiEOrfr4mYnG0zWcSsg34UN22tcB3asrXALcB\nREQ3sBrYBDxJcetlDeUtnYg4HVhc1pu10dExDhxorw/VQv4SWMjqr9XR0bHDjit5ft/TbLr3KZZ+\na//BbcVS7mPTLuU+3bV1uM9JM3XaSaecx3wwFoWjGYdO/3w1aj6TkNuBz0TEheXrs4CzKabcAtwE\nbImILRRrhFwJvAjcn5ljEXELsCEitlHMwrkGuCszd89jG6W25jLukjpJQ0lIOVtlHHhV+f69wHhm\nHpOZuyPiPOCLwJeAHwLrMnMAIDMfiIirgDsp1hF5GDgnM18qD/9p4DjgUaAHuJdiaq8kSepAjU7R\nPewNo8z8JsWCY9OV3wzcPE3Zy8Bl5R+po00sblbLBc4kLTQ+RVeqQP3iZuACZ5IWHpMQqSL14zva\neYEzSWqGSYjUxqa6rePKppLahUmI1Mbqb+sUU3aZdsquJLUSkxCpzbXqtN2pnkFjL42kWiYhUgep\nvz1T5YybgYGdhzyDxl4aSfVMQqQOUn97puoZN63aSyOpNZiESB3mcEu/wyt7S15++WUAXvWqVwGu\nVyLp6DEJkRaYqXpLjjl++bQPzpOkI8UkRFqA6ntLlva/4bC9J5J0JJiEVKB+1oDd35KkhcgkpAL1\nswbs/pYkLUQmIRWZafCgpPkx0fPY09NNX18vQ0PDnHrqaa5XIrUAkxBJHW3q9UrGXK9EagEmIZI6\nnuuVSK2pu+oGSJKkhcmeEEkty+fPzI/6OPb0dPOrv/quClskFUxCjgKn5KrTTZUszMd17vNn5sdU\ncdzU18sv/dI/rbhlWuhMQo4Cp+Sq09Vf4zDzdT5V4gKv7OlwPMf8MI5qRSYhR4lTctXOZvO8mfpf\ncjNd51MlLp3e0+HtJelQJiGSZnSknjdTn7jUJzsz3dKZbW9Kq/D2knQokxBJs3I0njczVbJzuOTm\naPWmzGey420RaZJJiKSjYra9HI3eupxrb8pstNKto3a+pdNKPVcjIyM89tjAwVV0R0fH2iaOnaSl\nkpCIWAncCLwL2A98JTN/t9pWSZoPjfZyHM2fM5tf7K3Sg9HOt3RaKZlr5zh2kpZKQoC7gYeB3wKW\nA/dHxE8z8/PVNkvSfDhaA7Qb/Tnt9gupVRKiZrRS2+fallbq2WlXLZOERMQ7gLcCZ2bmc8BzEXE9\n8DGgbZKQI7VegqT5udVSf4yJ4zT6C2mmGUP175ttb6topdtArdKWVurZaVctk4QAq4EfZuZQzbbt\nQETEsZn5fEXtOqypFiL78n2PN7RegqTZmY9bOvXHmOo4s0l2ZjNjqPb9bH7OVInLVNOha02VVM1U\nZyoz/WKfTW/RfCQHsznGTG2Z6hjNxGQ2Wqlnpx21UhLSDwzWbdtb/n0iMGMSMjw8zMjIS4yOjgHw\n+OMDdHV1zWsj6z3xxN9x/Z/9Jcf0vRaAvU8nJ6186yv2279n18HXLzz7U2B82vez2Wc+6lT1c63T\nXm1ttTrHHL/8kDq1n61mj1F/nN0/2sHnfzDCMX07gKk/19MdZyYz/ZxXH7vs4PfJVNvq21J/jNnU\n2b9nF48//jjPPfciY2NFbOq/y14YeoYrPvSvOfXUXwbg+9/PV5zL97+f9PRMPoJsNseo//fav2cX\n3//+0oPHmekYs2lL/TFmG5PadszGbM5ntnV6et7JokXVPc6tkfOeT13j4+Mz73UURMRVwHsz8501\n204G/h54U2b+qLLGSZKkeddKT9HdTdEbUquf4r8wu49+cyRJ0pHUSknINmBlRJxQs+2dwOOZ+UJF\nbZIkSUdIy9yOAYiIbwF/C3wceD1wH3BtZv5ppQ2TJEnzrpV6QgDeT5F8/BT4OvBnJiCSJHWmluoJ\nkSRJC0er9YRIkqQFwiREkiRVwiREkiRVwiREkiRVwiREkiRVwiREkiRVopUeYNeUiFgJ3Ai8C9gP\nfCUzf7faVs2P8tw+D7wHeBn4S+BjmTkUEWcCG4FTgV3Axsy8o6bu5cClwArgMWB9Zm4vy5YANwDn\nAkuAbwCXZObEAwNbVkT8MUUMusv3Cy4OEbEB+AiwFPg28OHM/NFCi0VEvB24juIJ3MPAgxTntKeT\nYxERvw7cCnw9Mz9YV3bEzrsVv2tniMW/oIjFacDPgf+emX9QU94xsThcHGr26QIeBoYy88ya7ZXG\noRN6Qu4Gfgy8Efi/gPdGxPpKWzR/7qV4kvAbgDUUH6b/GhErgHso/vFPAtYDmyJiNUBEnA9cDVwA\nLAe2Alsjorc87jXAKuAM4BSK62DzUTqnppW/dP495SNRI+J1LLA4RMRHgA9SJKavAx4HfnuhXRMR\n0UOxovK3KM73NOC1wI2dHIuIuJLiPyZ/P0XZkT7vlvqunSEWb6A4v83ACcBvAb8TER8syzsmFoeL\nQ52PAifX1a08Dm2dhETEO4C3Ap/MzOcy8wfA9cDF1bZs7iLieIqs9arMHM7Mpygy3fcA64DMzFsz\ncyQzHwS+BlxUVr8Y2JyZ2zLzJeBail/c55df3hcCn83MpzJzH7ABOK/8EmtJZRZ/E8X/fCcsuDgA\nVwCfysx/KK/59Zm5noUXi9eVf/48Mw9k5iDFF+IqOjsWwxTP1PrBFGVH7Lxb9Lv2cLFYDmzKzE2Z\nOZqZDwN/Q/H9CZ0Vi8PFATj4H7YNwBfqiiqPQ1snIRTdsD/MzKGabduBiIhjK2rTvMjMZzPzosys\nfYLwG4B/pOgV2V5XZTuwtnx9SHlmjgOPlOUnA8cDO2rKk+JCXjPPpzGfLqFo4x0121azgOIQEb8A\n/CLQHxEDEfHziLgzIk5k4V0T/0jR3osj4tiIeC3wPor/yXVsLDLzTzJz/zTFR/K8W+679nCxKH+p\nXlG3+Q3AT8rXHROLGa6JCX9M8Z+4J+u2Vx6Hdk9C+oHBum0T921PPMptOaLKrPOjwB8w/XlPnPPh\nyvspMt368kFaNGYRsRz4DPAf64oWVByAf1L+/X7gTIr/hbwB2MQCi0X5Zfl+4N8AQ8DTQA/wKRZY\nLGocyfNu6+/aiLgMeBMw8SyyBROLcrzIaorxMfUqj0O7JyEAXVU34EiLiHcDD1B0e3293DzTec+1\nvJVcB3y5zMLrLaQ4TLT1jzLzZ+UtuquB36D4slgwsYiIxRRjpr5C8b+11wPPAreXuyyYWNQ5kufd\nljGJiI8Cvw/8Rmb+vKao42NRDiz9E+CjmTkyzW6VxqHdk5DdFNlYrYnsbfcrd28/5cCh+4DLM/NL\n5ebpzvuZWZTvprhw6stPqKnfMiLiLOBXgP9cbqq96BdMHEo/Lf9+tmbbDynO41UsrFicBbwxMz9V\n3o/+KUVv2XuBAyysWEw4kp+HtvyujYj/Avwu8H9m5kM1RQslFr8HbM/Mvyrf1ycNlceh3ZOQbcDK\niDihZts7gccz84WK2jRvIuJXgD8D3peZt9cUbeOV96fXAt+Zqjwiuim64x6iuCc4WFd+OrC4rNdq\n1lHMetgVEbuB7wFdEfEMsBN4R93+nRoHKO5nDwFvr9n2i8AIcD8LKxY9QHd5HhNeTfEF+DcsrFhM\nOJLfC233XRsRV1DMinlXZj5WV7xQYrEO+LWI2F1+f34B+OcR8UxEvJ4WiEPX+Ph4syfXEiLiW8Df\nAh+n6JK9D7g2M//0sBVbXDky+THgjzPzv9WVnQR8n2KmxO0U/yu8EzgjMwfKe4BbgLPLY1xJMco5\nMvOliNhIOZ2KYpDRZuCFzPyto3JyDShnCdUOcnoDxdoYr6dY52YnCyAOEyLiOorbL/+aYl7+3cDf\nUYyFWBDXBED5xfcEcDPFNMJjgC8DfcAHgH+gg2MREZuBJVmzJsSR/l5o1e/aaWLxJooBlu/KzMen\nqNNxsZgmDq/l0PXAPgD8O4rxVD8Ffo2K49D2i5VRBHMTRUCfBW6q+kMxT/4ZxYJDX4iILzJ5z38c\nCOA84IvAlyi65Ndl5gBAZj4QEVdRfAGdRDHV95xyChbAp4HjgEcp/kd5L8ViNS0nM5+l5vZDRLwK\nGM/Mp8v3CyIONa6i+J/Idyk+v1+lWLzthYUUi8zcW/4iuY6ih+glJhdS+nmnxiIihim+A15Vvn8v\nxefhmMzcfYTPu6W+aw8XC4q1dI4BtkXERJUuitkcv9xJsZjhmnimbt9B4KWJ70+g8ji0fU+IJElq\nT+0+JkSSJLUpkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJkxBJklQJ\nkxBJklQJkxBJklSJ/x9jw0BMk1TJdAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b828fb190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# full_data=pd.concat([train_df,test_df])\n",
    "\n",
    "# SSL = preprocessing.StandardScaler()\n",
    "# for col in features_to_use:\n",
    "#     full_data[col], lam = boxcox(full_data[col] - full_data[col].min() + 1)\n",
    "#     full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1)) \n",
    "#     train_df[col] = full_data.iloc[:ntrain][col]\n",
    "#     test_df[col] = full_data.iloc[ntrain][col]\n",
    "\n",
    "# del full_data\n",
    "full_data=pd.concat([train_df,test_df])\n",
    "SSL = preprocessing.StandardScaler()\n",
    "full_data['sc_price'].hist(bins =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 15.7866149428\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+UXWV97/H3ZMJgxAzCgIlVUovVL70RNRmjXGttF3Rd\nr0q66tJ2ucC2LqpcCqIBi0rTinp7oYWCWhXEXIpcCixZhbWQH9V7i7WrLotNDD9yw+XbW7k0aPgR\n84NJwkDMzNw/9p705DiTmTHn7HPOnvdrrayZs5+z93yfmck5n3n2s5/dNzExgSRJUrst6HQBkiRp\nfjB0SJKkShg6JElSJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIqYeiQJEmVMHRIkqRKLJzr\nDhHxNuAG4FuZeUZT268ClwHLgR8Df5WZ/62h/cPAucBS4CFgTWZuLNuOBD4PvBM4Evg2cE5m7ijb\nlwFXA6cAu4GvZeYn5lq/JEnqjDmNdETERcDngH+Zou0E4C7geuBY4L3AH0bEGWX7auAS4H3AkvK5\nd0XEovIQlwIrgDcBry5ru77hS9wOPA68Avh14F0RsWYu9UuSpM6Z6+mVUeCNwA+maFsCrMvMdZk5\nlpnrgb8D3lq2nw1cn5kbMvN54ApgAlgdEf3AWcBnMnNrZu4C1gKnR8TSiHgD8Frg45m5JzN/AFxV\nHlOSJPWAOYWOzPxiZu6epm1DZl7YtPkE4Ifl58PAxobnTwAPAKuAVwJHA/c3tCdFyBkGVgKPZeZI\nw7E3AhERR82lD5IkqTPaNpE0Is4HTgS+XG4aAnY2PW0HcFzZNjFF+86G9qn2pWyXJEldbs4TSWcj\nIj4EfBp4R2b+uKGpb4ZdD9U+076HNDExMdHXd1iHkCRpvmrJG2jLQ0dE/CnwfuDXMvOhhqZtFCMW\njYaATWVbX/n42Yb2Y4GngSOm2Xei3HdGfX19jIyMMjY2PruO9Kj+/gUMDi6qfV/tZ73Yz3qxn/Uy\n2c9WaGnoiIgLKa5aOSUzf9jUvIFifsaN5XMXUMzVWAc8SnH6ZJjiChUi4jXAQLnfE8CyiDh28hJa\nigmtD2dmY0g5pLGxcfbvr+8vRqP50lf7WS/2s17sp5q1LHRExInAp5g6cABcA9wSEbdQrNFxEfAc\ncE9mjkfEV4C1EbGBYgLppcBtmbkN2BYR64E/i4iPAi8DLqC4AkaSJPWAOYWOiBilOKVxRPn4XcBE\nZr4QOAN4IbAhIiZ36aO46uSXMvObEXExcCtwPLCeYs7H8+VzPwm8CHgQ6AfupFhIbNJ7KEZFngSe\nAa7JzC8jSZJ6Qt/ExESna6jKxM6de2s/BLZw4QKOOeYo6t5X+1kv9rNe7Ge9lP1syURS770iSZIq\nYeiQJEmVMHRIkqRKGDokSVIlDB2SJKkShg5JklQJQ4ckSaqEoUOSJFXC0CFJkiph6JAkSZUwdEiS\npEoYOiRJUiUMHZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKmHokCRJlTB0SJKkShg6JElSJQwd\nkiSpEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIqsbDTBUiSBLBv3z42b970U9uXLz+ZgYGBDlSkVjN0\nSJK6wubNm/jYVbezeGjZgW27t2/h8gthxYrhDlamVjF0SJK6xuKhZbx46as6XYbaxDkdkiSpEoYO\nSZJUCUOHJEmqhKFDkiRVwtAhSZIqYeiQJEmVMHRIkqRKGDokSVIlDB2SJKkShg5JklQJl0GXJB2W\n5hu19fcv4Fd+5ZQOVqRuZeiQJB2W5hu17d6+hXWDi/jFX/wPHa5M3cbQIUk6bN6oTbMx59AREW8D\nbgC+lZlnNLWdClwGnARsAS7LzJsb2j8MnAssBR4C1mTmxrLtSODzwDuBI4FvA+dk5o6yfRlwNXAK\nsBv4WmZ+Yq71S5KkzpjTRNKIuAj4HPAvU7QtBe6gCAbHA2uAdRGxsmxfDVwCvA9YAtwF3BURi8pD\nXAqsAN4EvLqs7fqGL3E78DjwCuDXgXdFxJq51C9JkjpnrlevjAJvBH4wRduZQGbmDZm5LzPvBb4O\nfKBsPxu4PjM3ZObzwBXABLA6IvqBs4DPZObWzNwFrAVOj4ilEfEG4LXAxzNzT2b+ALiqPKYkSeoB\ncwodmfnFzNw9TfMwsLFp20Zg1VTtmTkBPFC2vxI4Gri/oT0pQs4wsBJ4LDNHmo4dEXHUXPogSZI6\no5UTSYcoTn802gEc19C+c5r2IYpRj+b2nQ3tU+1L2b53NgX299d/WZLJPta9r/azXuxnb5uuP3Pt\n56GOs3Bh933P6vrzbNbK/rX66pW+NrbPtO+MBgcXzfykmpgvfbWf9WI/e9N0/ZlrPw91nGOO6d5B\n7br9PNuplaFjG8WIRKMh4OkZ2jeVbX3l42cb2o8t9z9imn0nyn1nZWRklLGx8dk+vSf19y9gcHBR\n7ftqP+vFfva2kZHRabfPpZ+HOs7OnbMa0K5UXX+ezSb72QqtDB0bgPc3bVsFfK+hfRi4ESAiFlDM\n1VgHPEpx+mSY8hRNRLwGGCj3ewJYFhHHTl5CSzGh9eHMbAwphzQ2Ns7+/fX9xWg0X/pqP+vFfvam\n6d5w59rPVh2nat1eXzdpZei4CfhURJxVfn4a8HaKS2ABrgFuiYhbKNbouAh4DrgnM8cj4ivA2ojY\nQDGB9FLgtszcBmyLiPXAn0XER4GXARdQXAEjSZJ6wFzX6RiNiGcp1tr4rYbHlOHgdOB8YBdwJXBm\nZm4u278JXAzcCmynCCXvKC+fBfgkcB/wIMUluc8AH2z48u+hCBtPAt8CvpqZX55zjyVJUkfMaaQj\nMw95Uiczv0OxwNd07dcC107T9hOKwHL+NO1bKVYrlSR1sfGx/Tz88MM/Nddh+fKTGRgY6GBl6jTv\nvSJJaqm9u57gs7dsZfHQv8/z3719C5dfCCtWDHewMnWaoUOS1HLeAE5TqfeKJpIkqWsYOiRJUiUM\nHZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKmHokCRJlTB0SJKkShg6JElSJQwdkiSpEoYOSZJU\nCUOHJEmqhKFDkiRVwtAhSZIqYeiQJEmVMHRIkqRKGDokSVIlFna6AElS/Y2P7SfzkYO2LV9+MgMD\nAx2qSJ1g6JAktd3eXU9w3d1bWXzfHgB2b9/C5RfCihXDHa5MVTJ0SJIqsXhoGS9e+qpOl6EOck6H\nJEmqhKFDkiRVwtAhSZIqYeiQJEmVMHRIkqRKGDokSVIlDB2SJKkShg5JklQJQ4ckSaqEK5JKkio3\n1b1Ymh+rfgwdkqTKNd+LBeCpR9ez5MRVHaxK7WbokCR1RPO9WHZvf7yD1agKzumQJEmVMHRIkqRK\nGDokSVIlDB2SJKkShg5JklSJll69EhGvB64EVgKjwL3AmszcHhGnApcBJwFbgMsy8+aGfT8MnAss\nBR4q99tYth0JfB54J3Ak8G3gnMzc0cr6JUlS+7RspCMi+oG7ge8CxwPLgZcAV0fEUuAO4OqybQ2w\nLiJWlvuuBi4B3gcsAe4C7oqIReXhLwVWAG8CXl3WfX2rapckSe3XytMrLy3//XVm7s/MncDtFGHh\nTCAz84bM3JeZ9wJfBz5Q7ns2cH1mbsjM54ErgAlgdRlmzgI+k5lbM3MXsBY4vQwzkiSpB7QydPwI\nuB84OyKOioiXAO+mGLUYBjY2PX8jMLn03EHtmTkBPFC2vxI4ujz2ZHtSnL4ZbmH9kiSpjVoWOsqg\n8B7gN4ER4AmgH/gjYAjY2bTLDuC48vNDtQ9RjHo0t+9s2F+SJHW5lk0kjYgB4E7gaxRzMF5EMYfj\npvIpfTMc4nDbZ9TfX/+LdSb7WPe+2s96sZ+9rd396e9fwMKF3fc9q+vPs1kr+9fKq1dOA16RmX9U\nPt4TEZ+iOE3ytxQjFo2GgKfLz7dN076pbOsrHz/b0H5sw/6zMji4aOYn1cR86av9rBf72Zva3Z/B\nwUUcc8xRbf0ah6NuP892amXo6AcWRMSCzBwvt72A4tTI3wHvb3r+KuB75ecbKOZn3AgQEQsoLrtd\nBzxKcSplGHi8bH8NMFDuN2sjI6OMjY3P/MQe1t+/gMHBRbXvq/2sF/vZ20ZGRtt+/J0797b1a/ws\n6vrzbDbZz1ZoZej4LrAH+HREXAq8kGI+xz9QhIlLIuIsitMtpwFvp7gEFuAa4JaIuIVijY6LgOeA\nezJzPCK+AqyNiA0UE0gvBW7LzG1zKXBsbJz9++v7i9FovvTVftaL/exN7X7D7fbvV7fX101aFjoy\nc0dEvI1icbAfAs/z74t4/TgiTge+AHwJeAw4MzM3l/t+MyIuBm6lWMdjPfCO8vJZgE9SzBF5kGJE\n5U6KhcQkSRXat28fmzdvOmhb5iMdqka9pqUrkmbm/cCp07R9h2LNjun2vRa4dpq2nwDnl/8kSR2y\nefMmPnbV7SweWnZg21OPrmfJiasOsZdUaGnokCTV3+KhZbx46asOPN69/fEOVqNeUu/rfCRJUtcw\ndEiSpEoYOiRJUiUMHZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKmHokCRJlTB0SJKkShg6JElS\nJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIqYeiQJEmVMHRIkqRKLOx0AZIkTWd8bD+Zjxy0\nbfnykxkYGOhQRTochg5JUtfau+sJrrt7K4vv2wPAM9v+Hx9c/QgRJx30PINIbzB0SJK62uKhZbx4\n6asA2L39ca67++EDIaTYtoXLL4QVK4Y7VaJmydAhSeopjSFEvcWJpJIkqRKGDkmSVAlDhyRJqoSh\nQ5IkVcLQIUmSKmHokCRJlTB0SJKkShg6JElSJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIq\nYeiQJEmVMHRIkqRKGDokSVIlDB2SJKkShg5JklSJhe04aESsBc4DFgP/BHwwM/8tIk4FLgNOArYA\nl2XmzQ37fRg4F1gKPASsycyNZduRwOeBdwJHAt8GzsnMHe3ogyRJaq2Wj3RExHnAGcBbgZcCDwMX\nRMRS4A7gauB4YA2wLiJWlvutBi4B3gcsAe4C7oqIReWhLwVWAG8CXl3Wfn2r65ckSe3RjpGOC4EL\nM/Nfy8drACLio0Bm5g3l9nsj4uvAByhGN84Grs/MDeXzrwA+AqyOiNuAs4D3ZebWsn0t8HBELM3M\nJ9vQD0mS1EItHemIiJ8DfgEYiojNEfHjiLg1Io4DhoGNTbtsBFaVnx/UnpkTwANl+yuBo4H7G9oT\nGC33kyRJXa7VIx0vLz++BzgV6AduA9YBLwQeb3r+DuC48vMhYOc07UPAxBTtOxv2n1F/f/3nzU72\nse59tZ/1Yj97R7fW3t+/gIULq62tDj/P2Whl/1odOvrKj3+emU8BRMQlwN8C/6uhfab9f9b2Qxoc\nXDTzk2pivvTVftaL/ex+3Vr74OAijjnmqI59bc1Oq0PH5NyKZxq2PUYRFo6gGLFoNAQ8XX6+bZr2\nTWVbX/n42Yb2Yxv2n9HIyChjY+OzfXpP6u9fwODgotr31X7Wi/3sHSMjo50uYUojI6Ps3Lm30q9Z\nh5/nbEz2sxVaHTp+CIwAr6eYjwHFHI99wD3A7zY9fxXwvfLzDRTzM24EiIgFwEqKUzOPUpxKGaY8\nRRMRrwEGyv1mZWxsnP376/uL0Wi+9NV+1ov97H7d+ubaye9pL/88q9bS0JGZYxFxHbA2Iv4R2A38\nCUWQ+B/An0TEWcBNwGnA2ykugQW4BrglIm6hWKPjIuA54J7MHI+Ir5TH3UAxgfRS4LbM3NbKPkiS\npPZox+yXi4FvAP8M/F8ggY+U4eB04HxgF3AlcGZmbgbIzG+W+94KbKcIJe/IzOfL434SuA94EPgB\nxSmcD7ahfkmS1AYtX6cjM/dRBIvzp2j7DsUCX9Ptey1w7TRtP5nuuJIkqfvV+zofSZLUNQwdkiSp\nEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIq0Y5b20uSamDfvn1s3rzpoG2Zj3SoGtWBoUOSNKXNmzfx\nsatuZ/HQsgPbnnp0PUtOXNXBqtTLDB2SpGktHlrGi5e+6sDj3dsf72A16nXO6ZAkSZUwdEiSpEoY\nOiRJUiUMHZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKmHokCRJlXBFUkkS8NP3WvE+K2o1Q4ck\nCfjpe614nxW1mqFDknRA471WvM+KWs05HZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKmHokCRJ\nlfCSWUlSrTQvcjZp+fKTGRgY6EBFmmTokCTVSvMiZwC7t2/h8gthxYrhDlYmQ4ckzUNTjQbUadnz\nxkXO1D0MHZI0D001GuCy52o3Q4ckzVPNowEue6528+oVSZJUCUc6JEk9bXxs/0HzUeo0N6VuDB2S\npJ62d9cTXHf3Vhbftwdwbko3M3RIknpe4/wU56Z0L+d0SJKkShg6JElSJQwdkiSpEoYOSZJUCUOH\nJEmqRNuuXomIzwIfycwF5eNTgcuAk4AtwGWZeXPD8z8MnAssBR4C1mTmxrLtSODzwDuBI4FvA+dk\n5o521S9JklqrLSMdEfF64HeAifLxS4E7gKuB44E1wLqIWFm2rwYuAd4HLAHuAu6KiEXlIS8FVgBv\nAl5d1n19O2qXJEnt0fLQERF9wDXAlQ2bzwQyM2/IzH2ZeS/wdeADZfvZwPWZuSEznweuoAgsqyOi\nHzgL+Exmbs3MXcBa4PSIWNrq+iVJUnu0Y6TjHGAUuLlh20pgY9PzNgKTS8YNN7Zn5gTwQNn+SuBo\n4P6G9iy/xnCLa5ckSW3S0jkdEbEE+BTw1qamIaB5ibgdwHEN7TunaR+iGPVobt/ZsP+s9PfXf97s\nZB/r3lf7WS/2s3rdUEPV+vsXsHBh6/rdTT/Pdmpl/1o9kfRK4LrMzIj4+aa2vhn2Pdz2GQ0OLpr5\nSTUxX/pqP+vFfs6vGqo2OLiIY445CoB9+/bx4IMP/tRzXve61zEwMDDn42p2WhY6IuI04M3AB8tN\njSFhG8WIRaMh4OkZ2jeVbX3l42cb2o9t2H9WRkZGGRsbn8suPae/fwGDg4tq31f7WS/2s3ojI6Md\n/fqdMDIyys6dewHYuPH7fPSKv2Hx0LID7bu3b+HKi0ZZuXJ2Z+676efZTpP9bIVWjnScCbwE2BIR\nUMwX6YuIpylGQM5oev4q4Hvl5xso5mfcCBARCyjmgawDHqU4lTJMeYomIl4DDJT7zdrY2Dj799f3\nF6PRfOmr/awX+1ltDfNN4/d9bGz8oJvETfWcn+W4OrRWho4LgD9ueHwC8E/A68qvc3FEnAXcBJwG\nvJ3iElgorna5JSJuoVij4yLgOeCezByPiK8AayNiA8UE0kuB2zJzWwvrlyRJbdSy0JGZzwDPTD6O\niCOAicx8onx8OvAF4EvAY8CZmbm53PebEXExcCvFOh7rgXeUl88CfBJ4EfAg0A/cSbGQmCRJ6hFt\nW5E0M/+NIiBMPv4OxQJf0z3/WuDaadp+Apxf/pMkST2o3tf5SJKkrtG2kQ5JkrrF+Nh+Mh858Ljx\nc1XH0CFJqr29u57guru3svi+PQA89eh6lpy4aoa91GqGDknSvNB4iezu7c2LZKsKzumQJEmVMHRI\nkqRKGDokSVIlDB2SJKkShg5JklQJQ4ckSaqEoUOSJFXC0CFJkiph6JAkSZUwdEiSpEoYOiRJUiUM\nHZIkqRKGDkmSVAlDhyRJqoShQ5IkVcLQIUmSKrGw0wVIktSt9u3bx+bNmw7atnz5yQwMDHSoot5m\n6JAkaRqbN2/iY1fdzuKhZQDs3r6Fyy+EFSuGO1xZbzJ0SJJ0CIuHlvHipa/qdBm14JwOSZJUCUc6\nJEkCxsf2k/nIQduaH+vwGDokSQL27nqC6+7eyuL79hzY9tSj61ly4qoOVlUvhg5JkkrN8zd2b3+8\ng9XUj3M6JElSJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRVwqtXJGkeaL6HiOtPqBMMHZI0DzTfQ8T1\nJ9QJhg5Jmica16Bw/Ql1gnM6JElSJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRVouVXr0TEMuBzwFuB\nnwDfAD6SmSMRcSpwGXASsAW4LDNvbtj3w8C5wFLgIWBNZm4s244EPg+8EzgS+DZwTmbuaHUfJElS\n67VjpONOYAdwAjAMLAf+IiKWAncAVwPHA2uAdRGxEiAiVgOXAO8DlgB3AXdFxKLyuJcCK4A3Aa8u\na7++DfVLkqQ2aGnoiIijgfXAxZk5mplbgRsoRj3OBDIzb8jMfZl5L/B14APl7mcD12fmhsx8HrgC\nmABWR0Q/cBbwmczcmpm7gLXA6WWYkSRJXa6lp1cy8xn+PURMOgH4EcWox8amto3Ab5efDwO3NBxr\nIiIeAFYBDwBHA/c3tGdEjJb73d3CbkhST2te8hxc9lzdoa0rkkbEG4APAb8BfBxoXgJvB3Bc+fkQ\nsHOa9iGKUY/m9p0N+0uS+Oklz8Flz9Ud2hY6IuKXKU6ffDwzvxURHwf6ZtjtcNsPqb+//hfrTPax\n7n21n/ViP1v/dRqXPAeXPW+l/v4FLFy4YN793rZCW0JHOSn0RuC8zLyp3LyNYsSi0RDw9Aztm8q2\nvvLxsw3txzbsP6PBwUUzP6km5ktf7We92M/eOP58Nzi4iGOOOeqgx5qddlwy+2bgq8C7y8mikzYA\n7296+irgew3twxRhhYhYAKwE1gGPUpxKGaY8RRMRrwEGyv1mZWRklLGx8Tn1p9f09y9gcHBR7ftq\nP+vFfrbWyMho246t4vu7c+feefd72wotDR3lVSbrKE6p3NvUfBPwqYg4q/z8NODtFJfAAlwD3BIR\nt1Cs0XER8BxwT2aOR8RXgLURsQEYpbiE9rbM3Dbb+sbGxtm/v76/GI3mS1/tZ73Yz9YdX+3T/POb\nL7+3rdDqkY7/SLHw119GxBcoJn/2lR8DOB34AvAl4DHgzMzcDJCZ34yIi4FbKdbxWA+8o7x8FuCT\nwIuAB4F+ivVAzm1x/ZIkqU1afcnsdygCwXQep1jga7r9rwWunabtJ8D55T9JktRj2nrJrCRJdTI+\ntv/AmieNczpOOmk5AwMDHa6u+xk6JEmapb27nuC6u7ey+L49B7bt3r6Fyy8cZ8WK4Q5W1hsMHZIk\nzUHzGiiavXqvaCJJkrqGoUOSJFXC0yuSJB2Gxsmlk5YvP9mJpVMwdEiSdBiaJ5cWE0txYukUDB2S\nJB2mXpxcum/fPjZv3nTQtnaP0Bg6JEmahzZv3sTHrrqdxUPLgGpGaAwdkiTNU1WP0Hj1iiRJqoSh\nQ5IkVcLQIUmSKmHokCRJlTB0SJKkSnj1iiT1sKnWWmheHVPVmmqFUnCVUjB0SFJPa15rAeCpR9ez\n5MRVHaxqfmteoRRcpXSSoUOSelzzWgu7tz/ewWoEvblCaRWc0yFJkiph6JAkSZUwdEiSpEoYOiRJ\nUiUMHZIkqRKGDkmSVAlDhyRJqoTrdEiS1AFTrSZb91VLDR2SJHVA82qy82HVUkOHJEltNtX9WDIf\nmXcrlxo6JKmHNA/Je3O33jDV/Vjm4z1yDB2S1EOah+Tn4xtXr/IeOYYOSeo5jW9e8/GNS73LS2Yl\nSVIlDB2SJKkSnl6RJKlL1W0tD0OHJEldqm5reRg6JEnqYo0Th6da7wN6Z/TD0CFJUheYbgGxRlOt\n99FLox+GDkmSusBsFxDr5VVMDR2SJHWJdi0gNtWE1E6sZmvokKQu1S1vFOo9Uy2Xf93dDx+YkAqd\nWc3W0CFJXar5ygVw2XPNznTL5Xd6GfaeCh0RsQy4GjgF2A18LTM/0dmqJKk1pvrr1Pt1aCazuYNt\nt/ze9FToAG4H1gPvBZYA90TEk5n5uc6WJUmHz5u56WfRS3ew7ZnQERFvAF4LnJqZe4A9EXEV8BHA\n0CGpFrrxr1N1v14ZEeuZ0AGsBB7LzJGGbRuBiIijMnNvh+qSpBnt27ePhx7azODgIkZGRnnuuecB\nOOKIIw48x0miqrteCh1DwM6mbTvKj8cBM4aO/v6D72+3ceP3W1JYN1mwoI8XvegF7NnzHOPjE50u\np23sZ73Mh34+8sj/4aqvfoMXDr4EgB1PJC846pgDjye3Hb/stQceP/vMk8DB34/mbbN5zs+6X52P\n3emv343H3r19C/39b2ThwoPfK5vfOw9HL4UOgL7D2XdwcNFBG0477a2HV40kzdJpp72V8877L50u\nQ+qoXrq1/TaK0Y5GQxQxbVv15UiSpLnopdCxAVgWEcc2bHsj8HBmPtuhmiRJ0iz1TUz0zvnTiPgu\n8L+BjwIvA+4GrsjML3e0MEmSNKNeGukAeA9F2HgS+BbwVQOHJEm9oadGOiRJUu/qtZEOSZLUowwd\nkiSpEoYOSZJUCUOHJEmqhKFDkiRVwtAhSZIq0Wv3XpmziFgGXA2cAuwGvpaZn+hsVa1X9vNzwFuB\nnwDfAD7SdFfeWomIz1L0sbbhOSLWAucBi4F/Aj6Ymf/W2apaKyJeD1xJcSfpUeBe4ILM/HFHCztM\nEfE24AbgW5l5RlPbqcBlwEnAFuCyzLy5+ioP3wz9/FWKfi4Hfgz8VWb+t+qrbI1D9bXhOX3AemAk\nM0+tsr5WmeFnuhj4IvCbwH7gb4APZ+bzszl2bV+sG9wOPA68Avh14F0RsaajFbXHnRR33T0BGKb4\nT/4XHa2ojco3qt+h+baJNRIR5wFnUATJlwIPAxd0tKgWi4h+ipWFvwscT/F7+xLgS52s63BFxEUU\nfwT8yxRtS4E7KP4YOh5YA6yLiJWVFtkCM/TzBOAu4HrgWOC9wB9GxJRv1t3uUH1t8iHgle2vqD1m\n0c+/Al4A/Dxwcvnx3bM9fq1HOiLiDcBrgVMzcw+wJyKuAj5C8U2thYg4miJZX5yZo8BoRNwAnN/Z\nytqj/EviGoq/jv+0w+W004XAhZn5r+XjOobll5b//joz9wM7I+J2ilsd9LJRintD/SVwZFPbmUBm\n5g3l43sj4uvAB4BzqyuxJQ7VzyXAusxcVz5eHxF/RxGie3FU51B9BSAiXgqsLZ/zK9WV1lLT9rMc\nUV8NnJAQr+oBAAAEPElEQVSZu4BdwH+ey8FrHToohmsfazrFsBGIiDgqM/d2qK6WysxnKF6wGi0D\nftSBcqpwDsV/jJupaeiIiJ8DfgEYiojNFC/gfw/8Qa+fdmjyI+B+4OyI+CRwFMVfTXd2tKrDlJlf\nBIiIqZqHKV6HGm0EfrvNZbXcofqZmRsobtTZ6ATgofZX1noz/EwnfZbiD6LH6NHQMUM/30JxOvB3\nI+JCYBz4a2BtZo7P5vh1P70yBOxs2raj/HhcxbVUphzh+RA1fEOOiCXAp4A/6HAp7fby8uN7gFMp\nRuxeDnylYxW1QWZOUPTxN4ER4AmgH/ijTtbVZtO9LtX2NQkgIs4HTgRqeb+sch7ESoo5LHX18oZ/\nr6L4A+H3Kd5vZqXuoQOgr9MFVCkifhn4JvCxzPz7TtfTBlcC12VmdrqQNpv8vf3zzHwqM7cClwC/\nEREDHayrpcq+3Al8DTia4oaOI/Tm8PtczLfXpQ8BnwZ+IzO3dbqeVouIIykmV34oM/d1up426qP4\no+CizHw2M/8Z+O/MYZSu7qdXtlH8VdFoiGLyYR1/8VcDNwLnZeZNna6n1SLiNODNwAfLTXV+4X6y\n/PhMw7bHKPr8EuCHVRfUJqcBr8jMyZGNPRFxCfBARLy4PG9cN9O9Lj3dgVraLiL+FHg/8GuZ2ZOn\nVmbhj4GNmfk/y8d1fW16Ehgt519Neow5hI66j3RsAJZFxLEN294IPJyZz3aopraIiDcDXwXeXcfA\nUTqT4g13S0RsA74P9EXE0xHRc+fDZ/BDir/4X9+w7RcoLofe2pGK2qMfWBARja9FL6DGVyVRvC4N\nN21bBXyvA7W0VXne/73AKTUOHFC8Nv2niNhWvjb9JfCW8rXpZR2urZUeBhZHxCsatr0CmPVl/LUe\n6cjMByJiPfBnEfFRiqHbC4ArOltZa5WXHa4DPp6Z93a6nja6gOIvikknUKxd8Tp++hx5T8vMsYi4\nDlgbEf9IscbMnwA3znbCVo/4LrAH+HREXAq8kGI+xz/UdJQD4CbgUxFxVvn5acDbgTd1tKoWi4gT\nKeZfnZKZdRmZm84pHPx++tvAb1HMV3pyyj16UGauj4jvA5+LiN+j+EPo9ymutJuVvomJOv9BceAq\ngHXAr1EMVV+Tmf+1o0W1WES8BfgH4HmKYb2Jho+RmY93sLy2iYifBx7NzP5O19IO5XyHKynW6lhI\nsQjP+TUcpVtB0c/XUfwOf5viUuGefbGOiFGK/39HlJv2AxOZ+cKy/S3AFygWB3sM+ERm3tGBUg/L\nofoZEX9METoa5zj0UVxR+EuVFtoCM/1Mm577e8Dv9eLiYLP43X0ZcC3Fe+pu4C8y88rZHr/2oUOS\nJHWHus/pkCRJXcLQIUmSKmHokCRJlTB0SJKkShg6JElSJQwdkiSpEoYOSZJUCUOHJEmqhKFDkiRV\nwtAhSZIqYeiQJEmV+P/viPZSgoveNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b828cff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data['tmpbox'] = 0\n",
    "full_data['tmpbox'] = boxcox(full_data['sc_price'],lmbda=0.1)\n",
    "fig = plt.hist(full_data.tmpbox,bins =100)\n",
    "print full_data.tmpbox.min(), full_data.tmpbox.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAFoCAYAAAD6jOlyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+UXeV52PvvzAhhARoMA0Zug+KY2g+5wrElWcY3btwW\nepcvGCXNiv/oNfQuLy5Qig3IONjBig1xWykxhdrXMRQUSigFLbMCa/EzoQ1epPFyIFKGH8oQnrim\nWDjCRpZGjAQDg2amf+w9ytFhRjPnzOw558x8P2tp6Zz97nfPux/tc/TMu9/33V3j4+NIkiTNte5W\nN0CSJC1MJhmSJKkSJhmSJKkSJhmSJKkSJhmSJKkSJhmSJKkSJhmSJKkSJhmSJKkSJhmSJKkSJhmS\nJKkSSxqtEBEfAm4A1gDDwGPAhszcExFnAZuB04GdwObMvLum7hXAZcAK4NmyXn9ZdjTwTeCTwNHA\n48Clmbm36bOTJEkt01BPRkT0AA8D3wdOBlYB7wJuiogVwP3ATWXZBmBLRKwp664HrgUuAE4BHgIe\niohl5eE3AauBM4H3l227fTYnJ0mSWqerkQekRcTPUfRQ/GJmZrntXwNfAG4B/p/M/HDN/luBwcy8\nLCIeBDIzf7Ms6wJ+DHweuBf4GXBBZj5clgfwHPAPM/Mnsz5TSZI0rxodk/F3wFPAJRFxbES8C/gN\nil6JtUB/3f79wLry9WHlmTkOPF2WnwYcXx57ojwpbsesbbCNkiSpDTSUZJSJwaeAfwEMAS8DPcCX\ngT5gsK7KXuCk8vWRyvuA8UnKB2vqS5KkDtLQwM+IWAo8CHyHYgzFcRRjMO4qd+ma5hCzLT+i8fHx\n8a6uWR1CkqTFas7/A210dsnZwHsy88vl+wMRcR3FbY8/puiRqNUHvFK+3j1F+Y6yrKt8/3pN+Yk1\n9afV1dXF0NAwo6NjM62yqPX0dNPbu8yYNci4Nc6YNce4Nc6YNWcibnOt0SSjB+iOiO7MnPjXewfF\nrY4/BT5Tt/864Mny9XaK8RV3AkREN8U02C3ACxS3RtYCL5XlZwBLy3ozNjo6xsGDXliNMGbNMW6N\nM2bNMW6NM2btodEk4/vAAeB3ImITcAzFeIw/o0gero2ICylun5wNnEMxJRXgZmBrOePkWeBq4A3g\nkcwci4hbgY0RsZ1iwOcm4N7M3D2bE5QkSa3R6MDPvcAngI9RTD/dQXF749OZ+TPgPOByYB/Fgl3n\nZ+ZAWfdR4BrgHmAPRRJybma+WR7+q8ATwDPAD4FXgYtnc3KSJKl1GlonowOMDw6+ZhfZDC1Z0s0J\nJxyLMWuMcWucMWuOcWucMWtOGbc5H/jps0skSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIk\nSVIlTDIkSVIlGl3xU1KHGhkZYWBgx2HbVq36AEuXLm1RiyQtdCYZ0iIxMLCDL954H8v7VgKwf89O\nvn4VrF69tsUtk7RQmWRIi8jyvpW8c8X7Wt0MSYuEYzIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIk\nSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIl\nTDIkSVIlTDIkSVIlTDIkSVIlTDIkSVIlljSyc0T8CvDfgPGazd3AUZnZExFnAZuB04GdwObMvLum\n/hXAZcAK4FlgQ2b2l2VHA98EPgkcDTwOXJqZe5s7NUlzbWRkhIGBHW/bvmrVB1i6dGkLWiSpnTWU\nZGTmnwPLardFxDXAByJiBXA/8DlgK/ArwAMR8Xxm9kfEeuBa4BPADuBK4KGIOC0zh4FNwGrgTOB1\n4A+A24Ffm8X5SZpDAwM7+OKN97G8b+Whbfv37OTrV8Hq1Wtb2DJJ7aihJKNeRKwErqJIDs4HMjPv\nKIsfi4gHgIsoei8uAW7PzO1l3espEo31EXEvcCFwQWbuKss3As9FxIrM/Mls2ilp7izvW8k7V7yv\n1c2Q1AFmOybja8AfZOaPgbVAf115P7CufH1YeWaOA0+X5acBxwNP1ZQnMFzWkyRJHabpnoyIeA/w\n68A/Kjf1AS/V7bYXOKmmfHCK8j6KcR715YM19Wekp8exrDM1EStj1phOjdtk7e3p6WbJkpmfx1Tn\nPN1xOjVmrWbcGmfMmlNVvGZzu+SzwH2ZubtmW9c0dWZbPq3e3mXT76TDGLPmdFrcJmtvb+8yTjjh\n2Fkdo5HjdFrM2oVxa5wxaw+zSTI+RTEeY8Juih6JWn3AK9OU7yjLusr3r9eUn1hTf0aGhoYZHR1r\npMqi1dPTTW/vMmPWoE6N29DQ8KTbBgdfm9UxZnKcTo1Zqxm3xhmz5kzEba41lWRExAeBlcB/r9m8\nHfhM3a7rgCdrytcCd5bH6AbWAFuAFyhujaylvOUSEWcAS8t6MzY6OsbBg15YjTBmzem0uE32hdvo\nOUz1pT3T43RazNqFcWucMWsPzfZkrAb2ZOaBmm13AddFxIXl67OBcyimpALcDGyNiK0Ua2RcDbwB\nPJKZYxFxK7AxIrZTDPjcBNxbdztGkiR1iGZHeqwADptWWiYD5wGXA/uAG4DzM3OgLH8UuAa4B9hD\nkYScm5lvlof4KvAE8AzwQ+BV4OIm2ydJklqsqZ6MzPxd4Hcn2f49il6OqerdAtwyRdlbFAnK5c20\nSZIktRfn+EiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiS\npEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqY\nZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEqYZEiS\npEosaaZSRGwEPgssB/4CuDgzfxQRZwGbgdOBncDmzLy7pt4VwGXACuBZYENm9pdlRwPfBD4JHA08\nDlyamXubOzVJktRKDfdkRMRngU8DHwfeDTwHfD4iVgD3AzcBJwMbgC0Rsaastx64FrgAOAV4CHgo\nIpaVh94ErAbOBN5ftu32ps9MkiS1VDM9GVcBV2Xm/yzfbwCIiC8AmZl3lNsfi4gHgIsoei8uAW7P\nzO3l/tcDVwLrI+Je4ELggszcVZZvBJ6LiBWZ+ZPmTk+SJLVKQz0ZEfEPgF8A+iJiICJ+FhH3RMRJ\nwFqgv65KP7CufH1YeWaOA0+X5acBxwNP1ZQnMFzWkyRJHabRnoyfK//+FHAW0APcC2wBjgFeqtt/\nL3BS+boPGJyivA8Yn6R8sKb+jPT0OJZ1piZiZcwa06lxm6y9PT3dLFky8/OY6pynO06nxqzVjFvj\njFlzqopXo0lGV/n372XmTwEi4lrgj4H/XlM+Xf1my6fV27ts+p10GGPWnE6L22Tt7e1dxgknHDur\nYzRynE6LWbswbo0zZu2h0SRjYmzEqzXbXqRIDo6i6JGo1Qe8Ur7ePUX5jrKsq3z/ek35iTX1Z2Ro\naJjR0bFGqixaPT3d9PYuM2YN6tS4DQ0NT7ptcPC1WR1jJsfp1Ji1mnFrnDFrzkTc5lqjScaPgSHg\nQxTjKaAYozECPAL8v3X7rwOeLF9vpxhfcSdARHQDayhutbxAcWtkLeUtl4g4A1ha1pux0dExDh70\nwmqEMWtOp8Vtsi/c2nMYGRlhYGDH2/ZZteoDLF26dMpj1B9nujZ0UszahXFrnDFrDw0lGZk5GhG3\nARsj4s+B/cBXKBKH/wJ8JSIuBO4CzgbOoZiSCnAzsDUitlKskXE18AbwSGaORcSt5XG3Uwz43ATc\nm5m7Z3uSkqY3MLCDL954H8v7Vh7atn/PTr5+Faxe7fhrSY1rZgrrNRQ9DH9Z1v8j4MrMfD0izgO+\nBXyb4jbK+Zk5AJCZj0bENcA9FOtobAPOzcw3y+N+FTgOeIZiQOmDFFNfJc2T5X0reeeK97W6GZIW\niIaTjMwcAS4v/9SXfY9iQa2p6t4C3DJF2VtTHVeSJHUe5/hIkqRKmGRIkqRKNPWANEmtNdlMkNpZ\nIJLUDkwypA5UPxPEWSCS2pFJhtShnAkiqd05JkOSJFXCJEOSJFXCJEOSJFXCJEOSJFXCJEOSJFXC\nJEOSJFXCKazSAjA2epDM5w/b5uJcklrNJENaAF7b9zK3PbyL5U8cAFycS1J7MMmQFggX55LUbhyT\nIUmSKmGSIUmSKmGSIUmSKmGSIUmSKmGSIUmSKmGSIUmSKmGSIUmSKuE6GdICNNkKoPXvJalqJhnS\nAlS/AijAT1/YxinvXTfvbRkZGeHZZwfo7V3G0NAwo6NjgMueS4uBSYa0QNWvALp/z0stacfAwA6+\neON9LO9bWdMWlz2XFgOTDEmVc8lzaXFy4KckSaqESYYkSapEw7dLImIMeBMYB7rKv7dk5pURcRaw\nGTgd2Alszsy7a+peAVwGrACeBTZkZn9ZdjTwTeCTwNHA48Clmbm36bOTJEkt00xPxjjw/sw8JjOX\nlX9fGRErgPuBm4CTgQ3AlohYAxAR64FrgQuAU4CHgIciYll53E3AauBM4P1l225v/tQkSVIrNZNk\ndJV/6p0PZGbekZkjmfkY8ABwUVl+CXB7Zm7PzDeB6ykSlvUR0QNcCHwtM3dl5j5gI3BembxIkqQO\n0+yYjN+LiB9FxGBE/KeIOBZYC/TX7dcPTEzMP6w8M8eBp8vy04DjgadqyhMYLutJkqQO00yS8RfA\nfwP+EfDR8s9NQB8wWLfvXuCk8vWRyvsoejXqywdr6kuSpA7S8MDPzPxY7duI+C3gQeB/MPltlFqz\nLZ9WT48TZmZqIlbGrDHtELe5+tk9Pd0sWXLk82l0n8nKGq2jQjtca53GmDWnqnjNxWJcLwI9wBhF\nj0StPuCV8vXuKcp3lGVd5fvXa8pPrKk/I729y6bfSYcxZs1pZdzm6mf39i7jhBOOPeIxG91nZGSE\nZ5555lDZj3/8v6atoyPzM9o4Y9YeGkoyIuJDwAWZ+Zs1m/8P4A3gEeAzdVXWAU+Wr7dTjK+4szxW\nN7AG2AK8QHFrZC3wUll+BrC0rDdjtc9G0JH19HS/7XkSml47xG1oaHjOjjM4+NoRj9noPv39f8UX\nrv+jQ8uIT/XMlNo6mlw7XGudxpg1ZyJuc63RnoxXgEsi4hXgG8B7gK8BtwD/Fbg2Ii4E7gLOBs6h\nmJIKcDOwNSK2UqyRcTVlcpKZYxFxK7AxIrZTDPjcBNybmbsbaeDo6BgHD3phNcKYNaeVcZurL8/a\nc5jqmM3sU7uM+FTPTPG6mzlj1Thj1h4augmTmbuAc4FfA34GfI+iB+NLZTJwHnA5sA+4ATg/MwfK\nuo8C1wD3AHsokpBzy+msAF8FngCeAX4IvApcPJuTkyRJrdPMwM/vAR87QtnqI9S9haLXY7KytygS\nlMsbbZMkSWo/Dr+VJEmVMMmQJEmVMMmQJEmVMMmQJEmVMMmQJEmVMMmQJEmVMMmQJEmVMMmQJEmV\nmIsHpEnqQGOjB8l8/tD72teSNBdMMqRF6rV9L3Pbw7tY/sQBYOoHmUlSs0wypEVsJg8yk6RmOSZD\nkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRV\nwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVwiRDkiRVYkmzFSPiPwJXZmZ3+f4s\nYDNwOrAT2JyZd9fsfwVwGbACeBbYkJn9ZdnRwDeBTwJHA48Dl2bm3mbbJ2l+jI0eJPP5Q+9rX0ta\n3JpKMiLiQ8C/AsbL9+8G7gc+B2wFfgV4ICKez8z+iFgPXAt8AtgBXAk8FBGnZeYwsAlYDZwJvA78\nAXA78GuzODdJ8+C1fS9z28O7WP7EAQB++sI2Tnnvuha3SlI7aPh2SUR0ATcDN9RsPh/IzLwjM0cy\n8zHgAeCisvwS4PbM3J6ZbwLXUyQo6yOiB7gQ+Fpm7srMfcBG4LyIWNH0mUmaN8v7VvLOFe/jnSve\nxzHH+7GVVGhmTMalwDBwd822NUB/3X79wMSvM2tryzNzHHi6LD8NOB54qqY8y5+xton2SZKkNtDQ\n7ZKIOAW4Dvh4XVEf8FLdtr3ASTXlg1OU91H0atSXD9bUn7GeHseyztRErIxZY9ohbvP5s3t6ulmy\nZO7Pufa4mlw7XGudxpg1p6p4NTom4wbgtszMiPj5urKuaerOtnxGenuXzcVhFhVj1pxWxm0+f3Zv\n7zJOOOHYOf+5tcfVkfkZbZwxaw8zTjIi4mzgl4GLy021ScFuih6JWn3AK9OU7yjLusr3r9eUn1hT\nf8aGhoYZHR1rtNqi1NPTTW/vMmPWoHaI29DQ8Lz+rMHB1+b859YeV5Nrh2ut0xiz5kzEba410pNx\nPvAuYGdEQDGeoysiXqHo4fh03f7rgCfL19spxlfcCRAR3RTjOLYAL1DcGllLecslIs4Alpb1GjI6\nOsbBg15YjTBmzZmvuI2MjDAwsOOwbfM5TbT2POfyS9vrbuaMVeOMWXtoJMn4PPDbNe9PBf4C+GB5\nnGsi4kLgLuBs4ByKKalQzEbZGhFbKdbIuBp4A3gkM8ci4lZgY0RspxjwuQm4NzN3N31m0gIxMLCD\nL954H8v7Vh7a5jRRSZ1gxklGZr4KvDrxPiKOAsYz8+Xy/XnAt4BvAy8C52fmQFn30Yi4BrgHOBnY\nBpxbTmcF+CpwHPAM0AM8SLFwlyT+forohP176sdZV8OFtiTNRtMrfmbmjygSgon336NYUGuq/W8B\nbpmi7C3g8vKPpDbhQluSZqPpJEPS4lDbizJfPSiSFgYnEkuSpEqYZEiSpEqYZEiSpEqYZEiSpEqY\nZEiSpEqYZEiSpEqYZEiSpEqYZEiSpEq4GJfUZuofiOZS3pI6lUmG1GbqH4i2GJfynuzJswCrVn2A\npUuXtqBFkpphkiG1ocW+lPdkT57dv2cnX78KVq9e28KWSWqESYaktlT/5FlJnceBn5IkqRImGZIk\nqRImGZIkqRKOyZAqNNksCWdISFosTDKkCtXPknCGhKTFxCRDqljtLImx0YNvW1zLng1JC5VJhjSP\nXtv3Mrc9vIvlTxwA7NmQtLCZZEjzzPUfJC0Wzi6RJEmVMMmQJEmVMMmQJEmVMMmQJEmVMMmQJEmV\nMMmQJEmVMMmQJEmVaHidjIj4IHAD8GFgGPgz4IrMfCUizgI2A6cDO4HNmXl3Td0rgMuAFcCzwIbM\n7C/Ljga+CXwSOBp4HLg0M/c2fXaSJKllGurJiIilwKPAd4GTgTOAU4CbI2IFcD9wU1m2AdgSEWvK\nuuuBa4ELyjoPAQ9FxLLy8JuA1cCZwPvLtt0+m5OTJEmt0+jtkmOALwO/m5lvZeYe4D6KZON8IDPz\njswcyczHgAeAi8q6lwC3Z+b2zHwTuB4YB9ZHRA9wIfC1zNyVmfuAjcB5ZfIiSZI6TENJRmbuy8z/\nnJljABERwGeA7wBrgf66Kv3AuvL1YeWZOQ48XZafBhwPPFVTnhS3Y3yogyRJHaipZ5dExErgB0AP\ncCtwHfDHwEt1u+4FTipf9wGDU5T3UfRq1JcP1tSfkZ4ex7LO1ESsjFljGonbTPdZsqT7sPeLQf15\n15c1Wmch8jPaOGPWnKri1VSSkZk7gaMj4jSKJOPOsqhrmqqzLZ9Wb++y6XfSYYxZc2YSt5nuc8IJ\nxzZUZyGoP+/6skbrLGSL5ZqYS8asPczqKayZ+cOI2Ah8H3iYokeiVh/wSvl69xTlO8qyrvL96zXl\nJ9bUn5GhoWFGR8caqbJo9fR009u7zJg1qJG4DQ0NH7F8bPQgf/mX/Yft9/zzfzMn7Wx3Q0PDDA6+\nNmVZo3UWIj+jjTNmzZmI21xrKMmIiH8G3JyZp9dsHi///CXwqboq64Any9fbKcZX3FkeqxtYA2wB\nXqC4NbKW8pZLRJwBLC3rzdjo6BgHD3phNcKYNWcmcZvuS+61fS+z5cFdLP/+/kPbfvrCNk5577oj\n1Op8Y6MHee655w6Lz6pVH2Dp0qXA1HFbrNfqYj3v2TBm7aHRnoy/Anoj4vcoxmEcRzEt9X8ANwNf\niIgLgbuAs4FzKKakUpZvjYitFGtkXA28ATySmWMRcSuwMSK2Uwz43ATcm5m7Z3F+Uttb3reSd654\n36H3+/fUD21aeF7b9zK3PbyL5U8cAGD/np18/SpYvXr+x3mPjIwwMLDjsG21CY+k5jWUZGTmUET8\nX8DvU9ziOECxZsb/l5k/i4jzgG8B3wZeBM7PzIGy7qMRcQ1wD8U6GtuAc8vprABfpUhanqEYUPog\nxcJdkhag+uSqVQYGdvDFG+9jed9KoLUJj7TQNDwmo0wa/tkUZd+jWFBrqrq3ALdMUfYWcHn5R5Lm\nTbskPNJC4xwfSZJUCZMMSZJUCZMMSZJUCZMMSZJUiVktxiVJc2Fs9CCZzx96X/taUucyyZDUcvXr\nZiyGBcmkxcAkQ1JbqJ1GuhgWJJMWA8dkSJKkSphkSJKkSphkSJKkSphkSJKkSphkSJKkSphkSJKk\nSphkSJKkSphkSJKkSrgYl6QFY2RkhIGBHYdtW7XqAyxdurRFLZIWN5MMSQvGwMAOvnjjfSzvWwnA\n/j07+fpVsHr12ha3TFqcTDIkLSi1y5NLai3HZEiSpErYkyGpI002/sJHxEvtxSRDmiP+pze/6sdf\ngI+Il9qNSYY0R/xPb/7Vj7/wEfFSezHJkOaQ/+lJ0t9z4KckSaqESYYkSaqEt0skdYSx0YOHDaSd\nyaDa+jrgCqDSfDLJkNQRXtv3Mrc9vIvlTxwAZjaotr6OK4BK88skQ1LHqB1YO9NBta4AKrVOw0lG\nRKwEvgF8HHgL+BPgyswcioizgM3A6cBOYHNm3l1T9wrgMmAF8CywITP7y7KjgW8CnwSOBh4HLs3M\nvU2fnSRJaplmBn4+COwFTgXWAquA/xARK4D7gZuAk4ENwJaIWAMQEeuBa4ELgFOAh4CHImJZedxN\nwGrgTOD9Zdtub+60JElSqzWUZETE8cA24JrMHM7MXcAdFL0a5wOZmXdk5khmPgY8AFxUVr8EuD0z\nt2fmm8D1wDiwPiJ6gAuBr2XmrszcB2wEziuTF0mS1GEaSjIy89XMvCgzd9dsPhX4O4pejf66Kv3A\nxMisw8ozcxx4uiw/DTgeeKqmPIHhsp4kSeowsxr4GREfBj4H/CrwJaB+JNZe4KTydR8wOEV5H0Wv\nRn35YE39GenpcemPmZqIlTFrzFRxM46doaenmyVLug97P90+reJntHHGrDlVxavpJCMiPkZxO+RL\nmfndiPgS0DVNtdmWT6u3d9n0O+kwxqw59XEzjp2ht3cZJ5xw7GHvp9un1by2GmfM2kNTSUY5iPNO\n4LOZeVe5eTdFj0StPuCVacp3lGVd5fvXa8pPrKk/I0NDw4yOjjVSZdHq6emmt3eZMWvQVHEbGhpu\nYas0U0NDwwwOvnbY++n2aRU/o40zZs2ZiNtca2YK6y8Dfwj8Rjm4c8J24DN1u68DnqwpX0uRnBAR\n3cAaYAvwAsWtkbWUt1wi4gxgaVlvxkZHxzh40AurEcasOfVx8wutM8zk363dPhPt1p5OYMzaQ0NJ\nRjkLZAvFLZLH6orvAq6LiAvL12cD51BMSQW4GdgaEVsp1si4GngDeCQzxyLiVmBjRGynGPC5Cbi3\nbpCpJEnqEI32ZPyfFAtt/f8R8S2KwZpd5d8BnAd8C/g28CJwfmYOAGTmoxFxDXAPxToa24Bzy+ms\nAF8FjgOeAXoo1uO4rOkzkyRJLdVQkpGZ36NIAKbyEsWCWlPVvwW4ZYqyt4DLyz+S1LZGRkYYGNhx\n2DYfvCa9nc8ukaQGDQzs4Is33sfyvpWAD16TpmKSIUlNaPTBa/Z+aDEyyZCkeWDvhxYjkwxJmic+\ndl6LjeuuSpKkSphkSJKkSphkSJKkSphkSJKkSphkSJKkSji7RGrSyMgIzzzzzKH3mc+3sDWS1H5M\nMqQm/fVfH77uwU9f2MYp713X4lZJUvswyZBmoXbdg/17XmpxaySpvTgmQ5IkVcKeDEmLxtjowbeN\nnXEsjVQdkwxJi8Zr+17mtod3sfyJA4e2OZZGqo5JhqRFpf75IY6lkarjmAxJklQJkwxJklQJkwxJ\nklQJx2RIUo3JZqAArFr1AZYuXdqCFmkxGhkZYWBgx9u2d9p1aJIhSTUmm4Gyf89Ovn4VrF69toUt\n02IyMHD4isLQmdehSYYk1amfgSK1wkK4Dh2TIUmSKmGSIUmSKmGSIUmSKmGSIUmSKmGSIUmSKtHw\n7JKI+ARwB/DdzPx0XdlZwGbgdGAnsDkz764pvwK4DFgBPAtsyMz+suxo4JvAJ4GjgceBSzNzb+On\nJUnzx7U1pMk1lGRExNXAhcDfTlK2Argf+BywFfgV4IGIeD4z+yNiPXAt8AlgB3Al8FBEnJaZw8Am\nYDVwJvA68AfA7cCvNXlukjQvXFtDmlyjt0uGgY8AP5yk7HwgM/OOzBzJzMeAB4CLyvJLgNszc3tm\nvglcD4wD6yOihyJ5+Vpm7srMfcBG4LwyeZGktjaxpsHEn9pFlKTFqqEkIzN/PzP3T1G8Fuiv29YP\nrJusPDPHgafL8tOA44GnasqTIqnx1wBJkjrQXK742Qe8VLdtL3BSTfngFOV9FL0a9eWDNfWleTPZ\ncwO8v7541Y+5mGz8haS3m+tlxbsqLp9WT48TZmZqIlbG7O2efXbgsOcG7N+zkxuu7mbNmrWH4tXd\nPevLVR2ifszFT1/YxinvXTdNreKztWTJ1J+z2vLJjI4eZNu2bRw48AZjY+OHtp9xhgnvVBbK99pU\n7Z/umpnrnzdbc5lk7KbokajVB7wyTfmOsqyrfP96TfmJNfVnpLd3WSO7C2M2md7eZW97bkBv7zJO\nOOHYQ++PO+4drWiaWqT2eti/p77TdnK118xkn7P6a6retm3buPgrd77tIVlb/u0y1q2bPslZzDr9\ne22q9k93zbSbuUwytgOfqdu2DniypnwtcCdARHQDa4AtwAsUt0bWUt5yiYgzgKVlvRkbGhpmdHSs\nqRNYbHot//1PAAAK9ElEQVR6uuntXWbMJjE0NDzptsHB1w7F7cCBN1rQMnWSiWtm4vWRyidz4MAb\nkz4ka7p6i9lC+V6b7HqZ2F7Fv/1E3ObaXCYZdwHXRcSF5euzgXMopqQC3AxsjYitFGtkXA28ATyS\nmWMRcSuwMSK2Uwz43ATcm5m7G2nE6OgYBw927oXVCsbs7Sb7cqqPU233tTSZ2mtmJtdUvamuMT+z\n0+v0GE2VIHXaeTW6TsYwxQDNo8r3vw6MZ+Yxmbk7Is4DvgV8G3gROD8zBwAy89GIuAa4BzgZ2Aac\nW05nBfgqcBzwDNADPEixcJckSepADSUZmXnEvpTM/B7FglpTld8C3DJF2VvA5eUfSVp06mc1/eAH\n2cLWSLM317NLJElNGhjYcdisppnOYpHalUmGJLWRZmaxSO3KJEOagdrFmCZGYT///N+0uFVqZ9Mt\n4DXZQ9Vc5EsLjUmGNAOTPQDLrmwdyXQLeHlNaTEwyZBmqH69AruyNZ3pbn14TWmh6+x1VyVJUtuy\nJ0OSFpDJHu4HPuBPrWGSIUkLSP00WCied/L1q2D16rUtbJkWI5MMSVpgJnveidQKJhkSb+9idiqh\nJM2eSYaEKy1KUhVMMqSSKy1K0twyyZAkTctZK2qGSYYkaVrOWlEzTDIkSTPirBU1yiRDkjqYM6PU\nzkwyJKmDzWRm1GRPfHUsheaDSYYkdbjpZkbVP/H11d3/i4vXP0/E6Yf2MelQFUwyJGkRqE9Ebnv4\nuUNJhwM4VRWTDElahBzEqflgkiFJ6ngTA2B7errp7V3G0NAwp5++yltALWaSIUkdpH4QZ1WzSeZr\n1spki3w1Mz6kfgBscQtozFtALWaSoUVnsi81p/2pU9QP4qzqOTvz9TyfyZOD5saHeAuo/ZhkaNGZ\nbOVCH4imTjJfz9lpxc/RwmKSoUWp/kvNB6JJszdXtz60cJhkSJLmxFze+mgFHwI390wyJElzpopb\nH/PVQ1LVQ+AWcw+PSYYWPJ/tIB3ZZMuOz+RzMl8zXeazh6SKJKnTe3hmo62SjIhYCdwEfBTYD3wn\nM3+rta1Sp5uvUfJSp6qfsQIz+5w0M9Ol2aS/0weHTtf+hfrLUFslGcB9wDbgXwKnAI9ExE8y8xut\nbZY6yWQf1vkaJS91qmYHQx/pszVVD8ltDz8370l/VeMt5uq4C/WXobZJMiLiw8AvAWdl5gHgQETc\nCFwJmGRoxhbqh1XqNEfqIWk0MWl0n3pVjbeY7Lj1D6Brprdmofwy1DZJBrAGeDEzh2q29QMREcdm\n5mstapfaSP1vDW+99RYARx111KFt9lxI7aPRHpKZ3LqZyT6TjRepb8tMxpRMltDU91JMdo61D6Cb\nrm1T/eyFoJ2SjD5gsG7b3vLvk4AZJRk9Pd1TlvX3/1VTDVuouru7OO64d3DgwBuMjY23ujkz8vzz\nf8ONf/gnHNP7LgD2vpy849gTDr2f2Hbyyl869P71V38CjE/5fib7NFOn045rWxbvObZbW445/hTq\n7d+zs6F9dv/oKb7xwxGO6X0KePv3QrP7vD70Cld95v/m9NN/EYAf/CAP+7lTte9IbZvsZ08Wu/17\ndtLT8xGWLJn6/7lmHen/ztnoGh9vj/9cIuIa4Ncz8yM1204D/hZ4b2b+qGWNkyRJDasmdWnObore\njFp9FKnc7vlvjiRJmo12SjK2Aysj4sSabR8BnsvM11vUJkmS1KS2uV0CEBHfB/4a+ALwD4GHgesz\n8z+1tGGSJKlh7dSTAfApiuTiJ8B3gT80wZAkqTO1VU+GJElaONqtJ0OSJC0QJhmSJKkSJhmSJKkS\nJhmSJKkSJhmSJKkSJhmSJKkS7fSAtKZExErgJuCjwH7gO5n5W61tVfsp4/QN4OPAW8CfAFdm5lBE\nnAVsBk4HdgKbM/PuljW2DUXEf6SIV3f53pgdQURsBD4LLAf+Arg4M39k3KYWER8CbqB4IvUw8Biw\nITP3GLe/FxGfAO4AvpuZn64rO2KcIuIK4DJgBfAsRXz756vtrTJNzP4JRcxWAT8D/nNm/vua8lnF\nbCH0ZNwHvAS8B/jnwK9HxIaWtqg9PUjxVNtTgbUUF9R/iIgVwP0UidrJwAZgS0SsaVVD20355f+v\nKB+JGBHvxphNKSI+C3yaIqF9N/Ac8HmvtalFRA/FCsffp4jNKuBdwE3G7e9FxNUUvyz97SRlR4xT\nRKwHrgUuAE4BHgIeiohl89P61pgmZqdSxOF24ETgXwK/GRGfLstnHbOOTjIi4sPALwFfyswDmflD\n4Ebgkta2rL1ExPHANuCazBzOzF0UWe3HgfOBzMw7MnMkMx8DHgAual2L20dEdAE3U/yGOcGYHdlV\nwJcz83+Wn8sNmbkB43Yk7y7//NfMPJiZgxS/QK3GuNUapnim1Q8nKZsuTpcAt2fm9sx8E7ie4heH\n9fPQ7lY6UsxOAbZk5pbMHM3MbcCfUvzfAHMQs45OMii6FV/MzKGabf1ARMSxLWpT28nMVzPzosys\nfZrtqcDfUfRq1Hd99QPr5qt9be5Sig9pbdf0GozZpCLiHwC/APRFxEBE/Cwi7omIk/BaO5K/A54C\nLomIYyPiXcBvUPzmaNxKmfn7mbl/iuLp4nRYeWaOA0+zwON4pJiVycNVdZtPBX5cvp51zDo9yegD\nBuu27S3/Pmme29Ixyh6gzwH/nqljuOjjFxGnANcB/6auyJhN7efKvz8FnEXR03gqsAXjNqXyy/tT\nwL8AhoCXgR7gyxi3mZouTsZxGhFxOfBeYOKZYbOOWacnGQBdrW5AJ4mIjwGPUtxi+m652RhO7gbg\ntszMScqM2eQm4vJ7mfnT8tbctcCvUnSzGrdJRMRSinFT3wGOp3hQ5KvAXeUuxm1mpouTcZxCRHwO\n+B3gVzPzZzVFs4pZp88u2U2RadXqo/gy2/323Re3chDPncBnM3Piy2uqGL4yn21rNxFxNvDLwMXl\nptoPmjGb2k/Kv1+t2fYiRfyOwrhN5WzgPZn55fL9gYi4jqJr+o8xbjMx3edyqvIdFber7UXEvwM+\nA/zTzHy2pmjWMev0noztwMqIOLFm20eA5zLz9Ra1qS1FxC8Dfwj8Rk2CAUUM19btvg54cp6a1q7O\npxjdvzMidgN/BXRFxCsUH7AP1+1vzAo/puju/1DNtl8ARoBHMG5T6QG6I6L2O/kdFL8w/SnGbSam\n+y47rLyM9RoWeRwj4iqKWSUfrUswYA5i1tE9GZn5dERsA343Ir5A0cX4eYoRsCqV0+O2UNwieayu\n+C7guoi4sHx9NnAOcOb8trLtfB747Zr3p1Ks9/BBis/NNcbs7TJzNCJuAzZGxJ9TrF3zFYoetP8C\nfMW4Ter7wAHgdyJiE3AMxXiMP6OI3bXGbVrTfZfdDGyNiK0U6z1cDbxBMXV4UYqI91KMO/toZv54\nkl1mHbOu8fHxOWhq65Sj2bcA/5Sii/bmzPy3LW1Um4mIf0zxZfUmRbf1eM3fAfw88C2KBWxeBH4r\nM+9vSWPbVET8PPBCZvaU7/8xxmxS5fiCGyjWylgC/BFweWa+btymFhGrKeL2QYrP6uPAVZn5E+NW\niIhhiu+to8pNB4HxzDymLD9inCLiX1MkbydTTOv/N5n53LydQAscKWYR8dsUScZITZUuilmbv1jW\nn1XMOj7JkCRJ7anTx2RIkqQ2ZZIhSZIqYZIhSZIqYZIhSZIqYZIhSZIqYZIhSZIqYZIhSZIqYZIh\nSZIqYZIhSZIqYZIhSZIqYZIhSZIq8b8BifY2OILjSbUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b8115f3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# full_data['tmpstd'] = 0\n",
    "# full_data['tmpstd'] = SSL.fit_transform(full_data['sc_price'].values.reshape(-1,1)) \n",
    "fig = plt.hist(np.sqrt(full_data['sc_price']),bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
