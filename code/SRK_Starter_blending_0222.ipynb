{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.stats import skew, boxcox\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed =1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n",
      "49352\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_file = data_path + \"train.json\"\n",
    "test_file = data_path + \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "ntrain = train_df.shape[0]\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc_price\n",
    "tmp = pd.concat([train_df['price'],test_df['price']])\n",
    "ulimit = np.percentile(tmp.values, 99)\n",
    "\n",
    "train_df.loc[:,'sc_price'] = train_df['price'].values.reshape(-1, 1)\n",
    "test_df.loc[:,'sc_price'] = test_df['price'].values.reshape(-1, 1)\n",
    "\n",
    "train_df.loc[train_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "test_df.loc[test_df['sc_price']>ulimit, ['sc_price']] = ulimit\n",
    "\n",
    "# sc_ba_price\n",
    "inx_train = train_df['bathrooms'] == 0\n",
    "inx_test = test_df['bathrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "train_df.loc[non0_inx_train,'sc_ba_price'] = train_df.loc[non0_inx_train,'sc_price']/train_df.loc[non0_inx_train,'bathrooms']\n",
    "test_df.loc[non0_inx_test,'sc_ba_price'] = test_df.loc[non0_inx_test,'sc_price']/test_df.loc[non0_inx_test,'bathrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_ba_price'] = 0\n",
    "test_df.loc[inx_test,'sc_ba_price'] = 0\n",
    "\n",
    "# price per bedrooms\n",
    "\n",
    "inx_train = train_df['bedrooms'] == 0\n",
    "inx_test = test_df['bedrooms'] == 0\n",
    "\n",
    "non0_inx_train = ~inx_train\n",
    "non0_inx_test = ~inx_test\n",
    "train_df.loc[non0_inx_train,'sc_be_price'] = train_df.loc[non0_inx_train,'sc_price']/train_df.loc[non0_inx_train,'bedrooms']\n",
    "test_df.loc[non0_inx_test,'sc_be_price'] = test_df.loc[non0_inx_test,'sc_price']/test_df.loc[non0_inx_test,'bedrooms']\n",
    "\n",
    "train_df.loc[inx_train,'sc_be_price'] = 0\n",
    "test_df.loc[inx_test,'sc_be_price'] = 0\n",
    "\n",
    "\n",
    "# bathrooms\n",
    "\n",
    "ulimit = 5\n",
    "\n",
    "train_df['sc_bathrooms']=train_df['bathrooms']\n",
    "test_df['sc_bathrooms']=test_df['bathrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bathrooms']>ulimit,['sc_bathrooms']] = ulimit\n",
    "\n",
    "# bedrooms\n",
    "\n",
    "ulimit = 8\n",
    "\n",
    "train_df['sc_bedrooms']=train_df['bedrooms']\n",
    "test_df['sc_bedrooms']=test_df['bedrooms']\n",
    "\n",
    "train_df.loc[train_df['sc_bedrooms']>ulimit, ['sc_bedrooms']] = ulimit\n",
    "test_df.loc[test_df['sc_bedrooms']>ulimit,['sc_bedrooms']] = ulimit\n",
    "\n",
    "# longitude\n",
    "\n",
    "tmp = pd.concat([train_df['longitude'],test_df['longitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_longitude']=train_df['longitude']\n",
    "test_df['sc_longitude']=test_df['longitude']\n",
    "\n",
    "train_df.loc[train_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "test_df.loc[test_df['sc_longitude']>ulimit, ['sc_longitude']] = ulimit\n",
    "train_df.loc[train_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "test_df.loc[test_df['sc_longitude']<llimit, ['sc_longitude']] = llimit\n",
    "\n",
    "# latitude\n",
    "\n",
    "tmp = pd.concat([train_df['latitude'],test_df['latitude']])\n",
    "llimit = np.percentile(tmp.values, 0.1)\n",
    "ulimit = np.percentile(tmp.values, 99.9)\n",
    "\n",
    "train_df['sc_latitude']=train_df['latitude']\n",
    "test_df['sc_latitude']=test_df['latitude']\n",
    "\n",
    "train_df.loc[train_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "test_df.loc[test_df['sc_latitude']>ulimit, ['sc_latitude']] = ulimit\n",
    "train_df.loc[train_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "test_df.loc[test_df['sc_latitude']<llimit, ['sc_latitude']] = llimit\n",
    "\n",
    "\n",
    "features_to_use  = [\"sc_bathrooms\", \"sc_bedrooms\", \"sc_latitude\", \"sc_longitude\", \"sc_price\", \"sc_ba_price\", \"sc_be_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# adding all these new features to use list #\n",
    "features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \n",
    "                        \"created_day\", \"created_hour\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sc_bathrooms', 'sc_bedrooms', 'sc_latitude', 'sc_longitude', 'sc_price', 'sc_ba_price', 'sc_be_price', 'num_photos', 'num_features', 'num_description_words', 'created_month', 'created_day', 'created_hour']\n"
     ]
    }
   ],
   "source": [
    "print features_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_df,test_df])\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "for col in features_to_use:\n",
    "    full_data[col], lam = boxcox(full_data[col] - full_data[col].min() + 1)\n",
    "    full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1)) \n",
    "    train_df[col] = full_data[:ntrain,col]\n",
    "    test_df[col] = full_data[ntrain:,col]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.append(\"listing_id\")\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10                                                         \n",
      "10000     Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "100004    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "100007                               Hardwood_Floors No_Fee\n",
      "100013                                              Pre-War\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 218) (74659, 218)\n"
     ]
    }
   ],
   "source": [
    "# with categorical: fold 1 CV = 0.547696\n",
    "# without categorical: fold 1 CV = 0.566962\n",
    "\n",
    "# del_features_to_use = list(set(features_to_use) - set(categorical))\n",
    "\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with StandardScaler fold 1 cv = 0.543454\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   0.548962\n",
      "4   0.544886\n",
      "5   0.546504\n",
      "6   0.545221\n",
      "7   0.547026\n",
      "8   0.551261\n",
      "9   0.55255\n",
      "10   0.556356\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "for x in [3,4,5,6,7,8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print rgr.get_xgb_params()['max_depth'], '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "# 3   0.548962\n",
    "# 4   0.544886\n",
    "# 5   0.546504\n",
    "# 6   0.545221\n",
    "# 7   0.547026\n",
    "# 8   0.551261\n",
    "# 9   0.55255\n",
    "# 10   0.556356\n",
    "\n",
    "# SRK ori\n",
    "# 3   0.553568\n",
    "# 4   0.549047\n",
    "# 5   0.549068\n",
    "# 6   0.549237\n",
    "# 7   0.550124\n",
    "# 8   0.551921\n",
    "# 9   0.557487\n",
    "# 10   0.558945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.544886\n",
      "5   0.544297\n",
      "10   0.547673\n",
      "20   0.546779\n",
      "50   0.550753\n",
      "80   0.551513\n",
      "120   0.552604\n",
      "180   0.556093\n",
      "240   0.561224\n",
      "300   0.561104\n"
     ]
    }
   ],
   "source": [
    "for x in [5,10,20,50,80,120,180,240,300]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "    print rgr.get_xgb_params()['min_child_weight'], '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_weight = 5\n",
    "# 1   0.544886\n",
    "# 5   0.544297\n",
    "# 10   0.547673\n",
    "# 20   0.546779\n",
    "# 50   0.550753\n",
    "# 80   0.551513\n",
    "# 120   0.552604\n",
    "# 180   0.556093\n",
    "# 240   0.561224\n",
    "# 300   0.561104\n",
    "\n",
    "\n",
    "# SRK ori\n",
    "# 1   0.549068\n",
    "# 5   0.547466\n",
    "# 10   0.548375\n",
    "# 20   0.551334\n",
    "# 50   0.551166\n",
    "# 80   0.551872\n",
    "# 120   0.553488\n",
    "# 180   0.554964\n",
    "# 240   0.55856\n",
    "# 300   0.561123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.545806\n",
      "0.4 \t0.542401\n",
      "0.5 \t0.544305\n",
      "0.6 \t0.544499\n",
      "0.7 \t0.544259\n",
      "0.8 \t0.543949\n",
      "0.9 \t0.545657\n",
      "1.0 \t0.544297\n"
     ]
    }
   ],
   "source": [
    "for x in [0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "\n",
    "    print rgr.get_xgb_params()['colsample_bytree'], '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colsample_bytree = 0.8\n",
    "# 0.3 \t0.545806\n",
    "# 0.4 \t0.542401\n",
    "# 0.5 \t0.544305\n",
    "# 0.6 \t0.544499\n",
    "# 0.7 \t0.544259\n",
    "# 0.8 \t0.543949\n",
    "# 0.9 \t0.545657\n",
    "# 1.0 \t0.544297\n",
    "\n",
    "\n",
    "# SRK ori\n",
    "# 0.3   0.547816\n",
    "# 0.4   0.545827\n",
    "# 0.5   0.547024\n",
    "# 0.6   0.54467\n",
    "# 0.7   0.543454\n",
    "# 0.8   0.546214\n",
    "# 0.9   0.547861\n",
    "# 1.0   0.547466\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.55036\n",
      "0.6 \t0.544181\n",
      "0.7 \t0.545074\n",
      "0.8 \t0.544391\n",
      "0.9 \t0.541175\n",
      "1.0 \t0.543949\n"
     ]
    }
   ],
   "source": [
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print rgr.get_xgb_params()['subsample'], '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = 1\n",
    "# 0.5 \t0.55036\n",
    "# 0.6 \t0.544181\n",
    "# 0.7 \t0.545074\n",
    "# 0.8 \t0.544391\n",
    "# 0.9 \t0.541175\n",
    "# 1.0 \t0.543949\n",
    "\n",
    "\n",
    "# SRK ori\n",
    "# 0.5   0.550979\n",
    "# 0.6   0.54549\n",
    "# 0.7   0.545445\n",
    "# 0.8   0.543629\n",
    "# 0.9   0.544821\n",
    "# 1.0   0.543454\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.542805\n",
      "0.6 \t0.54408\n",
      "0.9 \t0.545023\n",
      "1.2 \t0.543232\n",
      "1.5 \t0.543877\n",
      "1.8 \t0.544225\n",
      "2.1 \t0.544766\n",
      "2.4 \t0.547343\n",
      "2.7 \t0.550244\n",
      "3.0 \t0.553193\n"
     ]
    }
   ],
   "source": [
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print rgr.get_xgb_params()['gamma'], '\\t', rgr.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.3\n",
    "# 0.3 \t0.542805\n",
    "# 0.6 \t0.54408\n",
    "# 0.9 \t0.545023\n",
    "# 1.2 \t0.543232\n",
    "# 1.5 \t0.543877\n",
    "# 1.8 \t0.544225\n",
    "# 2.1 \t0.544766\n",
    "# 2.4 \t0.547343\n",
    "# 2.7 \t0.550244\n",
    "# 3.0 \t0.553193\n",
    "# SRK ori\n",
    "# 0   0.543454\n",
    "# 0.3   0.545775\n",
    "# 0.6   0.544581\n",
    "# 0.9   0.544173\n",
    "# 1.2   0.545422\n",
    "# 1.5   0.545404\n",
    "# 1.8   0.544855\n",
    "# 2.1   0.544393\n",
    "# 2.4   0.545129\n",
    "# 2.7   0.546333\n",
    "# 3.0   0.549442\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = max_depth\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = min_child_weight\n",
    "    param['subsample'] = subsample\n",
    "    param['colsample_bytree'] = colsample_bytree\n",
    "    param['gamma'] = gamma\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cv_scores = []\n",
    "# kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "# for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "#         dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "#         dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "#         preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "#         cv_scores.append(log_loss(val_y, preds))\n",
    "#         print(cv_scores)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_blend(params, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(params)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((N_params))\n",
    "    \n",
    "    for j, param in enumerate(params):\n",
    "        param['objective']='multi:softprob'\n",
    "        param['eval_metric']='mlogloss',\n",
    "        param['num_class']=3\n",
    "        param['silent']= False\n",
    "        param['eta'] = 0.03\n",
    "#         param['verbose_eval'] = 10  \n",
    "        print (\"Model %d:\" %(j+1))\n",
    "        \n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "        cv_result = xgb.cv(param, xgtrain,\n",
    "                           num_boost_round=10000, nfold=fold,\n",
    "                           metrics = 'mlogloss',\n",
    "                           seed=seed,callbacks=[xgb.callback.early_stop(early_stopping_rounds)])    \n",
    "        best_round = cv_result.shape[0] - 1\n",
    "        print 'best_round',best_round\n",
    "        best_rounds[j]=best_round\n",
    "        \n",
    "        param.pop('eval_metric')\n",
    "        all_round = best_round / (1 - 1. / fold)\n",
    "        est_test_blend = xgb.train(param, xgtrain,num_boost_round=int(all_round))\n",
    "\n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = est_test_blend.predict(xgb.DMatrix(test_x))\n",
    "\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "            xgtrain_fold = xgb.DMatrix(train_x_fold, label=train_y_fold)\n",
    "            est_train_blend = xgb.train(param, xgtrain_fold,num_boost_round=best_round)\n",
    "            \n",
    "            val_y_predict_fold = est_train_blend.predict(xgb.DMatrix(val_x_fold))\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 5 folds\n",
      "Model 1:\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3087]\ttrain-mlogloss:0.405973+0.00103189\ttest-mlogloss:0.545781+0.00657891\n",
      "\n",
      "best_round 3087\n",
      "Model 1 fold 1\n",
      "('Score: ', 0.55203147819051201)\n",
      "Model 1 fold 1 fitting finished in 272.326s\n",
      "Model 1 fold 2\n",
      "('Score: ', 0.5420850846688795)\n",
      "Model 1 fold 2 fitting finished in 270.435s\n",
      "Model 1 fold 3\n",
      "('Score: ', 0.54142525511151163)\n",
      "Model 1 fold 3 fitting finished in 270.289s\n",
      "Model 1 fold 4\n",
      "('Score: ', 0.5493672661635377)\n",
      "Model 1 fold 4 fitting finished in 264.125s\n",
      "Model 1 fold 5\n",
      "('Score: ', 0.56116053485582518)\n",
      "Model 1 fold 5 fitting finished in 262.688s\n",
      "Score for model 1 is 0.549214\n",
      "Score for blended models is 0.549214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_params = [{'max_depth':4,\n",
    "               'min_child_weight':5,\n",
    "               'colsample_bytree':0.8,\n",
    "               'subsample':1,\n",
    "               'gamma':0.3},\n",
    "# #               score -0.530282              \n",
    "\n",
    "#               {'max_depth':4,\n",
    "#                'min_child_weight':2,\n",
    "#                'colsample_bytree':0.828660,\n",
    "#                'subsample':0.821156,\n",
    "#                'gamma':2.751725},\n",
    "# #               score -0.530455\n",
    "          \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':12,\n",
    "#                'colsample_bytree':0.736776,\n",
    "#                'subsample':0.947351,\n",
    "#                'gamma':2.677209},\n",
    "# #               score -0.530621    \n",
    "              \n",
    "#               {'max_depth':5,\n",
    "#                'min_child_weight':12,\n",
    "#                'colsample_bytree':0.736769,\n",
    "#                'subsample':0.947350,\n",
    "#                'gamma':2.677208},\n",
    "# #               score -0.530917                 \n",
    "\n",
    "#               {'max_depth':4,\n",
    "#                'min_child_weight':7,\n",
    "#                'colsample_bytree':0.838006,\n",
    "#                'subsample':0.930783,\n",
    "#                'gamma':2.668471}\n",
    "# #               score -0.530937\n",
    "             ]\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(xgb_params,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              5,\n",
    "                              300)\n",
    "\n",
    "# print (np.mean(blend_scores_xgb_le,axis=0))\n",
    "# print (np.mean(best_rounds_xgb_le,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.22046652e-02,   1.47814408e-01,   8.19980919e-01],\n",
       "       [  7.82494491e-04,   1.47806276e-02,   9.84436870e-01],\n",
       "       [  5.79654947e-02,   4.18617249e-01,   5.23417234e-01],\n",
       "       [  1.62354447e-02,   5.85017875e-02,   9.25262749e-01],\n",
       "       [  1.48545334e-03,   4.72688749e-02,   9.51245606e-01],\n",
       "       [  3.11728679e-02,   1.84923366e-01,   7.83903718e-01],\n",
       "       [  1.71255842e-02,   1.19503237e-01,   8.63371134e-01],\n",
       "       [  1.92419603e-03,   8.05531368e-02,   9.17522669e-01],\n",
       "       [  1.16785206e-01,   7.12138593e-01,   1.71076208e-01],\n",
       "       [  3.31162512e-02,   2.65499383e-01,   7.01384366e-01],\n",
       "       [  6.79918099e-04,   1.50236925e-02,   9.84296381e-01],\n",
       "       [  2.32769016e-04,   3.40811606e-03,   9.96359050e-01],\n",
       "       [  3.62770468e-01,   4.48342055e-01,   1.88887507e-01],\n",
       "       [  3.06826551e-04,   1.26267700e-02,   9.87066448e-01],\n",
       "       [  1.44303008e-03,   3.46196666e-02,   9.63937223e-01],\n",
       "       [  4.79623079e-02,   2.70421147e-01,   6.81616545e-01],\n",
       "       [  3.53140235e-02,   1.03278756e-01,   8.61407280e-01],\n",
       "       [  1.24862196e-03,   7.37149781e-03,   9.91379857e-01],\n",
       "       [  5.34051214e-05,   3.98138026e-03,   9.95965242e-01],\n",
       "       [  5.97895938e-04,   2.11113878e-03,   9.97290909e-01]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds, model = runXGB(train_X, train_y, test_X, num_rounds=int(3087/.8))\n",
    "out_df = pd.DataFrame(test_blend_x_xgb)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"../output/xgb_starter_SRK0222_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
