{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../input/\"\n",
    "\n",
    "train_df = pd.read_pickle(data_path + 'train_2017-03-05-22-40.pkl')\n",
    "train_y = pd.read_pickle(data_path + 'y_2017-03-05-22-40.pkl')\n",
    "test_df = pd.read_pickle(data_path + 'test_2017-03-05-22-40.pkl')\n",
    "features_to_use = pd.read_pickle(data_path + 'featurestouse_2017-03-05-22-40.pkl')\n",
    "\n",
    "tr_desc_sparse = pd.read_pickle(data_path + 'tr_desc_sparse_2017-03-05-22-40.pkl')\n",
    "tr_feat_sparse = pd.read_pickle(data_path + 'tr_feat_sparse_2017-03-05-22-40.pkl')\n",
    "te_desc_sparse = pd.read_pickle(data_path + 'te_desc_sparse_2017-03-05-22-40.pkl')\n",
    "te_feat_sparse = pd.read_pickle(data_path + 'te_feat_sparse_2017-03-05-22-40.pkl')\n",
    "\n",
    "desc_sparse_cols = pd.read_pickle(data_path + 'desc_sparse_cols_2017-03-05-22-40.pkl')\n",
    "feat_sparse_cols = pd.read_pickle(data_path + 'feat_sparse_cols_2017-03-05-22-40.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 457) (74659, 457)\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_desc_sparse, tr_feat_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_desc_sparse, te_feat_sparse]).tocsr()\n",
    "\n",
    "\n",
    "all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[25]\tvalid_0's multi_logloss: 0.627417\n",
      "[50]\tvalid_0's multi_logloss: 0.582572\n",
      "[75]\tvalid_0's multi_logloss: 0.568281\n",
      "[100]\tvalid_0's multi_logloss: 0.560913\n",
      "[125]\tvalid_0's multi_logloss: 0.557244\n",
      "[150]\tvalid_0's multi_logloss: 0.554635\n",
      "[175]\tvalid_0's multi_logloss: 0.553015\n",
      "[200]\tvalid_0's multi_logloss: 0.552212\n",
      "[225]\tvalid_0's multi_logloss: 0.55191\n",
      "[250]\tvalid_0's multi_logloss: 0.551563\n",
      "[275]\tvalid_0's multi_logloss: 0.551774\n",
      "[300]\tvalid_0's multi_logloss: 0.552159\n",
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's multi_logloss: 0.551424\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(learning_rate = 0.1)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "        \n",
    "clf = clf.fit(X_train, y_train,\n",
    "              eval_set = [(X_val,y_val)],\n",
    "              eval_metric = 'multi_logloss',\n",
    "              early_stopping_rounds = 50,\n",
    "              verbose = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = clf.predict_proba(test_X, num_iteration = clf.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "sub_name = '../output/sub_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(pred_y[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cv_dataset = lgb.Dataset(train_X, train_y,\n",
    "#                         free_raw_data = False,\n",
    "#                         feature_name = all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55142379592804802"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.evals_result.values()[0]['multi_logloss'][250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8  \t0.554749035506 727\n",
      "15  \t0.550121665474 581\n",
      "31  \t0.551423795928 251\n",
      "63  \t0.551923841631 149\n",
      "127  \t0.55330551502 102\n",
      "255  \t0.562214075515 70\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.set_params(learning_rate = 0.1)\n",
    "clf.set_params(subsample_freq = 1)\n",
    "clf.set_params(objective = 'multiclass')\n",
    "clf.set_params(n_estimators = 100000)\n",
    "        \n",
    "\n",
    "tmp  = 1000\n",
    "for x in [8,15,31,63,127,255]:\n",
    "    clf.set_params(num_leaves = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        num_leaves = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=15,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(num_leaves = num_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  \t0.550121665474 581\n",
      "20  \t0.549912688816 512\n",
      "30  \t0.550117278371 473\n",
      "50  \t0.550495059319 580\n",
      "70  \t0.549630570323 490\n",
      "80  \t0.550301382549 583\n",
      "90  \t0.549211688569 541\n",
      "100  \t0.549800126976 521\n",
      "110  \t0.550536072369 452\n",
      "120  \t0.548761755286 521\n",
      "150  \t0.549187250993 554\n",
      "170  \t0.548417297631 453\n",
      "200  \t0.547889637762 559\n",
      "230  \t0.548951807203 465\n",
      "260  \t0.549699494886 506\n"
     ]
    }
   ],
   "source": [
    "tmp  = 1000\n",
    "for x in [10, 20, 30, 50, 70, 80,90,100,110,120,150,170,200,230,260]:\n",
    "    clf.set_params(min_child_samples = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        min_child_samples = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=200, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=15,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(min_child_samples = min_child_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4  \t0.549440682317 661\n",
      "0.5  \t0.548130940147 646\n",
      "0.6  \t0.547785619614 488\n",
      "0.7  \t0.546980258999 454\n",
      "0.8  \t0.549510061419 581\n",
      "0.9  \t0.548454288363 526\n",
      "1  \t0.547889637762 559\n"
     ]
    }
   ],
   "source": [
    "tmp  = 1000\n",
    "for x in [0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    clf.set_params(colsample_bytree = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        colsample_bytree = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.7, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=200, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=15,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(colsample_bytree = colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5  \t0.553313333624 532\n",
      "0.6  \t0.552616940574 402\n",
      "0.7  \t0.550063288956 491\n",
      "0.8  \t0.5499600939 498\n",
      "0.9  \t0.547960666237 508\n",
      "1  \t0.546980258999 454\n"
     ]
    }
   ],
   "source": [
    "tmp  = 1000\n",
    "for x in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    clf.set_params(subsample = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        subsample = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.7, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=15, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=200, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=15,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(subsample = subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  \t0.546597489793 675\n",
      "300  \t0.547176672163 574\n",
      "400  \t0.548544964529 574\n"
     ]
    }
   ],
   "source": [
    "tmp  = 1000\n",
    "for x in [200,300,400]:#[15,31,63, 127, 255, 511, 1023, 2047]:\n",
    "    clf.set_params(max_bin = x)\n",
    "    clf = clf.fit(X_train, y_train,\n",
    "                  eval_set = [(X_val,y_val)],\n",
    "                  eval_metric = 'multi_logloss',\n",
    "                  early_stopping_rounds = 50,\n",
    "                  verbose = False)\n",
    "    if tmp > clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]:\n",
    "        max_bin = x\n",
    "        tmp = clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1]\n",
    "\n",
    "\n",
    "    print x, ' \\t', clf.evals_result.values()[0]['multi_logloss'][clf.best_iteration -1], clf.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "15  \t0.551604274677 627\n",
    "31  \t0.553090529644 464\n",
    "63  \t0.549920964586 639\n",
    "127  \t0.549448332103 560\n",
    "200  \t0.546597489793 675\n",
    "255  \t0.546980258999 454\n",
    "300  \t0.547176672163 574\n",
    "400  \t0.548544964529 574\n",
    "511  \t0.549596013107 411\n",
    "1023  \t0.548102295097 566\n",
    "2047  \t0.548225477116 552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_samples |   num_leaves |   subsample | \n",
      "    1 | 08m01s | \u001b[35m  -0.54963\u001b[0m | \u001b[32m            0.9460\u001b[0m | \u001b[32m 211.6904\u001b[0m | \u001b[32m           217.7480\u001b[0m | \u001b[32m     20.4129\u001b[0m | \u001b[32m     0.8806\u001b[0m | \n",
      "    2 | 03m03s |   -0.55004 |             0.7666 |  234.1145 |            130.0934 |      27.0889 |      0.9055 | \n",
      "    3 | 04m16s |   -0.54991 |             0.8285 |  275.2507 |            168.6393 |      12.8018 |      0.9677 | \n",
      "    4 | 03m32s | \u001b[35m  -0.54932\u001b[0m | \u001b[32m            0.8229\u001b[0m | \u001b[32m 261.7395\u001b[0m | \u001b[32m           157.2411\u001b[0m | \u001b[32m     17.8669\u001b[0m | \u001b[32m     0.8453\u001b[0m | \n",
      "    5 | 03m35s |   -0.55102 |             0.7590 |  173.4688 |            132.5745 |      14.9747 |      0.9379 | \n",
      "    6 | 02m55s |   -0.54977 |             0.6249 |  270.8928 |            206.3019 |      27.2594 |      0.8490 | \n",
      "    7 | 03m13s | \u001b[35m  -0.54905\u001b[0m | \u001b[32m            0.6317\u001b[0m | \u001b[32m 187.2825\u001b[0m | \u001b[32m           170.8267\u001b[0m | \u001b[32m     18.8127\u001b[0m | \u001b[32m     0.8950\u001b[0m | \n",
      "    8 | 03m51s |   -0.55009 |             0.9424 |  222.8695 |            209.0662 |      14.8586 |      0.8493 | \n",
      "    9 | 03m39s |   -0.55032 |             0.6714 |  276.3794 |            225.2630 |      15.0386 |      0.9458 | \n",
      "   10 | 05m10s |   -0.55079 |             0.7010 |  197.2413 |            191.4957 |       8.5219 |      0.9618 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |   max_bin |   min_child_samples |   num_leaves |   subsample | \n",
      "   11 | 03m59s |   -0.54952 |             0.8438 |  139.6069 |            229.7884 |      25.7003 |      0.9549 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00040854]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 03m18s |   -0.55000 |             0.7579 |  201.4824 |            174.6327 |      30.6962 |      0.8784 | \n",
      "   13 | 03m47s | \u001b[35m  -0.54864\u001b[0m | \u001b[32m            0.9206\u001b[0m | \u001b[32m 288.7490\u001b[0m | \u001b[32m           124.6431\u001b[0m | \u001b[32m     24.1693\u001b[0m | \u001b[32m     0.8716\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00058548]), 'nit': 4, 'funcalls': 57}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 03m12s |   -0.55034 |             0.6142 |  132.9146 |            168.0585 |      28.6169 |      0.9421 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00012027]), 'nit': 5, 'funcalls': 62}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 03m30s |   -0.54901 |             0.8155 |  286.7125 |            153.2362 |      26.3706 |      0.9325 | \n",
      "   16 | 04m53s |   -0.55237 |             0.8832 |  289.6937 |            124.1972 |       8.0902 |      0.8075 | \n",
      "   17 | 03m18s |   -0.55015 |             0.7765 |  283.6394 |            133.5231 |      29.8266 |      0.8444 | \n",
      "   18 | 03m30s |   -0.54995 |             0.8383 |  299.8306 |            194.4852 |      30.3670 |      0.8682 | \n",
      "   19 | 05m08s |   -0.55130 |             0.9380 |  176.2912 |            229.5021 |      12.5143 |      0.9461 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -9.26755798e-05]), 'nit': 6, 'funcalls': 58}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 03m40s | \u001b[35m  -0.54847\u001b[0m | \u001b[32m            0.8531\u001b[0m | \u001b[32m 295.1750\u001b[0m | \u001b[32m           124.7518\u001b[0m | \u001b[32m     30.5231\u001b[0m | \u001b[32m     0.9252\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00052732]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 03m53s |   -0.54975 |             0.9705 |  156.4544 |            201.3598 |      28.0879 |      0.9061 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -9.86243676e-05]), 'nit': 5, 'funcalls': 61}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 03m06s |   -0.55062 |             0.6619 |  139.1473 |            120.8136 |      27.0732 |      0.9462 | \n",
      "   23 | 03m29s |   -0.55110 |             0.8387 |  246.5130 |            222.5992 |      30.6178 |      0.8588 | \n",
      "   24 | 03m13s |   -0.54990 |             0.6149 |  174.4775 |            169.5950 |      28.4826 |      0.8611 | \n",
      "   25 | 03m49s |   -0.55009 |             0.7837 |  128.7771 |            199.4040 |      16.6673 |      0.8735 | \n",
      "   26 | 03m38s |   -0.55013 |             0.8313 |  263.1905 |            172.2646 |      27.7548 |      0.8922 | \n",
      "   27 | 04m45s |   -0.55129 |             0.7480 |  214.8425 |            136.6220 |      10.1192 |      0.9577 | \n",
      "   28 | 03m46s |   -0.54951 |             0.8868 |  297.3956 |            222.4299 |      29.8218 |      0.8842 | \n",
      "   29 | 03m48s |   -0.55017 |             0.7577 |  299.9109 |            148.0573 |      19.8152 |      0.9457 | \n",
      "   30 | 05m45s |   -0.55116 |             0.8396 |  140.1333 |            124.3684 |       8.2570 |      0.9175 | \n",
      "   31 | 04m43s |   -0.55058 |             0.7325 |  135.2644 |            226.2855 |      10.3124 |      0.8708 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.02295928e-05]), 'nit': 3, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00039799]), 'nit': 6, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00166096]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 | 05m44s |   -0.55105 |             0.9660 |  153.9076 |            163.3903 |       9.9017 |      0.9103 | \n",
      "   33 | 03m09s |   -0.55066 |             0.6215 |  129.4628 |            221.8249 |      29.5196 |      0.8512 | \n",
      "   34 | 03m44s |   -0.54971 |             0.7369 |  274.4785 |            120.6243 |      24.2176 |      0.8231 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -3.81744321e-05]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -7.33602647e-05]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 03m48s |   -0.54906 |             0.9571 |  285.2068 |            164.9676 |      29.6011 |      0.8406 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -5.55520561e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   36 | 04m30s |   -0.55034 |             0.8858 |  226.8172 |            170.4293 |      14.4001 |      0.8586 | \n",
      "   37 | 03m05s |   -0.55148 |             0.6120 |  184.8332 |            226.6471 |      29.6228 |      0.8342 | \n",
      "   38 | 03m15s |   -0.54989 |             0.6624 |  196.0310 |            145.2668 |      29.9471 |      0.8057 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0004217]), 'nit': 5, 'funcalls': 58}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39 | 05m36s |   -0.55134 |             0.9794 |  213.6993 |            225.1597 |       9.0249 |      0.9152 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00132868]), 'nit': 5, 'funcalls': 59}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40 | 03m44s |   -0.55037 |             0.8811 |  198.7085 |            121.4965 |      26.3915 |      0.9500 | \n",
      "   41 | 03m53s |   -0.54956 |             0.7665 |  298.0426 |            212.8628 |      18.6427 |      0.8351 | \n",
      "   42 | 03m52s |   -0.55089 |             0.8129 |  291.2952 |            122.8679 |      22.8399 |      0.8387 | \n",
      "   43 | 04m52s |   -0.54974 |             0.8611 |  254.4196 |            140.8148 |      14.4113 |      0.9423 | \n",
      "   44 | 04m23s |   -0.55170 |             0.6287 |  294.1900 |            229.7629 |      10.4736 |      0.8196 | \n",
      "   45 | 05m33s |   -0.55045 |             0.9916 |  272.8526 |            142.5846 |      12.9358 |      0.9678 | \n",
      "   46 | 05m59s |   -0.55032 |             0.9979 |  287.4494 |            198.5770 |      11.5600 |      0.9247 | \n",
      "   47 | 05m49s |   -0.55077 |             0.9919 |  252.9265 |            175.9670 |      10.7541 |      0.9384 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00044591]), 'nit': 5, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48 | 07m00s |   -0.55098 |             0.9629 |  186.0242 |            175.2644 |       8.1343 |      0.9797 | \n",
      "   49 | 03m46s |   -0.55017 |             0.9731 |  167.5765 |            127.8197 |      30.4875 |      0.8551 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0001718]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 | 04m12s |   -0.54998 |             0.9323 |  240.9843 |            154.7499 |      30.9518 |      0.9867 | \n"
     ]
    }
   ],
   "source": [
    "def lgbm_cv(max_bin, num_leaves, min_child_samples, colsample_bytree, subsample, learning_rate=0.1):\n",
    "    skf = KFold(n_splits=5,random_state=seed)\n",
    "    scores=[]\n",
    "    for i, (train, val) in enumerate(skf.split(train_X)):\n",
    "        est=lgb.LGBMClassifier(learning_rate=0.1,\n",
    "                               max_bin=int(max_bin),\n",
    "                               num_leaves=int(num_leaves),\n",
    "                               min_child_samples=int(min_child_samples),\n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               subsample=subsample,\n",
    "                               subsample_freq = 1\n",
    "                              )\n",
    " \n",
    "        train_x_fold = train_X[train]\n",
    "        train_y_fold = train_y[train]\n",
    "        val_x_fold = train_X[val]\n",
    "        val_y_fold = train_y[val]\n",
    "        est.set_params( n_estimators=100000)\n",
    "        est.fit(train_x_fold,\n",
    "                train_y_fold,\n",
    "                eval_set=[(val_x_fold, val_y_fold)],\n",
    "                eval_metric='multi_logloss',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False\n",
    "               )\n",
    "        val_y_predict_fold = est.predict_proba(val_x_fold)\n",
    "        score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "        scores.append(score)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "\n",
    "lgbm_BO = BayesianOptimization(lgbm_cv, \n",
    "                               {\n",
    "                                'max_bin': (127,300),\n",
    "                                'num_leaves': (8,31),\n",
    "                                'min_child_samples' :(120,230),\n",
    "                                'colsample_bytree': (0.6,1.0),\n",
    "                                'subsample' : (0.8,1)})\n",
    "\n",
    "lgbm_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>min_child_samples</th>\n",
       "      <th>max_bin</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.523147</td>\n",
       "      <td>124.751807</td>\n",
       "      <td>295.175031</td>\n",
       "      <td>0.853059</td>\n",
       "      <td>0.925184</td>\n",
       "      <td>-0.548472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.169263</td>\n",
       "      <td>124.643095</td>\n",
       "      <td>288.749046</td>\n",
       "      <td>0.920628</td>\n",
       "      <td>0.871552</td>\n",
       "      <td>-0.548643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.370628</td>\n",
       "      <td>153.236247</td>\n",
       "      <td>286.712464</td>\n",
       "      <td>0.815477</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>-0.549005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29.601053</td>\n",
       "      <td>164.967606</td>\n",
       "      <td>285.206768</td>\n",
       "      <td>0.957067</td>\n",
       "      <td>0.840632</td>\n",
       "      <td>-0.549058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29.821756</td>\n",
       "      <td>222.429922</td>\n",
       "      <td>297.395617</td>\n",
       "      <td>0.886827</td>\n",
       "      <td>0.884196</td>\n",
       "      <td>-0.549511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_leaves  min_child_samples     max_bin  colsample_bytree  subsample  \\\n",
       "9    30.523147         124.751807  295.175031          0.853059   0.925184   \n",
       "2    24.169263         124.643095  288.749046          0.920628   0.871552   \n",
       "4    26.370628         153.236247  286.712464          0.815477   0.932513   \n",
       "24   29.601053         164.967606  285.206768          0.957067   0.840632   \n",
       "17   29.821756         222.429922  297.395617          0.886827   0.884196   \n",
       "\n",
       "       score  \n",
       "9  -0.548472  \n",
       "2  -0.548643  \n",
       "4  -0.549005  \n",
       "24 -0.549058  \n",
       "17 -0.549511  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_bo_scores = pd.DataFrame([[s[0]['num_leaves'],\n",
    "                               s[0]['min_child_samples'],\n",
    "                               s[0]['max_bin'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[1]] for s in zip(lgbm_BO.res['all']['params'],lgbm_BO.res['all']['values'])],\n",
    "                            columns = ['num_leaves',\n",
    "                                       'min_child_samples',\n",
    "                                       'max_bin',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'score'])\n",
    "gbm_bo_scores=gbm_bo_scores.sort_values('score',ascending=False)\n",
    "gbm_bo_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbm_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(learning_rate = 0.01)\n",
    "        est.set_params(subsample_freq = 1)\n",
    "        est.set_params(objective = 'multiclass')\n",
    "        est.set_params(n_estimators = 100000)\n",
    "\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est)) \n",
    "\n",
    "        \n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x[val_index]\n",
    "            val_y_fold = train_y[val_index]\n",
    "            \n",
    "            est.fit(train_x_fold, train_y_fold,\n",
    "                   eval_set = [(val_x_fold,val_y_fold)],\n",
    "                   eval_metric = 'multi_logloss',\n",
    "                   early_stopping_rounds = early_stopping_rounds,\n",
    "                   verbose = False)\n",
    "            \n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            \n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,num_iteration = best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score   \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,num_iteration=best_round)\n",
    "            \n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "            \n",
    "        test_blend_x[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 10 folds\n",
      "Model 1: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.853059, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.01, max_bin=295, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=124, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=30,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.925184, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 1 fold 1\n",
      "best round 3918\n",
      "('Score: ', 0.55298554008510625)\n",
      "Model 1 fold 1 fitting finished in 543.753s\n",
      "Model 1 fold 2\n",
      "best round 3920\n",
      "('Score: ', 0.5403799269476991)\n",
      "Model 1 fold 2 fitting finished in 517.663s\n",
      "Model 1 fold 3\n",
      "best round 4274\n",
      "('Score: ', 0.52314649078651643)\n",
      "Model 1 fold 3 fitting finished in 568.412s\n",
      "Model 1 fold 4\n",
      "best round 3564\n",
      "('Score: ', 0.53809719083518437)\n",
      "Model 1 fold 4 fitting finished in 486.254s\n",
      "Model 1 fold 5\n",
      "best round 3832\n",
      "('Score: ', 0.52808003024281225)\n",
      "Model 1 fold 5 fitting finished in 523.322s\n",
      "Model 1 fold 6\n",
      "best round 3430\n",
      "('Score: ', 0.53879891399245416)\n",
      "Model 1 fold 6 fitting finished in 499.550s\n",
      "Model 1 fold 7\n",
      "best round 3585\n",
      "('Score: ', 0.5490678921001092)\n",
      "Model 1 fold 7 fitting finished in 531.832s\n",
      "Model 1 fold 8\n",
      "best round 3802\n",
      "('Score: ', 0.53863496879067141)\n",
      "Model 1 fold 8 fitting finished in 541.451s\n",
      "Model 1 fold 9\n",
      "best round 3537\n",
      "('Score: ', 0.553536346529515)\n",
      "Model 1 fold 9 fitting finished in 496.589s\n",
      "Model 1 fold 10\n",
      "best round 4016\n",
      "('Score: ', 0.55698490611827634)\n",
      "Model 1 fold 10 fitting finished in 536.016s\n",
      "Score for model 1 is 0.541971\n",
      "Model 2: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.920628, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.01, max_bin=288, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=124, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=24,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.871552, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 2 fold 1\n",
      "best round 4934\n",
      "('Score: ', 0.55200563135098901)\n",
      "Model 2 fold 1 fitting finished in 594.918s\n",
      "Model 2 fold 2\n",
      "best round 4594\n",
      "('Score: ', 0.540863666722324)\n",
      "Model 2 fold 2 fitting finished in 546.139s\n",
      "Model 2 fold 3\n",
      "best round 4980\n",
      "('Score: ', 0.52258236145526715)\n",
      "Model 2 fold 3 fitting finished in 585.523s\n",
      "Model 2 fold 4\n",
      "best round 4670\n",
      "('Score: ', 0.53733847118008937)\n",
      "Model 2 fold 4 fitting finished in 542.762s\n",
      "Model 2 fold 5\n",
      "best round 5000\n",
      "('Score: ', 0.52742695155907537)\n",
      "Model 2 fold 5 fitting finished in 590.102s\n",
      "Model 2 fold 6\n",
      "best round 3967\n",
      "('Score: ', 0.54008579968756187)\n",
      "Model 2 fold 6 fitting finished in 478.618s\n",
      "Model 2 fold 7\n",
      "best round 4121\n",
      "('Score: ', 0.54849297430475075)\n",
      "Model 2 fold 7 fitting finished in 501.278s\n",
      "Model 2 fold 8\n",
      "best round 4771\n",
      "('Score: ', 0.53876258288506529)\n",
      "Model 2 fold 8 fitting finished in 556.245s\n",
      "Model 2 fold 9\n",
      "best round 4086\n",
      "('Score: ', 0.55376351196990392)\n",
      "Model 2 fold 9 fitting finished in 512.458s\n",
      "Model 2 fold 10\n",
      "best round 4440\n",
      "('Score: ', 0.55662290916228274)\n",
      "Model 2 fold 10 fitting finished in 527.961s\n",
      "Score for model 2 is 0.541794\n",
      "Model 3: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.815477, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.01, max_bin=286, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=153, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=26,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.932513, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 3 fold 1\n",
      "best round 4295\n",
      "('Score: ', 0.55312759204703044)\n",
      "Model 3 fold 1 fitting finished in 514.533s\n",
      "Model 3 fold 2\n",
      "best round 4993\n",
      "('Score: ', 0.54011603876897507)\n",
      "Model 3 fold 2 fitting finished in 562.240s\n",
      "Model 3 fold 3\n",
      "best round 4881\n",
      "('Score: ', 0.5220764033075016)\n",
      "Model 3 fold 3 fitting finished in 573.339s\n",
      "Model 3 fold 4\n",
      "best round 4799\n",
      "('Score: ', 0.53761950956129534)\n",
      "Model 3 fold 4 fitting finished in 547.261s\n",
      "Model 3 fold 5\n",
      "best round 5010\n",
      "('Score: ', 0.52744245802161782)\n",
      "Model 3 fold 5 fitting finished in 640.491s\n",
      "Model 3 fold 6\n",
      "best round 3698\n",
      "('Score: ', 0.53908429775220645)\n",
      "Model 3 fold 6 fitting finished in 446.549s\n",
      "Model 3 fold 7\n",
      "best round 3776\n",
      "('Score: ', 0.54903641973661865)\n",
      "Model 3 fold 7 fitting finished in 467.703s\n",
      "Model 3 fold 8\n",
      "best round 3773\n",
      "('Score: ', 0.53874783303592344)\n",
      "Model 3 fold 8 fitting finished in 454.476s\n",
      "Model 3 fold 9\n",
      "best round 4132\n",
      "('Score: ', 0.55399869297556381)\n",
      "Model 3 fold 9 fitting finished in 500.007s\n",
      "Model 3 fold 10\n",
      "best round 4575\n",
      "('Score: ', 0.55647515467406139)\n",
      "Model 3 fold 10 fitting finished in 535.880s\n",
      "Score for model 3 is 0.541772\n",
      "Model 4: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.957067, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.01, max_bin=285, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=164, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=29,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.840632, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 4 fold 1\n",
      "best round 3744\n",
      "('Score: ', 0.55345631657299199)\n",
      "Model 4 fold 1 fitting finished in 524.726s\n",
      "Model 4 fold 2\n",
      "best round 4181\n",
      "('Score: ', 0.54072022569316802)\n",
      "Model 4 fold 2 fitting finished in 549.640s\n",
      "Model 4 fold 3\n",
      "best round 4767\n",
      "('Score: ', 0.52057419685200079)\n",
      "Model 4 fold 3 fitting finished in 626.272s\n",
      "Model 4 fold 4\n",
      "best round 3706\n",
      "('Score: ', 0.53770764183399988)\n",
      "Model 4 fold 4 fitting finished in 521.455s\n",
      "Model 4 fold 5\n",
      "best round 4056\n",
      "('Score: ', 0.52778561334530294)\n",
      "Model 4 fold 5 fitting finished in 562.636s\n",
      "Model 4 fold 6\n",
      "best round 3359\n",
      "('Score: ', 0.53872765722580152)\n",
      "Model 4 fold 6 fitting finished in 475.412s\n",
      "Model 4 fold 7\n",
      "best round 2988\n",
      "('Score: ', 0.5498100854120066)\n",
      "Model 4 fold 7 fitting finished in 434.813s\n",
      "Model 4 fold 8\n",
      "best round 3618\n",
      "('Score: ', 0.53911021429800587)\n",
      "Model 4 fold 8 fitting finished in 512.399s\n",
      "Model 4 fold 9\n",
      "best round 3609\n",
      "('Score: ', 0.55406572583651448)\n",
      "Model 4 fold 9 fitting finished in 511.742s\n",
      "Model 4 fold 10\n",
      "best round 3703\n",
      "('Score: ', 0.55650966325996032)\n",
      "Model 4 fold 10 fitting finished in 503.420s\n",
      "Score for model 4 is 0.541847\n",
      "Model 5: LGBMClassifier(boosting_type='gbdt', colsample_bytree=0.886827, drop_rate=0.1,\n",
      "        is_unbalance=False, learning_rate=0.01, max_bin=297, max_depth=-1,\n",
      "        max_drop=50, min_child_samples=222, min_child_weight=5,\n",
      "        min_split_gain=0, n_estimators=100000, nthread=-1, num_leaves=29,\n",
      "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
      "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
      "        skip_drop=0.5, subsample=0.884196, subsample_for_bin=50000,\n",
      "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)\n",
      "Model 5 fold 1\n",
      "best round 4238\n",
      "('Score: ', 0.55258825648576493)\n",
      "Model 5 fold 1 fitting finished in 588.645s\n",
      "Model 5 fold 2\n",
      "best round 4185\n",
      "('Score: ', 0.54086107364378555)\n",
      "Model 5 fold 2 fitting finished in 541.120s\n",
      "Model 5 fold 3\n",
      "best round 4394\n",
      "('Score: ', 0.52044822913069944)\n",
      "Model 5 fold 3 fitting finished in 585.535s\n",
      "Model 5 fold 4\n",
      "best round 4247\n",
      "('Score: ', 0.5366830517646064)\n",
      "Model 5 fold 4 fitting finished in 557.487s\n",
      "Model 5 fold 5\n",
      "best round 3886\n",
      "('Score: ', 0.52782961783394422)\n",
      "Model 5 fold 5 fitting finished in 529.469s\n",
      "Model 5 fold 6\n",
      "best round 3386\n",
      "('Score: ', 0.5384682114827678)\n",
      "Model 5 fold 6 fitting finished in 469.637s\n",
      "Model 5 fold 7\n",
      "best round 3588\n",
      "('Score: ', 0.54952127169900433)\n",
      "Model 5 fold 7 fitting finished in 497.457s\n",
      "Model 5 fold 8\n",
      "best round 3601\n",
      "('Score: ', 0.54013410435695042)\n",
      "Model 5 fold 8 fitting finished in 489.376s\n",
      "Model 5 fold 9\n",
      "best round 3343\n",
      "('Score: ', 0.55404447921854305)\n",
      "Model 5 fold 9 fitting finished in 471.219s\n",
      "Model 5 fold 10\n",
      "best round 3678\n",
      "('Score: ', 0.55709600038904639)\n",
      "Model 5 fold 10 fitting finished in 503.741s\n",
      "Score for model 5 is 0.541767\n",
      "Score for blended models is 0.541830\n"
     ]
    }
   ],
   "source": [
    "lgb_params = [lgb.LGBMClassifier(num_leaves = 30,\n",
    "                                min_child_samples = 124,\n",
    "                                colsample_bytree = 0.853059,\n",
    "                                subsample = 0.925184,\n",
    "                                max_bin = 295),\n",
    "             lgb.LGBMClassifier(num_leaves = 24,\n",
    "                                min_child_samples = 124,\n",
    "                                colsample_bytree = 0.920628,\n",
    "                                subsample = 0.871552,\n",
    "                                max_bin = 288),\n",
    "             lgb.LGBMClassifier(num_leaves = 26,\n",
    "                                min_child_samples = 153,\n",
    "                                colsample_bytree = 0.815477,\n",
    "                                subsample = 0.932513,\n",
    "                                max_bin = 286),\n",
    "             lgb.LGBMClassifier(num_leaves = 29,\n",
    "                                min_child_samples = 164,\n",
    "                                colsample_bytree = 0.957067,\n",
    "                                subsample = 0.840632,\n",
    "                                max_bin = 285),\n",
    "             lgb.LGBMClassifier(num_leaves = 29,\n",
    "                                min_child_samples = 222,\n",
    "                                colsample_bytree = 0.886827,\n",
    "                                subsample = 0.884196,\n",
    "                                max_bin = 297)]\n",
    "\n",
    "#  \tnum_leaves \tmin_child_samples \tmax_bin \tcolsample_bytree \tsubsample \tscore\n",
    "# 9 \t30.523147 \t124.751807 \t\t\t295.175031 \t0.853059 \t\t\t0.925184 \t-0.548472\n",
    "# 2 \t24.169263 \t124.643095 \t\t\t288.749046 \t0.920628 \t\t\t0.871552 \t-0.548643\n",
    "# 4 \t26.370628 \t153.236247 \t\t\t286.712464 \t0.815477 \t\t\t0.932513 \t-0.549005\n",
    "# 24 \t29.601053 \t164.967606 \t\t\t285.206768 \t0.957067 \t\t\t0.840632 \t-0.549058\n",
    "# 17 \t29.821756 \t222.429922 \t\t\t297.395617 \t0.886827 \t\t\t0.884196 \t-0.549511\n",
    "\n",
    "(train_blend_x_gbm,\n",
    " test_blend_x_gbm,\n",
    " blend_scores_gbm,\n",
    " best_rounds_gbm) = lgbm_blend(lgb_params, \n",
    "                               train_X, train_y, \n",
    "                               test_X,\n",
    "                               10,\n",
    "                               500) #as the learning rate decreases the number of stopping rounds need to be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54197122  0.54179449  0.54177244  0.54184673  0.54176743]\n",
      "[ 3787.8  4556.3  4393.2  3773.1  3854.6]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "\n",
    "name_train_blend = '../blend/train_blend_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend = '../blend/test_blend_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_gbm,axis=0))\n",
    "print (np.mean(best_rounds_gbm,axis=0))\n",
    "\n",
    "np.savetxt(name_train_blend,train_blend_x_gbm, delimiter=\",\")\n",
    "np.savetxt(name_test_blend,test_blend_x_gbm, delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 3, 6, 17, 11, 34, 262468)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LightGBM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_gbm[:,:3])\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
