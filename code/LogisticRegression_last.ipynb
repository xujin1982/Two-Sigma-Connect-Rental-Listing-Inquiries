{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import gc\n",
    "from scipy.stats import skew, boxcox\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import log_loss\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322) (74659, 322) (49352L,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "train_y = np.ravel(pd.read_csv(data_path + 'labels_BrandenMurray.csv'))\n",
    "ntrain = train_X.shape[0]\n",
    "sub_id = test_X.listing_id.values\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 323)\n",
      "(74659, 323)\n"
     ]
    }
   ],
   "source": [
    "time_feature = pd.read_csv(data_path + 'listing_image_time.csv')\n",
    "time_feature.columns = ['listing_id','time_stamp']\n",
    "train_X = train_X.merge(time_feature,on='listing_id',how='left')\n",
    "test_X = test_X.merge(time_feature,on='listing_id',how='left')\n",
    "\n",
    "print train_X.shape\n",
    "print test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "full_data=pd.concat([train_X,test_X])\n",
    "features_to_use = train_X.columns.values\n",
    "\n",
    "skewed_cols = full_data[features_to_use].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "SSL = preprocessing.StandardScaler()\n",
    "skewed_cols = skewed_cols[skewed_cols > 0.25].index.values\n",
    "for skewed_col in skewed_cols:\n",
    "    full_data[skewed_col], lam = boxcox(full_data[skewed_col] - full_data[skewed_col].min() + 1)\n",
    "#     print skewed_col, '\\t', lam\n",
    "for col in features_to_use:\n",
    "    full_data[col] = SSL.fit_transform(full_data[col].values.reshape(-1,1))\n",
    "    train_X[col] = full_data.iloc[:ntrain][col]\n",
    "    test_X[col] = full_data.iloc[ntrain:][col]\n",
    "\n",
    "    \n",
    "del full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 322)\n",
      "(9871, 322)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.607441650321\tC: 0.001\tTime: 0.3min\n",
      "score: 0.602299176254\tC: 0.003\tTime: 0.5min\n",
      "score: 0.600219508974\tC: 0.010\tTime: 0.8min\n",
      "score: 0.599425613119\tC: 0.030\tTime: 1.2min\n",
      "score: 0.599538246159\tC: 0.100\tTime: 1.9min\n",
      "score: 0.600854537522\tC: 0.300\tTime: 2.8min\n",
      "score: 0.60265340933\tC: 1.000\tTime: 4.4min\n",
      "score: 0.604083830029\tC: 3.000\tTime: 5.4min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'lbfgs',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1,3]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_lbfgs = C\n",
    "        best_score = score\n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_ovr_lbfgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.607440212881\tC: 0.001\tTime: 0.1min\n",
      "score: 0.60229957472\tC: 0.003\tTime: 0.1min\n",
      "score: 0.600224624787\tC: 0.010\tTime: 0.3min\n",
      "score: 0.599436395677\tC: 0.030\tTime: 0.9min\n",
      "score: 0.599522410698\tC: 0.100\tTime: 2.1min\n",
      "score: 0.600747665864\tC: 0.300\tTime: 4.3min\n",
      "score: 0.602525576646\tC: 1.000\tTime: 8.4min\n",
      "score: 0.603449824267\tC: 3.000\tTime: 13.6min\n",
      "score: 0.604114104\tC: 10.000\tTime: 14.0min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'sag',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1,3]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_sag = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_ovr_sag)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.607439473809\tC: 0.001\tTime: 0.4min\n",
      "score: 0.602297376877\tC: 0.003\tTime: 0.5min\n",
      "score: 0.600221208788\tC: 0.010\tTime: 0.6min\n",
      "score: 0.599431156636\tC: 0.030\tTime: 0.9min\n",
      "score: 0.599543550764\tC: 0.100\tTime: 1.2min\n",
      "score: 0.600847368746\tC: 0.300\tTime: 1.7min\n",
      "score: 0.602621046919\tC: 1.000\tTime: 2.8min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'newton-cg',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_newton = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_ovr_newton)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.637789283435\tC: 0.001\tTime: 0.1min\n",
      "score: 0.61187146867\tC: 0.003\tTime: 0.2min\n",
      "score: 0.60219040783\tC: 0.010\tTime: 0.3min\n",
      "score: 0.599778339024\tC: 0.030\tTime: 0.4min\n",
      "score: 0.599588403551\tC: 0.100\tTime: 0.6min\n",
      "score: 0.600837392602\tC: 0.300\tTime: 0.9min\n",
      "score: 0.602611227563\tC: 1.000\tTime: 1.2min\n",
      "C:0.100\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'ovr',solver = 'liblinear',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_ovr_liblinear = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_ovr_liblinear)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.599532004941\tC: 0.001\tTime: 0.3min\n",
      "score: 0.596628363578\tC: 0.003\tTime: 0.4min\n",
      "score: 0.595804094488\tC: 0.010\tTime: 0.7min\n",
      "score: 0.595786124507\tC: 0.030\tTime: 1.1min\n",
      "score: 0.596885316922\tC: 0.100\tTime: 1.6min\n",
      "score: 0.598492150375\tC: 0.300\tTime: 2.6min\n",
      "score: 0.600523460812\tC: 1.000\tTime: 4.0min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'lbfgs',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_lbfgs = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_lbfgs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.599530446452\tC: 0.001\tTime: 0.1min\n",
      "score: 0.596635438637\tC: 0.003\tTime: 0.2min\n",
      "score: 0.59580380439\tC: 0.010\tTime: 0.5min\n",
      "score: 0.595785644181\tC: 0.030\tTime: 1.2min\n",
      "score: 0.596844285732\tC: 0.100\tTime: 2.8min\n",
      "score: 0.598410461283\tC: 0.300\tTime: 5.0min\n",
      "score: 0.599961322512\tC: 1.000\tTime: 7.3min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'sag',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_sag = C\n",
    "        best_score = score      \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_sag)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.599531042992\tC: 0.001\tTime: 0.4min\n",
      "score: 0.596633476024\tC: 0.003\tTime: 0.5min\n",
      "score: 0.595802096672\tC: 0.010\tTime: 0.6min\n",
      "score: 0.595787123687\tC: 0.030\tTime: 0.9min\n",
      "score: 0.596880941226\tC: 0.100\tTime: 1.5min\n",
      "score: 0.598460730286\tC: 0.300\tTime: 2.3min\n",
      "score: 0.6004649384\tC: 1.000\tTime: 3.6min\n",
      "C:0.030\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class = 'multinomial',solver = 'newton-cg',\n",
    "                            n_jobs = 6, max_iter=10000000,tol = 1e-4,\n",
    "                            random_state = seed)\n",
    "best_score = 100\n",
    "for C in [1e-3,3e-3,1e-2,3e-2,1e-1,3e-1,1]:\n",
    "    start = time.time()\n",
    "    logreg.set_params(**{'C': C})\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_y = logreg.predict_proba(X_val)\n",
    "    score = log_loss(y_val, pred_y)\n",
    "    if score < best_score:\n",
    "        C_multinomial_newton = C\n",
    "        best_score = score    \n",
    "    print 'score: {0}\\tC: {1:.3f}\\tTime: {2:.1f}min'.format(score, C, (time.time()-start)/60)\n",
    "print 'C:{0:.3f}'.format(C_multinomial_newton)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LR_blend(est, train_x, train_y, test_x, fold):\n",
    "    N_params = len(est)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "    \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros((fold,N_params))\n",
    "    best_rounds = np.zeros((fold, N_params))    \n",
    "    \n",
    "    for j, ester in enumerate(est):\n",
    "        print (\"Model %d:\" %(j+1))\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "\n",
    "            \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]            \n",
    "            \n",
    "\n",
    "            ester.fit(train_x_fold,train_y_fold)\n",
    "            \n",
    "            val_y_predict_fold = ester.predict_proba(val_x_fold)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score            \n",
    "            \n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = ester.predict_proba(test_x)\n",
    "            \n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))            \n",
    "\n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 5 folds\n",
      "Model 1:\n",
      "Model 1 fold 1\n",
      "('Score: ', 0.55651923438442019)\n",
      "Model 1 fold 1 fitting finished in 61.522s\n",
      "Model 1 fold 2\n",
      "('Score: ', 0.56003855948810499)\n",
      "Model 1 fold 2 fitting finished in 62.697s\n",
      "Model 1 fold 3\n",
      "('Score: ', 0.57988217922476992)\n",
      "Model 1 fold 3 fitting finished in 64.531s\n",
      "Model 1 fold 4\n",
      "('Score: ', 0.59871670602243976)\n",
      "Model 1 fold 4 fitting finished in 61.646s\n",
      "Model 1 fold 5\n",
      "('Score: ', 0.60288089749911122)\n",
      "Model 1 fold 5 fitting finished in 66.374s\n",
      "Score for model 1 is 0.579608\n",
      "Score for blended models is 0.579608\n"
     ]
    }
   ],
   "source": [
    "est = [LogisticRegression(C = 0.030,\n",
    "                          solver = 'lbfgs',\n",
    "                          multi_class = 'multinomial',\n",
    "                          n_jobs = 6, max_iter=10000000,tol = 1e-7,\n",
    "                          random_state = seed),]\n",
    "\n",
    "(train_blend_x_LR,\n",
    " test_blend_x_LR_mean,\n",
    " test_blend_x_LR_gmean,\n",
    " blend_scores_LR,\n",
    " best_rounds_LR) = LR_blend(est, \n",
    "                             train_X, train_y, \n",
    "                             test_X,\n",
    "                             5) #as the learning rate decreases the number of stopping rounds need to be increased\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57960752]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_LR_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_LR_mean_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_LR_gmean_last_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_LR,axis=0))\n",
    "# print (np.mean(best_rounds_LR,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_LR, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_LR_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_LR_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_name = '../output/sub_LR_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_LR_mean[:,-3:])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.99825948e-01,   4.14264109e-01,   8.59099439e-02],\n",
       "       [  9.61754449e-01,   3.54476789e-02,   2.79787173e-03],\n",
       "       [  8.95388169e-01,   8.92753393e-02,   1.53364921e-02],\n",
       "       ..., \n",
       "       [  8.98833551e-01,   8.57164167e-02,   1.54500320e-02],\n",
       "       [  9.87695048e-01,   1.21304643e-02,   1.74487682e-04],\n",
       "       [  6.25250605e-01,   3.37126019e-01,   3.76233760e-02]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_LR_mean[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52045174,  0.39842677,  0.08112149,  0.52045357,  0.39843122,\n",
       "        0.0811152 ,  0.52045527,  0.39843378,  0.08111095,  0.51907442,\n",
       "        0.3980157 ,  0.08290988,  0.49980675,  0.41427613,  0.08591712,\n",
       "        0.49982491,  0.41426329,  0.08591179,  0.49982595,  0.41426411,\n",
       "        0.08590994])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_LR_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [ 0.5998131   0.59981265  0.59981346  0.59993284  0.59603874  0.59603833\n",
    "#   0.59603854]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
