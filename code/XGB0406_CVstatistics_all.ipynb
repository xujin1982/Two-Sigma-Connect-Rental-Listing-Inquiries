{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn import model_selection,ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "import random\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, LabelBinarizer, MultiLabelBinarizer,LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_train(train,y,max_depth = 6,min_child_weight = 1,colsample_bytree = 1, subsample = 1, gamma = 0 , verbose_eval = None,\n",
    "            seed = 0, early_stop = 50, nfold = 5, eta=0.3):\n",
    "    xgtrain = xgb.DMatrix(train, label=y)\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=0\n",
    "    params['eta'] = eta\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "#     params['booster'] = 'dart'\n",
    "#     params['rate_drop'] = 0.1\n",
    "#     params['skip_drop'] = 0.5\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=nfold,\n",
    "        metrics = 'mlogloss', verbose_eval = verbose_eval,\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(early_stop)]\n",
    "    )\n",
    "\n",
    "    return cv_result['test-mlogloss-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CV_st(train,test,feature1,feature2):\n",
    "    index=list(range(train.shape[0]))\n",
    "    random.shuffle(index)\n",
    "    kf = KFold(n_splits=5,shuffle=True, random_state=0)\n",
    "    \n",
    "    # median feature names\n",
    "    features_tmp = []\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_median')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].median().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[0]] = f_low\n",
    "    train[features_tmp[1]] = f_medium\n",
    "    train[features_tmp[2]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].median().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[0]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[1]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[2]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "\n",
    "    # mean feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_mean')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[3]] = f_low\n",
    "    train[features_tmp[4]] = f_medium\n",
    "    train[features_tmp[5]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[3]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[4]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[5]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # max feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_max')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].max().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[6]] = f_low\n",
    "    train[features_tmp[7]] = f_medium\n",
    "    train[features_tmp[8]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].max().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[6]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[7]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[8]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # min feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_min')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].min().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[9]] = f_low\n",
    "    train[features_tmp[10]] = f_medium\n",
    "    train[features_tmp[11]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].min().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[9]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[10]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[11]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "\n",
    "#     # std feature names\n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_low_std') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_medium_std') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_high_std')\n",
    "    \n",
    "#     # train data \n",
    "#     f_low=pd.Series([np.nan]*len(train))\n",
    "#     f_medium=pd.Series([np.nan]*len(train))\n",
    "#     f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "#     for train_index, test_index in kf.split(index):\n",
    "#         tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].std().\\\n",
    "#                 reset_index().rename(columns={feature2:'new'})\n",
    "#         f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                                             on=feature1,how='left')['new'].values   \n",
    "#     train[features_tmp[12]] = f_low\n",
    "#     train[features_tmp[13]] = f_medium\n",
    "#     train[features_tmp[14]] = f_high\n",
    "\n",
    "#     # test data\n",
    "#     tmp = train.groupby(['interest_level',feature1])[feature2].std().\\\n",
    "#             reset_index().rename(columns={feature2:'new'})    \n",
    "#     test[features_tmp[12]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[13]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[14]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                        on=feature1,how='left')['new'].values \n",
    "    \n",
    "#     # var feature names\n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_low_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_medium_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_high_var')\n",
    "    \n",
    "#     # train data \n",
    "#     f_low=pd.Series([np.nan]*len(train))\n",
    "#     f_medium=pd.Series([np.nan]*len(train))\n",
    "#     f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "#     for train_index, test_index in kf.split(index):\n",
    "#         tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#                 reset_index().rename(columns={feature2:'new'})\n",
    "#         f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                                             on=feature1,how='left')['new'].values   \n",
    "#     train[features_tmp[15]] = f_low\n",
    "#     train[features_tmp[16]] = f_medium\n",
    "#     train[features_tmp[17]] = f_high\n",
    "\n",
    "#     # test data\n",
    "#     tmp = train.groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#             reset_index().rename(columns={feature2:'new'})    \n",
    "#     test[features_tmp[15]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[16]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[17]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                        on=feature1,how='left')['new'].values \n",
    "    \n",
    "#     # ratio/diff feature\n",
    "#     cols = features_tmp[:]\n",
    "# #     features_tmp = []\n",
    "#     for col in cols:\n",
    "#         new_feature = col+'_ratio'\n",
    "#         train[new_feature] = train[col] / train[feature2]\n",
    "#         test[new_feature] = test[col] / test[feature2]\n",
    "#         features_tmp.append(new_feature)\n",
    "        \n",
    "    print feature1,' vs ', feature2,'Done!'\n",
    "    return train,test,features_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#input data\n",
    "train_df=pd.read_json('../input/train.json').reset_index(drop = True)\n",
    "test_df=pd.read_json('../input/test.json').reset_index(drop = True)\n",
    "test_df.loc[test_df.bathrooms == 112.0,'bathrooms'] = 1.5    \n",
    "test_df.loc[test_df.bathrooms == 20.0,'bathrooms'] = 2.0\n",
    "test_df.loc[test_df.listing_id == 7220763,'bedrooms'] = 3\n",
    "test_df.loc[test_df.listing_id == 7047074,'bedrooms'] = 6\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    df[\"num_photo_count\"] = df[\"photos\"].apply(len)\n",
    "    df[\"street_address\"] = df['street_address'].apply(fmt)\n",
    "    df[\"display_address\"] = df[\"display_address\"].apply(fmt)\n",
    "    df[\"num_desc_wordcount\"] = df[\"description\"].apply(len)\n",
    "    df[\"num_pricePerBed\"] = df['price'] / df['bedrooms']\n",
    "    df[\"num_pricePerBath\"] = df['price'] / df['bathrooms']\n",
    "    df[\"num_pricePerRoom\"] = df['price'] / (df['bedrooms'] + df['bathrooms'])\n",
    "    df[\"num_bedPerBath\"] = df['bedrooms'] / df['bathrooms']\n",
    "    df[\"num_bedBathDiff\"] = df['bedrooms'] - df['bathrooms']\n",
    "    df[\"num_bedBathSum\"] = df[\"bedrooms\"] + df['bathrooms']\n",
    "    df[\"num_bedsPerc\"] = df[\"bedrooms\"] / (df['bedrooms'] + df['bathrooms'])\n",
    "\n",
    "    df = df.fillna(-1).replace(np.inf, -1)\n",
    "    return df\n",
    "\n",
    "# Add common features\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df) \n",
    "\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df['num_desc_length_null'] = (train_df.description.str.len()==0).astype(float)\n",
    "test_df['num_desc_length_null'] = (test_df.description.str.len()==0).astype(float)\n",
    "    \n",
    "features_to_use=[\n",
    "    \"latitude\", \"longitude\",\"num_pricePerBed\",\n",
    "    'num_bedBathSum','num_pricePerBath','num_pricePerRoom','num_bedPerBath',\n",
    "    'num_bedBathDiff','num_bedsPerc',\n",
    "    \"num_photo_count\", \"num_features\", \"num_desc_wordcount\",'num_desc_length_null',\n",
    "    \"listing_id\"]\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Location features: Latitude, longitude\n",
    "precision = 3\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2)\n",
    "train_df['num_dist_from_center'] = x.values\n",
    "x = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2)\n",
    "test_df['num_dist_from_center'] = x.values\n",
    "train_df['position'] = train_df.longitude.round(precision).astype(str) + '_' + train_df.latitude.round(precision).astype(str)\n",
    "test_df['position'] = test_df.longitude.round(precision).astype(str) + '_' + test_df.latitude.round(precision).astype(str)\n",
    "\n",
    "new_feature = ['num_dist_from_center']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Degree of \"outlierness\"\n",
    "OutlierAggregated = (train_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated2 = (test_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms < 1).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms < 1).astype(float)\n",
    "x = np.abs((train_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.abs((test_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.log1p(train_df.price/(train_df.bedrooms.clip(1,3) + train_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.log1p(test_df.price/(test_df.bedrooms.clip(1,3) + test_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "train_df['num_OutlierAggregated'] = OutlierAggregated.values\n",
    "test_df['num_OutlierAggregated'] = OutlierAggregated2.values\n",
    "\n",
    "\n",
    "new_feature = ['num_OutlierAggregated']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Density in unique locations at given precision\n",
    "vals = train_df['position'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df['num_pos_density'] = train_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df['num_pos_density'] = test_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "# Building null\n",
    "train_df['num_building_null'] = (train_df.building_id=='0').astype(float)\n",
    "test_df['num_building_null'] = (test_df.building_id=='0').astype(float)\n",
    "\n",
    "\n",
    "new_feature = ['num_pos_density','num_building_null']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Creation time features\n",
    "train_df['created'] = pd.to_datetime(train_df.created)\n",
    "train_df['num_created_weekday'] = train_df.created.dt.dayofweek.astype(float)\n",
    "train_df['num_created_weekofyear'] = train_df.created.dt.weekofyear\n",
    "train_df['num_created_day'] = train_df.created.dt.day\n",
    "train_df['num_created_month'] = train_df.created.dt.month\n",
    "train_df['num_created_hour'] = train_df.created.dt.hour\n",
    "  \n",
    "test_df['created'] = pd.to_datetime(test_df.created)\n",
    "test_df['num_created_weekday'] = test_df.created.dt.dayofweek\n",
    "test_df['num_created_weekofyear'] = test_df.created.dt.weekofyear\n",
    "test_df['num_created_day'] = test_df.created.dt.day\n",
    "test_df['num_created_month'] = test_df.created.dt.month\n",
    "test_df['num_created_hour'] = test_df.created.dt.hour\n",
    "\n",
    "\n",
    "new_feature = ['num_created_weekday','num_created_weekofyear','num_created_day','num_created_month','num_created_hour']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Bedrooms/Bathrooms/Price\n",
    "train_df['num_bathrooms'] = train_df.bathrooms.clip_upper(4)\n",
    "test_df['num_bathrooms'] = test_df.bathrooms.clip_upper(4)\n",
    "\n",
    "train_df['num_bedrooms'] = train_df.bedrooms.clip_upper(5)\n",
    "test_df['num_bedrooms'] = test_df.bedrooms.clip_upper(5)\n",
    "\n",
    "train_df['num_price'] = train_df.price.clip_upper(10000)\n",
    "test_df['num_price'] = test_df.price.clip_upper(10000)\n",
    "\n",
    "bins = train_df.price.quantile(np.arange(0.05, 1, 0.05))\n",
    "train_df['num_price_q'] = np.digitize(train_df.price, bins)\n",
    "test_df['num_price_q'] = np.digitize(test_df.price, bins)\n",
    "\n",
    "\n",
    "new_feature = ['num_bathrooms','num_bedrooms','num_price','num_price_q']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Composite features based on: \n",
    "# https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths\n",
    "train_df['num_priceXroom'] = (train_df.price / (1 + train_df.bedrooms.clip(1, 4) + 0.5*train_df.bathrooms.clip(0, 2))).values\n",
    "test_df['num_priceXroom'] = (test_df.price / (1 + test_df.bedrooms.clip(1, 4) + 0.5*test_df.bathrooms.clip(0, 2))).values\n",
    "\n",
    "train_df['num_even_bathrooms'] = ((np.round(train_df.bathrooms) - train_df.bathrooms)==0).astype(float)\n",
    "test_df['num_even_bathrooms'] = ((np.round(test_df.bathrooms) - test_df.bathrooms)==0).astype(float)\n",
    "\n",
    "new_feature = ['num_priceXroom','num_even_bathrooms']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\",'position']\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            if f not in features_to_use:\n",
    "                features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dftemp = train_df.copy()\n",
    "for i in ['latitude', 'longitude']:\n",
    "    while(1):\n",
    "        x = dftemp[i].median()\n",
    "        ix = abs(dftemp[i] - x) > 3*dftemp[i].std()\n",
    "        if ix.sum()==0:\n",
    "            break\n",
    "        dftemp.loc[ix, i] = np.nan\n",
    "dftemp = dftemp.loc[dftemp[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n",
    "\n",
    "dfm = DataFrameMapper([(['latitude'], [StandardScaler()]), (['longitude'], [StandardScaler()])])\n",
    "\n",
    "for i in [6]:\n",
    "    pipe_location = make_pipeline(dfm, KMeans(n_clusters=i, random_state=1))\n",
    "    pipe_location.fit(dftemp);\n",
    "    train_df['location_'+str(i)] = pipe_location.predict(train_df).astype(str)\n",
    "    test_df['location_'+str(i)] = pipe_location.predict(test_df).astype(str)\n",
    "for i in train_df.location_6.unique():\n",
    "    f = 'num_location_6_'+str(i)\n",
    "    train_df[f] = (train_df.location_6==i).astype(float)\n",
    "    test_df[f] = (test_df.location_6==i).astype(float)\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "    \n",
    "    \n",
    "train_df['tmp_bathrooms'] = train_df.bathrooms.clip_upper(2)\n",
    "test_df['tmp_bathrooms'] = test_df.bathrooms.clip_upper(2)\n",
    "train_df['tmp_bedrooms'] = train_df.bedrooms.clip_upper(4)\n",
    "test_df['tmp_bedrooms'] = test_df.bedrooms.clip_upper(4)\n",
    "train_df['roomcal'] = train_df.tmp_bedrooms.astype(str) + '_' + train_df.tmp_bathrooms.astype(str)    \n",
    "test_df['roomcal'] = test_df.tmp_bedrooms.astype(str) + '_' + test_df.tmp_bathrooms.astype(str)    \n",
    "\n",
    "room_lb = LabelBinarizer()\n",
    "room_lb.fit(train_df['roomcal'])\n",
    "room_col = ['num_room_type_' + str(x) for x in range(len(train_df['roomcal'].unique()))]\n",
    "for f in room_col:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "train_df = train_df.join(pd.DataFrame(room_lb.transform(train_df['roomcal']),columns=room_col,index=train_df.index))\n",
    "test_df = test_df.join(pd.DataFrame(room_lb.transform(test_df['roomcal']),columns=room_col,index=test_df.index))\n",
    "\n",
    "tmp = train_df.groupby(['roomcal','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "\n",
    "test_df.loc[27462,'num_6_median_price'] =  7200.0\n",
    "\n",
    "train_df['num_6_price_ratio'] = train_df['num_price'] / train_df['num_6_median_price']\n",
    "train_df['num_6_price_diff'] = train_df['num_price'] - train_df['num_6_median_price']\n",
    "test_df['num_6_price_ratio'] = test_df['num_price'] / test_df['num_6_median_price']\n",
    "test_df['num_6_price_diff'] = test_df['num_price'] - test_df['num_6_median_price']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price','num_6_price_ratio','num_6_price_diff']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = train_df.groupby(['num_bedrooms','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price_bedroom'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "\n",
    "train_df['num_6_price_ratio_bedroom'] = train_df['num_price'] / train_df['num_6_median_price_bedroom']\n",
    "train_df['num_6_price_diff_bedroom'] = train_df['num_price'] - train_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_ratio_bedroom'] = test_df['num_price'] / test_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_diff_bedroom'] = test_df['num_price'] - test_df['num_6_median_price_bedroom']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price_bedroom','num_6_price_ratio_bedroom','num_6_price_diff_bedroom']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_binary_features(df):\n",
    "    bows = {\n",
    "        \"dogs\": (\"dogs\", \"dog\",'pet friendly','pets'),\n",
    "        \"cats\": (\"cats\",'pet friendly','pets'),\n",
    "        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n",
    "        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n",
    "        \"furnished\": (\"furnished\",'equipped'),\n",
    "        \"parquet\": (\"parquet\", \"hardwood\"),\n",
    "        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n",
    "        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n",
    "        \"laundry\": (\"laundry\", \"lndry\"),\n",
    "        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n",
    "        \"transport\": (\"train\", \"subway\", \"transport\"),\n",
    "        \"parking\": (\"parking\",),\n",
    "        \"utilities\": (\"utilities\", \"heat water\", \"water included\"),\n",
    "        'fireplace': ('fireplace','fireplaces'),\n",
    "        'elevator': ('elevator'),\n",
    "        'pool':('pool'),\n",
    "        'loft':('loft'),\n",
    "        'luxury':('luxury','valet'),\n",
    "        'marble':('marble'),\n",
    "        'onemounthfree': ('1 month free','one month free'),\n",
    "        'washer':('washer','dryer')\n",
    "    }\n",
    "\n",
    "    def indicator(bow):\n",
    "        return lambda s: int(any([x in s for x in bow]))\n",
    "\n",
    "    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n",
    "    for key in bows:\n",
    "        tmp_key = \"feature_\" + key\n",
    "        df[tmp_key] = features.apply(indicator(bows[key]))\n",
    "        if tmp_key not in features_to_use:\n",
    "            features_to_use.append(tmp_key)\n",
    "    return df\n",
    "\n",
    "# Create binarized features\n",
    "train_df = create_binary_features(train_df)\n",
    "test_df = create_binary_features(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322)\n",
      "(74659, 322)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "print train_X_0322.shape\n",
    "print test_X_0322.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment = [\n",
    "    'building_id_mean_med','building_id_mean_high', \n",
    "    'manager_id_mean_med','manager_id_mean_high',\n",
    "    'median_price_bed', 'ratio_bed',\n",
    "       'compound', 'neg', 'neu', 'pos', 'street',\n",
    "       'avenue', 'east', 'west', 'north', 'south', 'other_address',\n",
    "       'Zero_building_id', 'top_10_building', 'top_25_building',\n",
    "       'top_5_building', 'top_50_building', 'top_1_building',\n",
    "       'top_2_building', 'top_15_building', 'top_20_building',\n",
    "       'top_30_building','listing_id'\n",
    "]\n",
    "\n",
    "train_df = train_df.merge(train_X_0322[sentiment],on='listing_id', how='left')\n",
    "test_df = test_df.merge(test_X_0322[sentiment],on='listing_id', how='left')\n",
    "\n",
    "for f in sentiment:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','price')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','price')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_dist_from_center Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_dist_from_center')\n",
    "\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_created_hour Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_created_hour')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_created_hour Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_created_hour')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_desc_wordcount Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_desc_wordcount')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff_bedroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_6_price_diff_bedroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_bedrooms Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_bedrooms')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in new_feature:\n",
    "    features_to_use.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_bedrooms Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_bedrooms')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  building_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','building_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  building_id_mean_high Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','building_id_mean_high')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  manager_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','manager_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  manager_id_mean_high Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','manager_id_mean_high')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_nofee  vs  num_price Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'feature_nofee','num_price')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_price_q  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'num_price_q','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'num_building_null','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  feature_nofee Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','feature_nofee')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  manager_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','manager_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'position','price')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  pos Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','pos')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  median_price_bed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','median_price_bed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerBed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerBed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pos_density Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pos_density')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  longitude Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','longitude')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  latitude Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','latitude')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_ratio Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_ratio')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_features Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_features')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  ratio_bed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','ratio_bed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff_bedroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_photo_count Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_photo_count')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerBath Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerBath')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerRoom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerRoom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manager_id_num_bedrooms_medium_max',\n",
       " 'manager_id_num_bedrooms_high_max',\n",
       " 'manager_id_num_bedrooms_low_min',\n",
       " 'manager_id_num_bedrooms_medium_min',\n",
       " 'manager_id_num_bedrooms_high_min']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_use[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 223) (74659, 223)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n",
    "\n",
    "train_X.replace(np.inf, np.nan)\n",
    "test_X.replace(np.inf, np.nan)\n",
    "\n",
    "train_X.loc[:,'num_nan'] = train_X.isnull().sum(axis=1)\n",
    "test_X.loc[:,'num_nan'] = test_X.isnull().sum(axis=1)\n",
    "\n",
    "target_num_map = {'high':2, 'medium':1, 'low':0}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print train_X.shape, test_X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_id_num_6_price_diff_low_median</th>\n",
       "      <th>manager_id_num_6_price_diff_medium_median</th>\n",
       "      <th>manager_id_num_6_price_diff_high_median</th>\n",
       "      <th>manager_id_num_6_price_diff_low_mean</th>\n",
       "      <th>manager_id_num_6_price_diff_medium_mean</th>\n",
       "      <th>manager_id_num_6_price_diff_high_mean</th>\n",
       "      <th>manager_id_num_6_price_diff_low_max</th>\n",
       "      <th>manager_id_num_6_price_diff_medium_max</th>\n",
       "      <th>manager_id_num_6_price_diff_high_max</th>\n",
       "      <th>manager_id_num_6_price_diff_low_min</th>\n",
       "      <th>manager_id_num_6_price_diff_medium_min</th>\n",
       "      <th>manager_id_num_6_price_diff_high_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.735849</td>\n",
       "      <td>30.277778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>875.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1700.0</td>\n",
       "      <td>-700.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1052.5</td>\n",
       "      <td>-3121.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1197.735714</td>\n",
       "      <td>-3121.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>-3121.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1150.0</td>\n",
       "      <td>-3121.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-50.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-322.5</td>\n",
       "      <td>223.050847</td>\n",
       "      <td>-43.225000</td>\n",
       "      <td>-327.500</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>3025.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>-1505.0</td>\n",
       "      <td>-2510.0</td>\n",
       "      <td>-1340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>-550.0</td>\n",
       "      <td>255.252066</td>\n",
       "      <td>-207.264706</td>\n",
       "      <td>-69.375</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>-2100.0</td>\n",
       "      <td>-2516.5</td>\n",
       "      <td>-965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_id_num_6_price_diff_low_median  \\\n",
       "0                                   -75.0   \n",
       "1                                  1052.5   \n",
       "2                                   -50.0   \n",
       "3                                   135.0   \n",
       "4                                   -55.0   \n",
       "\n",
       "   manager_id_num_6_price_diff_medium_median  \\\n",
       "0                                      -25.0   \n",
       "1                                    -3121.5   \n",
       "2                                     -125.0   \n",
       "3                                     -300.0   \n",
       "4                                        NaN   \n",
       "\n",
       "   manager_id_num_6_price_diff_high_median  \\\n",
       "0                                      NaN   \n",
       "1                                      NaN   \n",
       "2                                   -322.5   \n",
       "3                                   -550.0   \n",
       "4                                      NaN   \n",
       "\n",
       "   manager_id_num_6_price_diff_low_mean  \\\n",
       "0                             -7.735849   \n",
       "1                           1197.735714   \n",
       "2                            223.050847   \n",
       "3                            255.252066   \n",
       "4                             15.000000   \n",
       "\n",
       "   manager_id_num_6_price_diff_medium_mean  \\\n",
       "0                                30.277778   \n",
       "1                             -3121.500000   \n",
       "2                               -43.225000   \n",
       "3                              -207.264706   \n",
       "4                                      NaN   \n",
       "\n",
       "   manager_id_num_6_price_diff_high_mean  manager_id_num_6_price_diff_low_max  \\\n",
       "0                                    NaN                                875.0   \n",
       "1                                    NaN                               4300.0   \n",
       "2                               -327.500                               3300.0   \n",
       "3                                -69.375                               4100.0   \n",
       "4                                    NaN                               1000.0   \n",
       "\n",
       "   manager_id_num_6_price_diff_medium_max  \\\n",
       "0                                   900.0   \n",
       "1                                 -3121.5   \n",
       "2                                  3025.0   \n",
       "3                                  1100.0   \n",
       "4                                     NaN   \n",
       "\n",
       "   manager_id_num_6_price_diff_high_max  manager_id_num_6_price_diff_low_min  \\\n",
       "0                                   NaN                              -1700.0   \n",
       "1                                   NaN                              -1150.0   \n",
       "2                                 495.0                              -1505.0   \n",
       "3                                2950.0                              -2100.0   \n",
       "4                                   NaN                               -300.0   \n",
       "\n",
       "   manager_id_num_6_price_diff_medium_min  \\\n",
       "0                                  -700.0   \n",
       "1                                 -3121.5   \n",
       "2                                 -2510.0   \n",
       "3                                 -2516.5   \n",
       "4                                     NaN   \n",
       "\n",
       "   manager_id_num_6_price_diff_high_min  \n",
       "0                                   NaN  \n",
       "1                                   NaN  \n",
       "2                               -1340.0  \n",
       "3                                -965.0  \n",
       "4                                   NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[new_feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[0]\ttrain-mlogloss:0.9081+0.000439152\ttest-mlogloss:0.914806+0.00097818\n",
      "[5]\ttrain-mlogloss:0.600289+0.000821237\ttest-mlogloss:0.631917+0.00300072\n",
      "[10]\ttrain-mlogloss:0.526073+0.000777907\ttest-mlogloss:0.575912+0.00432575\n",
      "[15]\ttrain-mlogloss:0.489517+0.00182507\ttest-mlogloss:0.556177+0.00400821\n",
      "[20]\ttrain-mlogloss:0.464523+0.00310209\ttest-mlogloss:0.546892+0.00445571\n",
      "[25]\ttrain-mlogloss:0.443435+0.00276597\ttest-mlogloss:0.541393+0.00449292\n",
      "[30]\ttrain-mlogloss:0.425666+0.00255616\ttest-mlogloss:0.537842+0.00496247\n",
      "[35]\ttrain-mlogloss:0.410124+0.00238644\ttest-mlogloss:0.535211+0.00466594\n",
      "[40]\ttrain-mlogloss:0.396522+0.0016213\ttest-mlogloss:0.533484+0.00509716\n",
      "[45]\ttrain-mlogloss:0.382935+0.00242961\ttest-mlogloss:0.532501+0.00489936\n",
      "[50]\ttrain-mlogloss:0.371589+0.00210863\ttest-mlogloss:0.531685+0.00502912\n",
      "[55]\ttrain-mlogloss:0.360453+0.00267513\ttest-mlogloss:0.531231+0.00522328\n",
      "[60]\ttrain-mlogloss:0.349661+0.00169442\ttest-mlogloss:0.530647+0.0053908\n",
      "[65]\ttrain-mlogloss:0.339191+0.00225167\ttest-mlogloss:0.530363+0.00548772\n",
      "[70]\ttrain-mlogloss:0.329766+0.0025686\ttest-mlogloss:0.530272+0.00538056\n",
      "[75]\ttrain-mlogloss:0.320544+0.00252402\ttest-mlogloss:0.530326+0.00547542\n",
      "[80]\ttrain-mlogloss:0.311336+0.00318315\ttest-mlogloss:0.530211+0.00570428\n",
      "[85]\ttrain-mlogloss:0.303211+0.00278038\ttest-mlogloss:0.530465+0.00572945\n",
      "[90]\ttrain-mlogloss:0.294969+0.00232601\ttest-mlogloss:0.530828+0.0058243\n",
      "[95]\ttrain-mlogloss:0.286858+0.00298929\ttest-mlogloss:0.531275+0.00580822\n",
      "Stopping. Best iteration:\n",
      "[80]\ttrain-mlogloss:0.311336+0.00318315\ttest-mlogloss:0.530211+0.00570428\n",
      "\n",
      "0.5302112\n",
      "\n",
      "Training :439.08s\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "print cv_train(train_X,train_y,verbose_eval = 5, early_stop = 20)\n",
    "print '\\nTraining :{:0.2f}s'.format(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [81]\ttrain-mlogloss:0.354253+0.00211099\ttest-mlogloss:0.536977+0.00608719 no cv feature\n",
    "# [65]\ttrain-mlogloss:0.367155+0.00211633\ttest-mlogloss:0.534893+0.0062382 price\n",
    "# [72]\ttrain-mlogloss:0.346812+0.00328497\ttest-mlogloss:0.53305+0.00688662 building_id  vs  price\n",
    "# [64]\ttrain-mlogloss:0.357396+0.00288563\ttest-mlogloss:0.531704+0.00625184 num_dist_from_center\n",
    "# [76]\ttrain-mlogloss:0.331548+0.00150892\ttest-mlogloss:0.530707+0.00719767 num_created_hour\n",
    "# [78]\ttrain-mlogloss:0.327187+0.00326905\ttest-mlogloss:0.530522+0.00662889 building_id  vs  num_created_hour\n",
    "# [69]\ttrain-mlogloss:0.341823+0.0009883\ttest-mlogloss:0.530539+0.00755474 num_desc_wordcount\n",
    "# del [64]\ttrain-mlogloss:0.348393+0.00437276\ttest-mlogloss:0.532399+0.00560396 building_id  vs  num_desc_wordcount\n",
    "# [78]\ttrain-mlogloss:0.322085+0.00255577\ttest-mlogloss:0.530573+0.00706681 num_6_price_diff_bedroom\n",
    "# [76]\ttrain-mlogloss:0.322108+0.00188804\ttest-mlogloss:0.53018+0.00868863 building_id  vs  num_6_price_diff_bedroom\n",
    "# [57]\ttrain-mlogloss:0.35853+0.0039214\ttest-mlogloss:0.530243+0.00654921 bedroom\n",
    "# del [80]\ttrain-mlogloss:0.313073+0.00177185\ttest-mlogloss:0.530923+0.00642305 building_id  vs  bedroom\n",
    "# [80]\ttrain-mlogloss:0.311336+0.00318315\ttest-mlogloss:0.530211+0.00570428 num_6_price_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [82]\ttrain-mlogloss:0.373255+0.00424222\ttest-mlogloss:0.551611+0.00772184 no cv feature\n",
    "# [69]\ttrain-mlogloss:0.364704+0.00323687\ttest-mlogloss:0.536954+0.00792152 price \n",
    "# [67]\ttrain-mlogloss:0.363512+0.0026217\ttest-mlogloss:0.536145+0.0072402 num_dist_from_center\n",
    "# [81]\ttrain-mlogloss:0.335269+0.00262769\ttest-mlogloss:0.534375+0.00789059 num_created_hour\n",
    "# [71]\ttrain-mlogloss:0.349284+0.00352997\ttest-mlogloss:0.534292+0.00880248 num_desc_wordcount\n",
    "# [68]\ttrain-mlogloss:0.349212+0.00212014\ttest-mlogloss:0.532903+0.00747242 num_6_price_ratio_bedroom\n",
    "# [74]\ttrain-mlogloss:0.337387+0.00425913\ttest-mlogloss:0.533063+0.00678103 building_id_mean_med\n",
    "# [83]\ttrain-mlogloss:0.320669+0.00289614\ttest-mlogloss:0.532428+0.00780398 building_id_mean_high\n",
    "# [76]\ttrain-mlogloss:0.330415+0.00167131\ttest-mlogloss:0.531293+0.00759899 'manager_id_mean_med','manager_id_mean_high',\n",
    "# [62]\ttrain-mlogloss:0.348022+0.00308306\ttest-mlogloss:0.5307+0.00586143 building_id  vs  num_6_price_diff_bedroom\n",
    "\n",
    "# del [69]\ttrain-mlogloss:0.331928+0.00270211\ttest-mlogloss:0.531707+0.00621119 building_id  vs  price\n",
    "# del [61]\ttrain-mlogloss:0.349982+0.00391241\ttest-mlogloss:0.537226+0.00635442 feature_nofee  vs  num_price\n",
    "# del [69]\ttrain-mlogloss:0.333043+0.00387723\ttest-mlogloss:0.531759+0.0078066 num_price_q  vs  num_priceXroom\n",
    "# del [64]\ttrain-mlogloss:0.345669+0.00295346\ttest-mlogloss:0.531806+0.00678929 num_building_null  vs  num_priceXroom\n",
    "# del [73]\ttrain-mlogloss:0.326921+0.00344616\ttest-mlogloss:0.532193+0.00673751 building_id  vs  num_priceXroom\n",
    "# del [68]\ttrain-mlogloss:0.342177+0.00136703\ttest-mlogloss:0.531945+0.00650216 feature_nofee\n",
    "# del [78]\ttrain-mlogloss:0.324279+0.00234253\ttest-mlogloss:0.531399+0.00642477 num_bedrooms\n",
    "# del [67]\ttrain-mlogloss:0.343537+0.0019745\ttest-mlogloss:0.533178+0.00505587 pos\n",
    "# del [84]\ttrain-mlogloss:0.323042+0.00215233\ttest-mlogloss:0.533643+0.00763431 median_price_bed\n",
    "# del [66]\ttrain-mlogloss:0.350688+0.00226534\ttest-mlogloss:0.533783+0.00685066 num_pricePerBed\n",
    "# del [76]\ttrain-mlogloss:0.333929+0.00233681\ttest-mlogloss:0.533241+0.00765436 num_pos_density\n",
    "# del  [74]\ttrain-mlogloss:0.336716+0.00472892\ttest-mlogloss:0.533678+0.00700685 longitude\n",
    "# del [70]\ttrain-mlogloss:0.343931+0.00190522\ttest-mlogloss:0.533995+0.00708828 latitude\n",
    "# del [66]\ttrain-mlogloss:0.353195+0.00347634\ttest-mlogloss:0.533698+0.00813442 num_6_price_ratio\n",
    "# del[76]\ttrain-mlogloss:0.335395+0.00269504\ttest-mlogloss:0.534366+0.0072334 num_features\n",
    "# del [70]\ttrain-mlogloss:0.344731+0.0040112\ttest-mlogloss:0.533613+0.00852658 ratio_bed\n",
    "# del [63]\ttrain-mlogloss:0.359234+0.00282059\ttest-mlogloss:0.533871+0.00691217 num_6_price_diff_bedroom\n",
    "# del [84]\ttrain-mlogloss:0.322303+0.00294342\ttest-mlogloss:0.534586+0.0076548 num_priceXroom\n",
    "# del [71]\ttrain-mlogloss:0.347942+0.00429741\ttest-mlogloss:0.535784+0.00840754 num_photo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.976351\n",
      "[20]\tvalidation_0-mlogloss:0.570767\n",
      "[40]\tvalidation_0-mlogloss:0.547269\n",
      "[60]\tvalidation_0-mlogloss:0.539594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.3,\n",
       "       gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=None, n_estimators=62, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=1, subsample=0.7)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=2016)\n",
    "rgr = xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            seed = 0, # use a fixed seed during tuning so we can reproduce the results\n",
    "            learning_rate = 0.2,\n",
    "            n_estimators = 62,\n",
    "            max_depth= 6,\n",
    "            nthread = -1,\n",
    "            colsample_bytree = 0.3,\n",
    "            subsample =0.7,\n",
    "            silent = 1\n",
    "        )\n",
    "rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=20,\n",
    "        verbose=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(rgr, feature_names=X_train.columns, OutputXlsxFile = '../FE/FI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 394)\n",
      "(9871, 394)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "# print X_train.shape\n",
    "# print X_val.shape\n",
    "# # xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[312]\ttrain-mlogloss:0.395492+0.00170774\ttest-mlogloss:0.527702+0.00663205\n",
      "\n",
      "3 \t0.5277018\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[141]\ttrain-mlogloss:0.396861+0.00183942\ttest-mlogloss:0.527621+0.00736431\n",
      "\n",
      "4 \t0.5276214\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[102]\ttrain-mlogloss:0.357724+0.00219303\ttest-mlogloss:0.528282+0.00654119\n",
      "\n",
      "5 \t0.5282822\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[57]\ttrain-mlogloss:0.35853+0.0039214\ttest-mlogloss:0.530243+0.00654921\n",
      "\n",
      "6 \t0.530243\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[59]\ttrain-mlogloss:0.283875+0.00317202\ttest-mlogloss:0.535384+0.00751206\n",
      "\n",
      "7 \t0.5353838\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[34]\ttrain-mlogloss:0.291909+0.00288588\ttest-mlogloss:0.538363+0.00620757\n",
      "\n",
      "8 \t0.538363\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[26]\ttrain-mlogloss:0.267564+0.0038825\ttest-mlogloss:0.541642+0.00515766\n",
      "\n",
      "9 \t0.541642\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[20]\ttrain-mlogloss:0.244491+0.00434577\ttest-mlogloss:0.546812+0.0055212\n",
      "\n",
      "10 \t0.5468122\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[20]\ttrain-mlogloss:0.195892+0.00375725\ttest-mlogloss:0.550402+0.00728209\n",
      "\n",
      "11 \t0.5504024\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[17]\ttrain-mlogloss:0.17067+0.00195634\ttest-mlogloss:0.557329+0.00518613\n",
      "\n",
      "12 \t0.557329\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[15]\ttrain-mlogloss:0.149427+0.00259833\ttest-mlogloss:0.562298+0.00633625\n",
      "\n",
      "13 \t0.5622976\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\ttrain-mlogloss:0.126763+0.00196544\ttest-mlogloss:0.569518+0.00782982\n",
      "\n",
      "14 \t0.5695176\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\ttrain-mlogloss:0.10453+0.00319293\ttest-mlogloss:0.572249+0.00606784\n",
      "\n",
      "15 \t0.5722486\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "for x in [3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= x,\n",
    "#         nthread = -1,\n",
    "#         silent = False\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "    tmp = cv_train(train_X,train_y,max_depth = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# max_depth = train_param\n",
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[186]\ttrain-mlogloss:0.367066+0.00186211\ttest-mlogloss:0.527145+0.0074838\n",
      "\n",
      "2 \t0.5271452\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[213]\ttrain-mlogloss:0.352706+0.00230145\ttest-mlogloss:0.526443+0.00732842\n",
      "\n",
      "4 \t0.5264426\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[159]\ttrain-mlogloss:0.390464+0.00279415\ttest-mlogloss:0.527391+0.00724981\n",
      "\n",
      "8 \t0.5273912\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[177]\ttrain-mlogloss:0.381805+0.00237731\ttest-mlogloss:0.526913+0.00594032\n",
      "\n",
      "12 \t0.5269126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[231]\ttrain-mlogloss:0.354327+0.00325227\ttest-mlogloss:0.525994+0.00717634\n",
      "\n",
      "16 \t0.5259938\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[159]\ttrain-mlogloss:0.398036+0.00265041\ttest-mlogloss:0.527198+0.00718988\n",
      "\n",
      "20 \t0.5271978\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[167]\ttrain-mlogloss:0.393573+0.00269923\ttest-mlogloss:0.526977+0.00679354\n",
      "\n",
      "24 \t0.5269766\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[179]\ttrain-mlogloss:0.388478+0.00255418\ttest-mlogloss:0.527911+0.00768457\n",
      "\n",
      "28 \t0.527911\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[189]\ttrain-mlogloss:0.385292+0.00262723\ttest-mlogloss:0.527001+0.00775232\n",
      "\n",
      "32 \t0.5270012\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[189]\ttrain-mlogloss:0.389593+0.00185931\ttest-mlogloss:0.527247+0.00665882\n",
      "\n",
      "40 \t0.5272468\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.387127+0.00289643\ttest-mlogloss:0.526661+0.00611834\n",
      "\n",
      "48 \t0.5266614\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[195]\ttrain-mlogloss:0.396135+0.00319157\ttest-mlogloss:0.526655+0.00698115\n",
      "\n",
      "64 \t0.5266546\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-mlogloss:0.404784+0.002127\ttest-mlogloss:0.527635+0.00737379\n",
      "\n",
      "80 \t0.5276348\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-mlogloss:0.40653+0.00239865\ttest-mlogloss:0.528011+0.00761088\n",
      "\n",
      "128 \t0.5280108\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20,24,28,32,40,48,64,80,128]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "    \n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[262]\ttrain-mlogloss:0.395672+0.00192811\ttest-mlogloss:0.532532+0.00697024\n",
      "\n",
      "0.05 \t0.5325316\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[251]\ttrain-mlogloss:0.38311+0.00308902\ttest-mlogloss:0.529113+0.00529031\n",
      "\n",
      "0.1 \t0.5291126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[212]\ttrain-mlogloss:0.387354+0.0026151\ttest-mlogloss:0.526019+0.00693851\n",
      "\n",
      "0.2 \t0.5260188\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.384431+0.00255739\ttest-mlogloss:0.526169+0.00587901\n",
      "\n",
      "0.3 \t0.5261692\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.381644+0.00252687\ttest-mlogloss:0.526699+0.00705682\n",
      "\n",
      "0.4 \t0.5266986\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-mlogloss:0.377252+0.00326099\ttest-mlogloss:0.527301+0.00618384\n",
      "\n",
      "0.5 \t0.5273014\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[218]\ttrain-mlogloss:0.367054+0.00132444\ttest-mlogloss:0.526421+0.00760733\n",
      "\n",
      "0.6 \t0.5264212\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[219]\ttrain-mlogloss:0.364027+0.00250635\ttest-mlogloss:0.526563+0.00715006\n",
      "\n",
      "0.7 \t0.5265626\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[168]\ttrain-mlogloss:0.392891+0.00148548\ttest-mlogloss:0.526076+0.00678444\n",
      "\n",
      "0.8 \t0.5260764\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[192]\ttrain-mlogloss:0.376406+0.00267377\ttest-mlogloss:0.526586+0.00671319\n",
      "\n",
      "0.9 \t0.5265856\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, colsample_bytree = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[114]\ttrain-mlogloss:0.429981+0.00163237\ttest-mlogloss:0.535683+0.0063162\n",
      "\n",
      "0.5 \t0.5356834\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[127]\ttrain-mlogloss:0.417173+0.00106604\ttest-mlogloss:0.532641+0.0071292\n",
      "\n",
      "0.6 \t0.5326408\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[117]\ttrain-mlogloss:0.422807+0.00248712\ttest-mlogloss:0.530766+0.00658263\n",
      "\n",
      "0.7 \t0.5307656\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[138]\ttrain-mlogloss:0.406591+0.002293\ttest-mlogloss:0.52841+0.00609406\n",
      "\n",
      "0.8 \t0.5284098\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.378796+0.00269963\ttest-mlogloss:0.528253+0.00591205\n",
      "\n",
      "0.9 \t0.528253\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = colsample_bytree,\n",
    "#         subsample = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, \n",
    "                   colsample_bytree = colsample_bytree, subsample = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[164]\ttrain-mlogloss:0.393296+0.00247187\ttest-mlogloss:0.526782+0.00697394\n",
      "\n",
      "0.3 \t0.5267822\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[186]\ttrain-mlogloss:0.378468+0.00338923\ttest-mlogloss:0.526772+0.00705368\n",
      "\n",
      "0.6 \t0.526772\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[175]\ttrain-mlogloss:0.38681+0.00159871\ttest-mlogloss:0.526846+0.00696989\n",
      "\n",
      "0.9 \t0.526846\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[167]\ttrain-mlogloss:0.390969+0.002374\ttest-mlogloss:0.527127+0.00787015\n",
      "\n",
      "1.2 \t0.527127\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[206]\ttrain-mlogloss:0.370844+0.00353107\ttest-mlogloss:0.526113+0.00799753\n",
      "\n",
      "1.5 \t0.5261126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[227]\ttrain-mlogloss:0.375197+0.0107539\ttest-mlogloss:0.526744+0.0071078\n",
      "\n",
      "1.8 \t0.5267444\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[193]\ttrain-mlogloss:0.386555+0.00438174\ttest-mlogloss:0.526173+0.00798028\n",
      "\n",
      "2.1 \t0.5261726\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[161]\ttrain-mlogloss:0.411347+0.00406703\ttest-mlogloss:0.526713+0.00664277\n",
      "\n",
      "2.4 \t0.5267126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-mlogloss:0.419464+0.00941197\ttest-mlogloss:0.527696+0.00723329\n",
      "\n",
      "2.7 \t0.527696\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.425483+0.00607894\ttest-mlogloss:0.52768+0.00621536\n",
      "\n",
      "3.0 \t0.5276796\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = colsample_bytree,\n",
    "#         subsample = subsample,\n",
    "#         gamma = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, \n",
    "                   colsample_bytree = colsample_bytree, subsample = subsample, gamma = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[428]\ttrain-mlogloss:0.363966+0.00300174\ttest-mlogloss:0.527457+0.00659426\n",
      "\n",
      "    1 | 27m24s | \u001b[35m  -0.52746\u001b[0m | \u001b[32m            0.5300\u001b[0m | \u001b[32m   2.5164\u001b[0m | \u001b[32m     6.7811\u001b[0m | \u001b[32m           36.9457\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-mlogloss:0.322439+0.00229235\ttest-mlogloss:0.528592+0.00693657\n",
      "\n",
      "    2 | 27m15s |   -0.52859 |             0.6644 |    2.0657 |     11.1918 |            52.9145 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-mlogloss:0.360994+0.00153016\ttest-mlogloss:0.528472+0.00710817\n",
      "\n",
      "    3 | 17m09s |   -0.52847 |             0.2023 |    1.6117 |      8.3270 |            78.7071 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[176]\ttrain-mlogloss:0.304+0.0033053\ttest-mlogloss:0.528335+0.00658829\n",
      "\n",
      "    4 | 22m13s |   -0.52834 |             0.4802 |    1.0852 |     11.8751 |            43.5901 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[360]\ttrain-mlogloss:0.364315+0.00119632\ttest-mlogloss:0.529083+0.00690435\n",
      "\n",
      "    5 | 54m12s |   -0.52908 |             0.7922 |    3.6335 |     11.1882 |            89.3461 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[301]\ttrain-mlogloss:0.385243+0.00226347\ttest-mlogloss:0.52922+0.00642015\n",
      "\n",
      "    6 | 26m00s |   -0.52922 |             0.6477 |    0.1119 |      7.4093 |            96.2057 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[519]\ttrain-mlogloss:0.369016+0.00197565\ttest-mlogloss:0.527703+0.00731201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0003838]), 'nit': 4, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7 | 16m01s |   -0.52770 |             0.1049 |    2.4241 |      7.3114 |            37.9278 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-mlogloss:0.348944+0.00172091\ttest-mlogloss:0.525854+0.00681073\n",
      "\n",
      "    8 | 28m49s | \u001b[35m  -0.52585\u001b[0m | \u001b[32m            0.4091\u001b[0m | \u001b[32m   3.8706\u001b[0m | \u001b[32m    12.0160\u001b[0m | \u001b[32m           32.5874\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[289]\ttrain-mlogloss:0.348944+0.00172091\ttest-mlogloss:0.525854+0.00681073\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00090961]), 'nit': 0, 'funcalls': 21}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 34m46s |   -0.52585 |             0.4091 |    3.8706 |     12.0160 |            32.5874 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\ttrain-mlogloss:0.323776+0.00206345\ttest-mlogloss:0.529308+0.00653546\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00553392]), 'nit': 5, 'funcalls': 59}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00571516]), 'nit': 11, 'funcalls': 66}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 | 37m54s |   -0.52931 |             0.7969 |    3.1037 |     13.4687 |            34.6022 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[508]\ttrain-mlogloss:0.349179+0.00116203\ttest-mlogloss:0.530599+0.00694128\n",
      "\n",
      "   11 | 27m02s |   -0.53060 |             0.1374 |    2.5442 |     13.4305 |            99.1244 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1439]\ttrain-mlogloss:0.429442+0.00211071\ttest-mlogloss:0.528189+0.00660848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.28920826e-05]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00017931]), 'nit': 19, 'funcalls': 70}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00015384]), 'nit': 8, 'funcalls': 66}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 68m05s |   -0.52819 |             0.3700 |    3.9238 |      5.3241 |            96.5526 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[484]\ttrain-mlogloss:0.397501+0.00177919\ttest-mlogloss:0.527865+0.0061275\n",
      "\n",
      "   13 | 16m02s |   -0.52787 |             0.1820 |    0.7637 |      5.2381 |            49.4190 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[789]\ttrain-mlogloss:0.392444+0.00281413\ttest-mlogloss:0.526668+0.00630868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00025387]), 'nit': 6, 'funcalls': 67}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 58m32s |   -0.52667 |             0.5682 |    3.9033 |      6.1013 |            42.5387 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[449]\ttrain-mlogloss:0.348903+0.00124365\ttest-mlogloss:0.526126+0.00618817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda2\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -5.64888451e-05]), 'nit': 6, 'funcalls': 60}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 28m27s |   -0.52613 |             0.3221 |    2.7118 |      7.4455 |            31.1321 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[546]\ttrain-mlogloss:0.405224+0.00253266\ttest-mlogloss:0.527397+0.00644255\n",
      "\n",
      "   16 | 30m37s |   -0.52740 |             0.4752 |    2.7278 |      5.8785 |            59.6967 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[449]\ttrain-mlogloss:0.348903+0.00124365\ttest-mlogloss:0.526126+0.00618817\n",
      "\n",
      "   17 | 27m18s |   -0.52613 |             0.3221 |    2.7118 |      7.4455 |            31.1321 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c11fd7636467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mxgb_BO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\bayes_opt\\bayesian_optimization.pyc\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[1;31m# Append most recently generated values to X and Y arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[1;31m# Updating the GP.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-c11fd7636467>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(min_child_weight, colsample_bytree, max_depth, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    395\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[1;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, gamma): #, subsample\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = 0.99# max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=1234,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (5,14),\n",
    "        'min_child_weight': (30,100),\n",
    "        'colsample_bytree': (0.1,0.8),\n",
    "#         'subsample': (0.7,1),\n",
    "        'gamma': (0,4)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.016003</td>\n",
       "      <td>32.587412</td>\n",
       "      <td>0.409076</td>\n",
       "      <td>3.870580</td>\n",
       "      <td>-0.525854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.016004</td>\n",
       "      <td>32.587412</td>\n",
       "      <td>0.409076</td>\n",
       "      <td>3.870580</td>\n",
       "      <td>-0.525854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.445485</td>\n",
       "      <td>31.132142</td>\n",
       "      <td>0.322098</td>\n",
       "      <td>2.711777</td>\n",
       "      <td>-0.526126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.445485</td>\n",
       "      <td>31.132142</td>\n",
       "      <td>0.322098</td>\n",
       "      <td>2.711777</td>\n",
       "      <td>-0.526126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.101341</td>\n",
       "      <td>42.538652</td>\n",
       "      <td>0.568188</td>\n",
       "      <td>3.903256</td>\n",
       "      <td>-0.526668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.878451</td>\n",
       "      <td>59.696664</td>\n",
       "      <td>0.475184</td>\n",
       "      <td>2.727763</td>\n",
       "      <td>-0.527397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.311439</td>\n",
       "      <td>37.927790</td>\n",
       "      <td>0.104916</td>\n",
       "      <td>2.424076</td>\n",
       "      <td>-0.527703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.238122</td>\n",
       "      <td>49.419011</td>\n",
       "      <td>0.182047</td>\n",
       "      <td>0.763688</td>\n",
       "      <td>-0.527865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.324136</td>\n",
       "      <td>96.552623</td>\n",
       "      <td>0.370024</td>\n",
       "      <td>3.923847</td>\n",
       "      <td>-0.528189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.409311</td>\n",
       "      <td>96.205662</td>\n",
       "      <td>0.647738</td>\n",
       "      <td>0.111881</td>\n",
       "      <td>-0.529220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree     gamma     score\n",
       "2   12.016003         32.587412          0.409076  3.870580 -0.525854\n",
       "3   12.016004         32.587412          0.409076  3.870580 -0.525854\n",
       "9    7.445485         31.132142          0.322098  2.711777 -0.526126\n",
       "11   7.445485         31.132142          0.322098  2.711777 -0.526126\n",
       "8    6.101341         42.538652          0.568188  3.903256 -0.526668\n",
       "10   5.878451         59.696664          0.475184  2.727763 -0.527397\n",
       "1    7.311439         37.927790          0.104916  2.424076 -0.527703\n",
       "7    5.238122         49.419011          0.182047  0.763688 -0.527865\n",
       "6    5.324136         96.552623          0.370024  3.923847 -0.528189\n",
       "0    7.409311         96.205662          0.647738  0.111881 -0.529220"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "#                                s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "#                                        'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=1234)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.02)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]      \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 5 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4091,\n",
      "       gamma=3.8706, learning_rate=0.02, max_delta_step=0, max_depth=12,\n",
      "       min_child_weight=32, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 1 fold 1\n",
      "best round 2232\n",
      "('Score: ', 0.53155544781621866)\n",
      "Model 1 fold 1 fitting finished in 2715.331s\n",
      "Model 1 fold 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6e3face9fbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                               \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                               \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                               300)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-e3465f52b445>\u001b[0m in \u001b[0;36mxgb_blend\u001b[0;34m(estimators, train_x, train_y, test_x, fold, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0meval_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mlogloss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     verbose=False)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mbest_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbest_rounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                               verbose_eval=verbose)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    201\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\python\\Anaconda2\\lib\\site-packages\\xgboost-0.6-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "#     xgb.XGBClassifier(max_depth = 7,\n",
    "#                               min_child_weight = 24,\n",
    "#                               colsample_bytree = 0.309861 ,\n",
    "#                               subsample = 0.998132 ,\n",
    "#                               gamma = 2.211859),\n",
    "#              xgb.XGBClassifier(max_depth = 6,\n",
    "#                               min_child_weight = 19,\n",
    "#                               colsample_bytree = 0.432358,\n",
    "#                               subsample = 0.949350,\n",
    "#                               gamma = 2.976848),\n",
    "#              xgb.XGBClassifier(max_depth = 7,\n",
    "#                               min_child_weight = 23,\n",
    "#                               colsample_bytree = 0.214791,\n",
    "#                               subsample = 0.997197,\n",
    "#                               gamma = 2.163581),         \n",
    "#              xgb.XGBClassifier(max_depth = 8,\n",
    "#                               min_child_weight = 23,\n",
    "#                               colsample_bytree = 0.5,\n",
    "#                               subsample = 0.988002,\n",
    "#                               gamma = 3.0),  \n",
    "             xgb.XGBClassifier(max_depth = 12,\n",
    "                              min_child_weight = 32,\n",
    "                              colsample_bytree = 0.4091,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 3.8706)              \n",
    "             ]\n",
    "\n",
    "#  \tmax_depth \tmin_child_weight \tcolsample_bytree \tgamma \tscore\n",
    "# 2 \t12.016003 \t32.587412 \t0.409076 \t3.870580 \t-0.525854\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb_mean,\n",
    " test_blend_x_xgb_gmean,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              5,\n",
    "                              300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_blend_x_xgb = pd.DataFrame(train_blend_x_xgb)\n",
    "train_blend_x_xgb.columns = [\"low\", \"medium\", \"high\"]\n",
    "train_blend_x_xgb[\"listing_id\"] = train_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_mean = pd.DataFrame(test_blend_x_xgb_mean)\n",
    "test_blend_x_xgb_mean.columns = [\"low\", \"medium\", \"high\"]\n",
    "test_blend_x_xgb_mean[\"listing_id\"] = test_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_gmean = pd.DataFrame(test_blend_x_xgb_gmean)\n",
    "test_blend_x_xgb_gmean.columns = [\"low\", \"medium\", \"high\"]\n",
    "test_blend_x_xgb_gmean[\"listing_id\"] = test_X.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train = train_X_0322[['listing_id']].merge(train_blend_x_xgb,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values\n",
    "tmp_test_mean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_mean,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values\n",
    "tmp_test_gmean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_gmean,on = 'listing_id', how = 'left')[[\"low\", \"medium\", \"high\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_mean_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_gmean_cv_price_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,tmp_train, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,tmp_test_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,tmp_test_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
