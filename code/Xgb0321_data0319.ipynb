{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.stats.mstats import gmean\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 464) (74659, 464) (49352,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_pickle(data_path + 'train_X_0319.pkl')\n",
    "test_X = pd.read_pickle(data_path + 'test_X_0319.pkl')\n",
    "train_y = np.ravel(pd.read_pickle(data_path + 'train_y_0319.pkl'))\n",
    "sub_id = test_X.listing_id.astype('int32').values\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 464)\n",
      "(9871, 464)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.03718\n",
      "Will train until validation_0-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\tvalidation_0-mlogloss:0.635855\n",
      "[50]\tvalidation_0-mlogloss:0.594012\n",
      "[75]\tvalidation_0-mlogloss:0.578747\n",
      "[100]\tvalidation_0-mlogloss:0.569621\n",
      "[125]\tvalidation_0-mlogloss:0.563314\n",
      "[150]\tvalidation_0-mlogloss:0.55842\n",
      "[175]\tvalidation_0-mlogloss:0.554957\n",
      "[200]\tvalidation_0-mlogloss:0.551957\n",
      "[225]\tvalidation_0-mlogloss:0.549701\n",
      "[250]\tvalidation_0-mlogloss:0.548004\n",
      "[275]\tvalidation_0-mlogloss:0.546493\n",
      "[300]\tvalidation_0-mlogloss:0.545255\n",
      "[325]\tvalidation_0-mlogloss:0.544215\n",
      "[350]\tvalidation_0-mlogloss:0.543186\n",
      "[375]\tvalidation_0-mlogloss:0.542323\n",
      "[400]\tvalidation_0-mlogloss:0.541659\n",
      "[425]\tvalidation_0-mlogloss:0.540934\n",
      "[450]\tvalidation_0-mlogloss:0.540231\n",
      "[475]\tvalidation_0-mlogloss:0.539755\n",
      "[500]\tvalidation_0-mlogloss:0.539421\n",
      "[525]\tvalidation_0-mlogloss:0.539184\n",
      "[550]\tvalidation_0-mlogloss:0.538677\n",
      "[575]\tvalidation_0-mlogloss:0.538061\n",
      "[600]\tvalidation_0-mlogloss:0.538047\n",
      "[625]\tvalidation_0-mlogloss:0.537998\n",
      "Stopping. Best iteration:\n",
      "[585]\tvalidation_0-mlogloss:0.537874\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=10000, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rgr = xgb.XGBClassifier(objective = 'multi:softprob',\n",
    "#                        learning_rate = 0.1,\n",
    "#                        n_estimators = 10000,\n",
    "#                        nthread = -1)\n",
    "\n",
    "# rgr.fit(X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "# #         num_class = 3,\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=25\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_y = rgr.predict_proba(test_X, ntree_limit = rgr.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "# sub_name = '../output/sub_xgb_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "# out_df = pd.DataFrame(pred_y[:,:3])\n",
    "# out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "# out_df[\"listing_id\"] = sub_id\n",
    "# out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.537874 585\n",
      "4 \t0.533246 581\n",
      "5 \t0.534774 392\n",
      "6 \t0.534358 252\n",
      "7 \t0.535118 189\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t0.534686 579\n",
      "4 \t0.534913 515\n",
      "8 \t0.53319 490\n",
      "12 \t0.533521 568\n",
      "16 \t0.532501 524\n",
      "20 \t0.533333 505\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 \t0.533516 565\n",
      "28 \t0.533766 482\n",
      "32 \t0.533918 480\n"
     ]
    }
   ],
   "source": [
    "for x in [24,28,32]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 \t0.539257 1092\n",
      "0.1 \t0.535705 640\n",
      "0.2 \t0.532362 648\n",
      "0.3 \t0.532268 671\n",
      "0.4 \t0.532383 622\n",
      "0.5 \t0.532407 560\n",
      "0.6 \t0.533004 511\n",
      "0.7 \t0.531554 605\n",
      "0.8 \t0.532438 604\n",
      "0.9 \t0.533032 621\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.535465 500\n",
      "0.6 \t0.534347 517\n",
      "0.7 \t0.533483 462\n",
      "0.8 \t0.532123 520\n",
      "0.9 \t0.531371 592\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.532157 616\n",
      "0.6 \t0.532724 520\n",
      "0.9 \t0.53279 613\n",
      "1.2 \t0.531874 602\n",
      "1.5 \t0.532611 597\n",
      "1.8 \t0.532247 529\n",
      "2.1 \t0.532049 678\n",
      "2.4 \t0.533522 529\n",
      "2.7 \t0.532063 721\n",
      "3.0 \t0.533154 661\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.3 \t0.528756 371\n",
    "# 0.6 \t0.530068 353\n",
    "# 0.9 \t0.530043 275\n",
    "# 1.2 \t0.530065 388\n",
    "# 1.5 \t0.529657 331\n",
    "# 1.8 \t0.529906 328\n",
    "# 2.1 \t0.528338 393\n",
    "# 2.4 \t0.529364 372\n",
    "# 2.7 \t0.527919 456\n",
    "# 3.0 \t0.528962 417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[402]\ttrain-mlogloss:0.349671+0.00279218\ttest-mlogloss:0.533837+0.00430309\n",
      "\n",
      "    1 | 32m04s | \u001b[35m  -0.53384\u001b[0m | \u001b[32m            0.7713\u001b[0m | \u001b[32m   1.7190\u001b[0m | \u001b[32m     6.1334\u001b[0m | \u001b[32m           24.0622\u001b[0m | \u001b[32m     0.9267\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[376]\ttrain-mlogloss:0.337871+0.00230234\ttest-mlogloss:0.532842+0.00528728\n",
      "\n",
      "    2 | 21m22s | \u001b[35m  -0.53284\u001b[0m | \u001b[32m            0.5063\u001b[0m | \u001b[32m   0.0960\u001b[0m | \u001b[32m     6.4121\u001b[0m | \u001b[32m           11.1358\u001b[0m | \u001b[32m     0.9271\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8629b0fe6176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mxgb_BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/bayes_opt/bayesian_optimization.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0my_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-8629b0fe6176>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(min_child_weight, colsample_bytree, max_depth, subsample, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mlogloss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    398\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xujin/AI/anaconda2/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (4,7),\n",
    "        'min_child_weight': (8,28),\n",
    "        'colsample_bytree': (0.5,0.8),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0,1.8)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.028481</td>\n",
       "      <td>4.216682</td>\n",
       "      <td>0.768085</td>\n",
       "      <td>0.772111</td>\n",
       "      <td>2.978740</td>\n",
       "      <td>-0.534220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.730213</td>\n",
       "      <td>19.955073</td>\n",
       "      <td>0.310748</td>\n",
       "      <td>0.722331</td>\n",
       "      <td>0.314302</td>\n",
       "      <td>-0.535228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "0   5.028481          4.216682          0.768085   0.772111  2.978740   \n",
       "1   7.730213         19.955073          0.310748   0.722331  0.314302   \n",
       "\n",
       "      score  \n",
       "0 -0.534220  \n",
       "1 -0.535228  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.02)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]      \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.309861,\n",
      "       gamma=2.211859, learning_rate=0.02, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=24, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.998132)\n",
      "Model 1 fold 1\n",
      "best round 3487\n",
      "('Score: ', 0.51383791963799097)\n",
      "Model 1 fold 1 fitting finished in 2071.905s\n",
      "Model 1 fold 2\n",
      "best round 3603\n",
      "('Score: ', 0.49909809177754771)\n",
      "Model 1 fold 2 fitting finished in 2110.754s\n",
      "Model 1 fold 3\n",
      "best round 2662\n",
      "('Score: ', 0.52464419065607182)\n",
      "Model 1 fold 3 fitting finished in 1629.569s\n",
      "Model 1 fold 4\n",
      "best round 3757\n",
      "('Score: ', 0.50177507994359405)\n",
      "Model 1 fold 4 fitting finished in 2193.679s\n",
      "Model 1 fold 5\n",
      "best round 2813\n",
      "('Score: ', 0.53355855754643977)\n",
      "Model 1 fold 5 fitting finished in 1742.833s\n",
      "Model 1 fold 6\n",
      "best round 1575\n",
      "('Score: ', 0.52200505888160187)\n",
      "Model 1 fold 6 fitting finished in 1542.116s\n",
      "Model 1 fold 7\n",
      "best round 2817\n",
      "('Score: ', 0.52895286317447998)\n",
      "Model 1 fold 7 fitting finished in 1910.581s\n",
      "Model 1 fold 8\n",
      "best round 2728\n",
      "('Score: ', 0.54664802977167382)\n",
      "Model 1 fold 8 fitting finished in 1644.739s\n",
      "Model 1 fold 9\n",
      "best round 2443\n",
      "('Score: ', 0.53921138947941538)\n",
      "Model 1 fold 9 fitting finished in 1490.510s\n",
      "Model 1 fold 10\n",
      "best round 2782\n",
      "('Score: ', 0.52886874952120944)\n",
      "Model 1 fold 10 fitting finished in 1675.671s\n",
      "Score for model 1 is 0.523860\n",
      "Model 2: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.432358,\n",
      "       gamma=2.976848, learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=19, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.94935)\n",
      "Model 2 fold 1\n",
      "best round 3521\n",
      "('Score: ', 0.51518757226016598)\n",
      "Model 2 fold 1 fitting finished in 2413.479s\n",
      "Model 2 fold 2\n",
      "best round 5420\n",
      "('Score: ', 0.49954523571824616)\n",
      "Model 2 fold 2 fitting finished in 3611.983s\n",
      "Model 2 fold 3\n",
      "best round 3876\n",
      "('Score: ', 0.52331778505515869)\n",
      "Model 2 fold 3 fitting finished in 2931.383s\n",
      "Model 2 fold 4\n",
      "best round 4878\n",
      "('Score: ', 0.50107709094303765)\n",
      "Model 2 fold 4 fitting finished in 9603.150s\n",
      "Model 2 fold 5\n",
      "best round 4325\n",
      "('Score: ', 0.53422298036595794)\n",
      "Model 2 fold 5 fitting finished in 3222.459s\n",
      "Model 2 fold 6\n",
      "best round 3377\n",
      "('Score: ', 0.52288106036748117)\n",
      "Model 2 fold 6 fitting finished in 2324.070s\n",
      "Model 2 fold 7\n",
      "best round 4041\n",
      "('Score: ', 0.52931429976306688)\n",
      "Model 2 fold 7 fitting finished in 2739.991s\n",
      "Model 2 fold 8\n",
      "best round 2507\n",
      "('Score: ', 0.54872902234125243)\n",
      "Model 2 fold 8 fitting finished in 1774.515s\n",
      "Model 2 fold 9\n",
      "best round 3041\n",
      "('Score: ', 0.53932161217736097)\n",
      "Model 2 fold 9 fitting finished in 2107.935s\n",
      "Model 2 fold 10\n",
      "best round 4811\n",
      "('Score: ', 0.52843415363735213)\n",
      "Model 2 fold 10 fitting finished in 3227.302s\n",
      "Score for model 2 is 0.524203\n",
      "Model 3: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.214791,\n",
      "       gamma=2.163581, learning_rate=0.02, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=23, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.997197)\n",
      "Model 3 fold 1\n",
      "best round 2707\n",
      "('Score: ', 0.51535879258728312)\n",
      "Model 3 fold 1 fitting finished in 1235.428s\n",
      "Model 3 fold 2\n",
      "best round 3859\n",
      "('Score: ', 0.5014999712118472)\n",
      "Model 3 fold 2 fitting finished in 1704.819s\n",
      "Model 3 fold 3\n",
      "best round 3539\n",
      "('Score: ', 0.52514350618240901)\n",
      "Model 3 fold 3 fitting finished in 1577.643s\n",
      "Model 3 fold 4\n",
      "best round 3795\n",
      "('Score: ', 0.50176561269835429)\n",
      "Model 3 fold 4 fitting finished in 1677.138s\n",
      "Model 3 fold 5\n",
      "best round 2786\n",
      "('Score: ', 0.53404400424739618)\n",
      "Model 3 fold 5 fitting finished in 1266.292s\n",
      "Model 3 fold 6\n",
      "best round 2150\n",
      "('Score: ', 0.52229649426128855)\n",
      "Model 3 fold 6 fitting finished in 1010.713s\n",
      "Model 3 fold 7\n",
      "best round 2997\n",
      "('Score: ', 0.5297577655582919)\n",
      "Model 3 fold 7 fitting finished in 1356.277s\n",
      "Model 3 fold 8\n",
      "best round 2638\n",
      "('Score: ', 0.5463555637082762)\n",
      "Model 3 fold 8 fitting finished in 1207.872s\n",
      "Model 3 fold 9\n",
      "best round 3567\n",
      "('Score: ', 0.53801963045987777)\n",
      "Model 3 fold 9 fitting finished in 1586.612s\n",
      "Model 3 fold 10\n",
      "best round 2991\n",
      "('Score: ', 0.52873409131415117)\n",
      "Model 3 fold 10 fitting finished in 1356.692s\n",
      "Score for model 3 is 0.524298\n",
      "Model 4: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n",
      "       gamma=3.0, learning_rate=0.02, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=23, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.988002)\n",
      "Model 4 fold 1\n",
      "best round 2816\n",
      "('Score: ', 0.51297332732737289)\n",
      "Model 4 fold 1 fitting finished in 2871.638s\n",
      "Model 4 fold 2\n",
      "best round 2880\n",
      "('Score: ', 0.49964021121543867)\n",
      "Model 4 fold 2 fitting finished in 2929.901s\n",
      "Model 4 fold 3\n",
      "best round 2696\n",
      "('Score: ', 0.52380578785602416)\n",
      "Model 4 fold 3 fitting finished in 2762.010s\n",
      "Model 4 fold 4\n",
      "best round 3182\n",
      "('Score: ', 0.5014350780996536)\n",
      "Model 4 fold 4 fitting finished in 3209.632s\n",
      "Model 4 fold 5\n",
      "best round 2015\n",
      "('Score: ', 0.53472894606310351)\n",
      "Model 4 fold 5 fitting finished in 2139.165s\n",
      "Model 4 fold 6\n",
      "best round 2267\n",
      "('Score: ', 0.52261181333791817)\n",
      "Model 4 fold 6 fitting finished in 2369.102s\n",
      "Model 4 fold 7\n",
      "best round 2349\n",
      "('Score: ', 0.52805699453110733)\n",
      "Model 4 fold 7 fitting finished in 2444.581s\n",
      "Model 4 fold 8\n",
      "best round 3034\n",
      "('Score: ', 0.54615976645902209)\n",
      "Model 4 fold 8 fitting finished in 3115.107s\n",
      "Model 4 fold 9\n",
      "best round 3778\n",
      "('Score: ', 0.53834183077019948)\n",
      "Model 4 fold 9 fitting finished in 3765.175s\n",
      "Model 4 fold 10\n",
      "best round 2814\n",
      "('Score: ', 0.52886842434910097)\n",
      "Model 4 fold 10 fitting finished in 2868.390s\n",
      "Score for model 4 is 0.523662\n",
      "Model 5: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.236769,\n",
      "       gamma=2.870883, learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=13, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.947349)\n",
      "Model 5 fold 1\n",
      "best round 3109\n",
      "('Score: ', 0.51628335218164001)\n",
      "Model 5 fold 1 fitting finished in 1337.625s\n",
      "Model 5 fold 2\n",
      "best round 5124\n",
      "('Score: ', 0.50012322809531962)\n",
      "Model 5 fold 2 fitting finished in 2120.919s\n",
      "Model 5 fold 3\n",
      "best round 4867\n",
      "('Score: ', 0.52487726390886713)\n",
      "Model 5 fold 3 fitting finished in 2067.849s\n",
      "Model 5 fold 4\n",
      "best round 5903\n",
      "('Score: ', 0.50191824820962938)\n",
      "Model 5 fold 4 fitting finished in 2420.696s\n",
      "Model 5 fold 5\n",
      "best round 3257\n",
      "('Score: ', 0.53527621997535013)\n",
      "Model 5 fold 5 fitting finished in 1391.718s\n",
      "Model 5 fold 6\n",
      "best round 3305\n",
      "('Score: ', 0.52176095506019649)\n",
      "Model 5 fold 6 fitting finished in 1406.905s\n",
      "Model 5 fold 7\n",
      "best round 4462\n",
      "('Score: ', 0.53020224694865936)\n",
      "Model 5 fold 7 fitting finished in 1866.603s\n",
      "Model 5 fold 8\n",
      "best round 4459\n",
      "('Score: ', 0.54652733644958884)\n",
      "Model 5 fold 8 fitting finished in 1878.934s\n",
      "Model 5 fold 9\n",
      "best round 4577\n",
      "('Score: ', 0.53848368223254861)\n",
      "Model 5 fold 9 fitting finished in 1906.415s\n",
      "Model 5 fold 10\n",
      "best round 5442\n",
      "('Score: ', 0.52956601456374131)\n",
      "Model 5 fold 10 fitting finished in 2251.300s\n",
      "Score for model 5 is 0.524502\n",
      "Score for blended models is 0.524105\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    xgb.XGBClassifier(max_depth = 7,\n",
    "                              min_child_weight = 24,\n",
    "                              colsample_bytree = 0.309861 ,\n",
    "                              subsample = 0.998132 ,\n",
    "                              gamma = 2.211859),\n",
    "             xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 19,\n",
    "                              colsample_bytree = 0.432358,\n",
    "                              subsample = 0.949350,\n",
    "                              gamma = 2.976848),\n",
    "             xgb.XGBClassifier(max_depth = 7,\n",
    "                              min_child_weight = 23,\n",
    "                              colsample_bytree = 0.214791,\n",
    "                              subsample = 0.997197,\n",
    "                              gamma = 2.163581),         \n",
    "             xgb.XGBClassifier(max_depth = 8,\n",
    "                              min_child_weight = 23,\n",
    "                              colsample_bytree = 0.5,\n",
    "                              subsample = 0.988002,\n",
    "                              gamma = 3.0),  \n",
    "             xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 13,\n",
    "                              colsample_bytree = 0.236769,\n",
    "                              subsample = 0.947349,\n",
    "                              gamma = 2.870883)              \n",
    "             ]\n",
    "\n",
    "#  \t\tmax_depth \tmin_child_weight \tcolsample_bytree \tsubsample \tgamma \t\tscore\n",
    "# 33 \t7.513940 \t24.341212 \t\t\t0.309861 \t\t\t0.998132 \t2.211859 \t-0.528512\n",
    "# 2 \t6.865995 \t19.894651 \t\t\t0.432358 \t\t\t0.949350 \t2.976848 \t-0.528667\n",
    "# 9 \t7.270910 \t23.515202 \t\t\t0.214791 \t\t\t0.997197 \t2.163581 \t-0.528757\n",
    "# 22 \t8.000000 \t23.438976 \t\t\t0.500000 \t\t\t0.988002 \t3.000000 \t-0.528779\n",
    "# 1 \t6.622058 \t13.857343 \t\t\t0.236769 \t\t\t0.947349 \t2.870883 \t-0.529040\n",
    "# 11 \t5.122812 \t11.339506 \t\t\t0.338006 \t\t\t0.930783 \t2.867388 \t-0.529081\n",
    "# 8 \t7.270911 \t23.515206 \t\t\t0.214791 \t\t\t0.997198 \t2.163575 \t-0.529257\n",
    "# 24 \t7.917312 \t13.816491 \t\t\t0.490651 \t\t\t0.889237 \t1.880521 \t-0.529304\n",
    "# 39 \t7.288872 \t29.477693 \t\t\t0.319768 \t\t\t0.937097 \t2.265661 \t-0.529336\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb_mean,\n",
    " test_blend_x_xgb_gmean,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52385999  0.52420308  0.52429754  0.52366222  0.52450185]\n",
      "[ 2866.7  3979.7  3102.9  2783.1  4450.5]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_BM_MB_add_desc_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_mean_BM_MB_add_desc_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_gmean_BM_MB_add_desc_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_xgb_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_xgb_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_mean_BM_MB_add_desc_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb_mean[:,9:12])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.20544260e-01,   6.10888687e-01,   6.56750820e-02],\n",
       "       [  9.66028463e-01,   2.30937453e-02,   1.06505838e-02],\n",
       "       [  9.53579737e-01,   4.13955017e-02,   3.97691225e-03],\n",
       "       ..., \n",
       "       [  9.78252564e-01,   2.03241753e-02,   1.01263996e-03],\n",
       "       [  9.71474749e-01,   2.74180665e-02,   5.09567844e-04],\n",
       "       [  5.87161787e-01,   3.92164954e-01,   1.94305868e-02]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb_gmean[:,9:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.20994298e-01,   6.04517100e-01,   7.12524217e-02],\n",
       "       [  9.57150480e-01,   3.01748109e-02,   1.18646473e-02],\n",
       "       [  9.58178383e-01,   3.74607705e-02,   3.46537444e-03],\n",
       "       ..., \n",
       "       [  9.80787885e-01,   1.80516908e-02,   8.23179767e-04],\n",
       "       [  9.70638017e-01,   2.80394743e-02,   5.14503672e-04],\n",
       "       [  5.81934901e-01,   3.95535699e-01,   2.09051621e-02]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb_gmean[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
