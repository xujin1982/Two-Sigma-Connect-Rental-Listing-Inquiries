{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn import model_selection,ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "import random\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, LabelBinarizer, MultiLabelBinarizer,LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats.mstats import gmean\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_train(train,y,max_depth = 6,min_child_weight = 1,colsample_bytree = 1, subsample = 1, gamma = 0 , verbose_eval = None,\n",
    "            seed = 0, early_stop = 50, nfold = 5, eta=0.3):\n",
    "    xgtrain = xgb.DMatrix(train, label=y)\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=0\n",
    "    params['eta'] = eta\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "#     params['booster'] = 'dart'\n",
    "#     params['rate_drop'] = 0.1\n",
    "#     params['skip_drop'] = 0.5\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=nfold,\n",
    "        metrics = 'mlogloss', verbose_eval = verbose_eval,\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(early_stop)]\n",
    "    )\n",
    "\n",
    "    return cv_result['test-mlogloss-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CV_st(train,test,feature1,feature2):\n",
    "    index=list(range(train.shape[0]))\n",
    "    random.shuffle(index)\n",
    "    kf = KFold(n_splits=5,shuffle=True, random_state=0)\n",
    "    \n",
    "    # median feature names\n",
    "    features_tmp = []\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_median') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_median')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].median().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[0]] = f_low\n",
    "    train[features_tmp[1]] = f_medium\n",
    "    train[features_tmp[2]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].median().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[0]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[1]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[2]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "\n",
    "    # mean feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_mean') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_mean')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[3]] = f_low\n",
    "    train[features_tmp[4]] = f_medium\n",
    "    train[features_tmp[5]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].mean().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[3]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[4]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[5]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # max feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_max') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_max')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].max().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[6]] = f_low\n",
    "    train[features_tmp[7]] = f_medium\n",
    "    train[features_tmp[8]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].max().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[6]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[7]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[8]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "    \n",
    "    # min feature names\n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_low_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_medium_min') \n",
    "    features_tmp.append(feature1 + '_' + feature2 + '_high_min')\n",
    "    \n",
    "    # train data \n",
    "    f_low=pd.Series([np.nan]*len(train))\n",
    "    f_medium=pd.Series([np.nan]*len(train))\n",
    "    f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "    for train_index, test_index in kf.split(index):\n",
    "        tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].min().\\\n",
    "                reset_index().rename(columns={feature2:'new'})\n",
    "        f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                                            on=feature1,how='left')['new'].values\n",
    "        f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "                                                            on=feature1,how='left')['new'].values   \n",
    "    train[features_tmp[9]] = f_low\n",
    "    train[features_tmp[10]] = f_medium\n",
    "    train[features_tmp[11]] = f_high\n",
    "\n",
    "    # test data\n",
    "    tmp = train.groupby(['interest_level',feature1])[feature2].min().\\\n",
    "            reset_index().rename(columns={feature2:'new'})    \n",
    "    test[features_tmp[9]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[10]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "                                       on=feature1,how='left')['new'].values\n",
    "    test[features_tmp[11]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "                                       on=feature1,how='left')['new'].values \n",
    "\n",
    "#     # std feature names\n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_low_std') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_medium_std') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_high_std')\n",
    "    \n",
    "#     # train data \n",
    "#     f_low=pd.Series([np.nan]*len(train))\n",
    "#     f_medium=pd.Series([np.nan]*len(train))\n",
    "#     f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "#     for train_index, test_index in kf.split(index):\n",
    "#         tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].std().\\\n",
    "#                 reset_index().rename(columns={feature2:'new'})\n",
    "#         f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                                             on=feature1,how='left')['new'].values   \n",
    "#     train[features_tmp[12]] = f_low\n",
    "#     train[features_tmp[13]] = f_medium\n",
    "#     train[features_tmp[14]] = f_high\n",
    "\n",
    "#     # test data\n",
    "#     tmp = train.groupby(['interest_level',feature1])[feature2].std().\\\n",
    "#             reset_index().rename(columns={feature2:'new'})    \n",
    "#     test[features_tmp[12]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[13]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[14]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                        on=feature1,how='left')['new'].values \n",
    "    \n",
    "#     # var feature names\n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_low_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_medium_var') \n",
    "#     features_tmp.append(feature1 + '_' + feature2 + '_high_var')\n",
    "    \n",
    "#     # train data \n",
    "#     f_low=pd.Series([np.nan]*len(train))\n",
    "#     f_medium=pd.Series([np.nan]*len(train))\n",
    "#     f_high=pd.Series([np.nan]*len(train))\n",
    "\n",
    "#     for train_index, test_index in kf.split(index):\n",
    "#         tmp = train.iloc[train_index].groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#                 reset_index().rename(columns={feature2:'new'})\n",
    "#         f_low[test_index]    = train.iloc[test_index].merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_medium[test_index] = train.iloc[test_index].merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                                             on=feature1,how='left')['new'].values\n",
    "#         f_high[test_index]   = train.iloc[test_index].merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                                             on=feature1,how='left')['new'].values   \n",
    "#     train[features_tmp[15]] = f_low\n",
    "#     train[features_tmp[16]] = f_medium\n",
    "#     train[features_tmp[17]] = f_high\n",
    "\n",
    "#     # test data\n",
    "#     tmp = train.groupby(['interest_level',feature1])[feature2].var().\\\n",
    "#             reset_index().rename(columns={feature2:'new'})    \n",
    "#     test[features_tmp[15]] = test.merge(tmp[tmp.interest_level == 'low'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[16]] = test.merge(tmp[tmp.interest_level == 'medium'],\n",
    "#                                        on=feature1,how='left')['new'].values\n",
    "#     test[features_tmp[17]] = test.merge(tmp[tmp.interest_level == 'high'],\n",
    "#                                        on=feature1,how='left')['new'].values \n",
    "    \n",
    "#     # ratio/diff feature\n",
    "#     cols = features_tmp[:]\n",
    "# #     features_tmp = []\n",
    "#     for col in cols:\n",
    "#         new_feature = col+'_ratio'\n",
    "#         train[new_feature] = train[col] / train[feature2]\n",
    "#         test[new_feature] = test[col] / test[feature2]\n",
    "#         features_tmp.append(new_feature)\n",
    "        \n",
    "    print feature1,' vs ', feature2,'Done!'\n",
    "    return train,test,features_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "#input data\n",
    "train_df=pd.read_json('../input/train.json').reset_index(drop = True)\n",
    "test_df=pd.read_json('../input/test.json').reset_index(drop = True)\n",
    "test_df.loc[test_df.bathrooms == 112.0,'bathrooms'] = 1.5    \n",
    "test_df.loc[test_df.bathrooms == 20.0,'bathrooms'] = 2.0\n",
    "test_df.loc[test_df.listing_id == 7220763,'bedrooms'] = 3\n",
    "test_df.loc[test_df.listing_id == 7047074,'bedrooms'] = 6\n",
    "print train_df.shape\n",
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    df[\"num_photo_count\"] = df[\"photos\"].apply(len)\n",
    "    df[\"street_address\"] = df['street_address'].apply(fmt)\n",
    "    df[\"display_address\"] = df[\"display_address\"].apply(fmt)\n",
    "    df[\"num_desc_wordcount\"] = df[\"description\"].apply(len)\n",
    "    df[\"num_pricePerBed\"] = df['price'] / df['bedrooms']\n",
    "    df[\"num_pricePerBath\"] = df['price'] / df['bathrooms']\n",
    "    df[\"num_pricePerRoom\"] = df['price'] / (df['bedrooms'] + df['bathrooms'])\n",
    "    df[\"num_bedPerBath\"] = df['bedrooms'] / df['bathrooms']\n",
    "    df[\"num_bedBathDiff\"] = df['bedrooms'] - df['bathrooms']\n",
    "    df[\"num_bedBathSum\"] = df[\"bedrooms\"] + df['bathrooms']\n",
    "    df[\"num_bedsPerc\"] = df[\"bedrooms\"] / (df['bedrooms'] + df['bathrooms'])\n",
    "\n",
    "    df = df.fillna(-1).replace(np.inf, -1)\n",
    "    return df\n",
    "\n",
    "# Add common features\n",
    "train_df = add_features(train_df)\n",
    "test_df = add_features(test_df) \n",
    "\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df['num_desc_length_null'] = (train_df.description.str.len()==0).astype(float)\n",
    "test_df['num_desc_length_null'] = (test_df.description.str.len()==0).astype(float)\n",
    "    \n",
    "features_to_use=[\n",
    "    \"latitude\", \"longitude\",\"num_pricePerBed\",\n",
    "    'num_bedBathSum','num_pricePerBath','num_pricePerRoom','num_bedPerBath',\n",
    "    'num_bedBathDiff','num_bedsPerc',\n",
    "    \"num_photo_count\", \"num_features\", \"num_desc_wordcount\",'num_desc_length_null',\n",
    "    \"listing_id\"]\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Location features: Latitude, longitude\n",
    "precision = 3\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2)\n",
    "train_df['num_dist_from_center'] = x.values\n",
    "x = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2)\n",
    "test_df['num_dist_from_center'] = x.values\n",
    "train_df['position'] = train_df.longitude.round(precision).astype(str) + '_' + train_df.latitude.round(precision).astype(str)\n",
    "test_df['position'] = test_df.longitude.round(precision).astype(str) + '_' + test_df.latitude.round(precision).astype(str)\n",
    "\n",
    "new_feature = ['num_dist_from_center']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Degree of \"outlierness\"\n",
    "OutlierAggregated = (train_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated2 = (test_df.bedrooms > 4).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms > 3).astype(float)\n",
    "OutlierAggregated += (train_df.bathrooms < 1).astype(float)\n",
    "OutlierAggregated2 += (test_df.bathrooms < 1).astype(float)\n",
    "x = np.abs((train_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.abs((test_df.price - train_df.price.median())/train_df.price.std()) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.log1p(train_df.price/(train_df.bedrooms.clip(1,3) + train_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.log1p(test_df.price/(test_df.bedrooms.clip(1,3) + test_df.bathrooms.clip(1,2))) > 8.2\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "x = np.sqrt(((train_df.latitude - train_df.latitude.median())**2) + (train_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated += x.astype(float)\n",
    "x2 = np.sqrt(((test_df.latitude - train_df.latitude.median())**2) + (test_df.longitude - train_df.longitude.median())**2) > 0.30\n",
    "OutlierAggregated2 += x2.astype(float)\n",
    "train_df['num_OutlierAggregated'] = OutlierAggregated.values\n",
    "test_df['num_OutlierAggregated'] = OutlierAggregated2.values\n",
    "\n",
    "\n",
    "new_feature = ['num_OutlierAggregated']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Density in unique locations at given precision\n",
    "vals = train_df['position'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df['num_pos_density'] = train_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df['num_pos_density'] = test_df['position'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "# Building null\n",
    "train_df['num_building_null'] = (train_df.building_id=='0').astype(float)\n",
    "test_df['num_building_null'] = (test_df.building_id=='0').astype(float)\n",
    "\n",
    "\n",
    "new_feature = ['num_pos_density','num_building_null']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Creation time features\n",
    "train_df['created'] = pd.to_datetime(train_df.created)\n",
    "train_df['num_created_weekday'] = train_df.created.dt.dayofweek.astype(float)\n",
    "train_df['num_created_weekofyear'] = train_df.created.dt.weekofyear\n",
    "train_df['num_created_day'] = train_df.created.dt.day\n",
    "train_df['num_created_month'] = train_df.created.dt.month\n",
    "train_df['num_created_hour'] = train_df.created.dt.hour\n",
    "  \n",
    "test_df['created'] = pd.to_datetime(test_df.created)\n",
    "test_df['num_created_weekday'] = test_df.created.dt.dayofweek\n",
    "test_df['num_created_weekofyear'] = test_df.created.dt.weekofyear\n",
    "test_df['num_created_day'] = test_df.created.dt.day\n",
    "test_df['num_created_month'] = test_df.created.dt.month\n",
    "test_df['num_created_hour'] = test_df.created.dt.hour\n",
    "\n",
    "\n",
    "new_feature = ['num_created_weekday','num_created_weekofyear','num_created_day','num_created_month','num_created_hour']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Bedrooms/Bathrooms/Price\n",
    "train_df['num_bathrooms'] = train_df.bathrooms.clip_upper(4)\n",
    "test_df['num_bathrooms'] = test_df.bathrooms.clip_upper(4)\n",
    "\n",
    "train_df['num_bedrooms'] = train_df.bedrooms.clip_upper(5)\n",
    "test_df['num_bedrooms'] = test_df.bedrooms.clip_upper(5)\n",
    "\n",
    "train_df['num_price'] = train_df.price.clip_upper(10000)\n",
    "test_df['num_price'] = test_df.price.clip_upper(10000)\n",
    "\n",
    "bins = train_df.price.quantile(np.arange(0.05, 1, 0.05))\n",
    "train_df['num_price_q'] = np.digitize(train_df.price, bins)\n",
    "test_df['num_price_q'] = np.digitize(test_df.price, bins)\n",
    "\n",
    "\n",
    "new_feature = ['num_bathrooms','num_bedrooms','num_price','num_price_q']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Composite features based on: \n",
    "# https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths\n",
    "train_df['num_priceXroom'] = (train_df.price / (1 + train_df.bedrooms.clip(1, 4) + 0.5*train_df.bathrooms.clip(0, 2))).values\n",
    "test_df['num_priceXroom'] = (test_df.price / (1 + test_df.bedrooms.clip(1, 4) + 0.5*test_df.bathrooms.clip(0, 2))).values\n",
    "\n",
    "train_df['num_even_bathrooms'] = ((np.round(train_df.bathrooms) - train_df.bathrooms)==0).astype(float)\n",
    "test_df['num_even_bathrooms'] = ((np.round(test_df.bathrooms) - test_df.bathrooms)==0).astype(float)\n",
    "\n",
    "new_feature = ['num_priceXroom','num_even_bathrooms']\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\",'position']\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            if f not in features_to_use:\n",
    "                features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dftemp = train_df.copy()\n",
    "for i in ['latitude', 'longitude']:\n",
    "    while(1):\n",
    "        x = dftemp[i].median()\n",
    "        ix = abs(dftemp[i] - x) > 3*dftemp[i].std()\n",
    "        if ix.sum()==0:\n",
    "            break\n",
    "        dftemp.loc[ix, i] = np.nan\n",
    "dftemp = dftemp.loc[dftemp[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n",
    "\n",
    "dfm = DataFrameMapper([(['latitude'], [StandardScaler()]), (['longitude'], [StandardScaler()])])\n",
    "\n",
    "for i in [6]:\n",
    "    pipe_location = make_pipeline(dfm, KMeans(n_clusters=i, random_state=1))\n",
    "    pipe_location.fit(dftemp);\n",
    "    train_df['location_'+str(i)] = pipe_location.predict(train_df).astype(str)\n",
    "    test_df['location_'+str(i)] = pipe_location.predict(test_df).astype(str)\n",
    "for i in train_df.location_6.unique():\n",
    "    f = 'num_location_6_'+str(i)\n",
    "    train_df[f] = (train_df.location_6==i).astype(float)\n",
    "    test_df[f] = (test_df.location_6==i).astype(float)\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "    \n",
    "    \n",
    "train_df['tmp_bathrooms'] = train_df.bathrooms.clip_upper(2)\n",
    "test_df['tmp_bathrooms'] = test_df.bathrooms.clip_upper(2)\n",
    "train_df['tmp_bedrooms'] = train_df.bedrooms.clip_upper(4)\n",
    "test_df['tmp_bedrooms'] = test_df.bedrooms.clip_upper(4)\n",
    "train_df['roomcal'] = train_df.tmp_bedrooms.astype(str) + '_' + train_df.tmp_bathrooms.astype(str)    \n",
    "test_df['roomcal'] = test_df.tmp_bedrooms.astype(str) + '_' + test_df.tmp_bathrooms.astype(str)    \n",
    "\n",
    "room_lb = LabelBinarizer()\n",
    "room_lb.fit(train_df['roomcal'])\n",
    "room_col = ['num_room_type_' + str(x) for x in range(len(train_df['roomcal'].unique()))]\n",
    "for f in room_col:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "\n",
    "train_df = train_df.join(pd.DataFrame(room_lb.transform(train_df['roomcal']),columns=room_col,index=train_df.index))\n",
    "test_df = test_df.join(pd.DataFrame(room_lb.transform(test_df['roomcal']),columns=room_col,index=test_df.index))\n",
    "\n",
    "tmp = train_df.groupby(['roomcal','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['roomcal','location_6'],how='left')\n",
    "\n",
    "test_df.loc[27462,'num_6_median_price'] =  7200.0\n",
    "\n",
    "train_df['num_6_price_ratio'] = train_df['num_price'] / train_df['num_6_median_price']\n",
    "train_df['num_6_price_diff'] = train_df['num_price'] - train_df['num_6_median_price']\n",
    "test_df['num_6_price_ratio'] = test_df['num_price'] / test_df['num_6_median_price']\n",
    "test_df['num_6_price_diff'] = test_df['num_price'] - test_df['num_6_median_price']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price','num_6_price_ratio','num_6_price_diff']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)\n",
    "        \n",
    "        \n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = train_df.groupby(['num_bedrooms','location_6'])['num_price'].median().\\\n",
    "            reset_index().rename(columns={'num_price':'num_6_median_price_bedroom'})\n",
    "    \n",
    "train_df = train_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "test_df = test_df.merge(tmp,on=['num_bedrooms','location_6'],how='left')\n",
    "\n",
    "train_df['num_6_price_ratio_bedroom'] = train_df['num_price'] / train_df['num_6_median_price_bedroom']\n",
    "train_df['num_6_price_diff_bedroom'] = train_df['num_price'] - train_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_ratio_bedroom'] = test_df['num_price'] / test_df['num_6_median_price_bedroom']\n",
    "test_df['num_6_price_diff_bedroom'] = test_df['num_price'] - test_df['num_6_median_price_bedroom']\n",
    "\n",
    "\n",
    "for f in ['num_6_median_price_bedroom','num_6_price_ratio_bedroom','num_6_price_diff_bedroom']:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_binary_features(df):\n",
    "    bows = {\n",
    "        \"dogs\": (\"dogs\", \"dog\",'pet friendly','pets'),\n",
    "        \"cats\": (\"cats\",'pet friendly','pets'),\n",
    "        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n",
    "        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n",
    "        \"furnished\": (\"furnished\",'equipped'),\n",
    "        \"parquet\": (\"parquet\", \"hardwood\"),\n",
    "        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n",
    "        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n",
    "        \"laundry\": (\"laundry\", \"lndry\"),\n",
    "        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n",
    "        \"transport\": (\"train\", \"subway\", \"transport\"),\n",
    "        \"parking\": (\"parking\",),\n",
    "        \"utilities\": (\"utilities\", \"heat water\", \"water included\"),\n",
    "        'fireplace': ('fireplace','fireplaces'),\n",
    "        'elevator': ('elevator'),\n",
    "        'pool':('pool'),\n",
    "        'loft':('loft'),\n",
    "        'luxury':('luxury','valet'),\n",
    "        'marble':('marble'),\n",
    "        'onemounthfree': ('1 month free','one month free'),\n",
    "        'washer':('washer','dryer')\n",
    "    }\n",
    "\n",
    "    def indicator(bow):\n",
    "        return lambda s: int(any([x in s for x in bow]))\n",
    "\n",
    "    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n",
    "    for key in bows:\n",
    "        tmp_key = \"feature_\" + key\n",
    "        df[tmp_key] = features.apply(indicator(bows[key]))\n",
    "        if tmp_key not in features_to_use:\n",
    "            features_to_use.append(tmp_key)\n",
    "    return df\n",
    "\n",
    "# Create binarized features\n",
    "train_df = create_binary_features(train_df)\n",
    "test_df = create_binary_features(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322)\n",
      "(74659, 322)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X_0322 = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X_0322 = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "\n",
    "print train_X_0322.shape\n",
    "print test_X_0322.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment = [\n",
    "#     'building_id_mean_med','building_id_mean_high', \n",
    "#     'manager_id_mean_med','manager_id_mean_high',\n",
    "    'median_price_bed', 'ratio_bed',\n",
    "       'compound', 'neg', 'neu', 'pos', 'street',\n",
    "       'avenue', 'east', 'west', 'north', 'south', 'other_address',\n",
    "       'Zero_building_id', 'top_10_building', 'top_25_building',\n",
    "       'top_5_building', 'top_50_building', 'top_1_building',\n",
    "       'top_2_building', 'top_15_building', 'top_20_building',\n",
    "       'top_30_building','listing_id'\n",
    "]\n",
    "\n",
    "train_df = train_df.merge(train_X_0322[sentiment],on='listing_id', how='left')\n",
    "test_df = test_df.merge(test_X_0322[sentiment],on='listing_id', how='left')\n",
    "\n",
    "for f in sentiment:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index=list(range(train_df.shape[0]))\n",
    "# random.shuffle(index)\n",
    "# a=[np.nan]*len(train_df)\n",
    "# b=[np.nan]*len(train_df)\n",
    "# c=[np.nan]*len(train_df)\n",
    "\n",
    "# for i in range(5):\n",
    "#     building_level={}\n",
    "#     for j in train_df['building_id'].values:\n",
    "#         building_level[j]=[0,0,0]\n",
    "#     test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "#     train_index=list(set(index).difference(test_index))\n",
    "#     for j in train_index:\n",
    "#         temp=train_df.iloc[j]\n",
    "#         if temp['interest_level']=='low':\n",
    "#             building_level[temp['building_id']][0]+=1\n",
    "#         if temp['interest_level']=='medium':\n",
    "#             building_level[temp['building_id']][1]+=1\n",
    "#         if temp['interest_level']=='high':\n",
    "#             building_level[temp['building_id']][2]+=1\n",
    "#     for j in test_index:\n",
    "#         temp=train_df.iloc[j]\n",
    "#         if sum(building_level[temp['building_id']])!=0:\n",
    "#             a[j]=building_level[temp['building_id']][0]*1.0/sum(building_level[temp['building_id']])\n",
    "#             b[j]=building_level[temp['building_id']][1]*1.0/sum(building_level[temp['building_id']])\n",
    "#             c[j]=building_level[temp['building_id']][2]*1.0/sum(building_level[temp['building_id']])\n",
    "# train_df['building_level_low']=a\n",
    "# train_df['building_level_medium']=b\n",
    "# train_df['building_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "# a=[]\n",
    "# b=[]\n",
    "# c=[]\n",
    "# building_level={}\n",
    "# for j in train_df['building_id'].values:\n",
    "#     building_level[j]=[0,0,0]\n",
    "# for j in range(train_df.shape[0]):\n",
    "#     temp=train_df.iloc[j]\n",
    "#     if temp['interest_level']=='low':\n",
    "#         building_level[temp['building_id']][0]+=1\n",
    "#     if temp['interest_level']=='medium':\n",
    "#         building_level[temp['building_id']][1]+=1\n",
    "#     if temp['interest_level']=='high':\n",
    "#         building_level[temp['building_id']][2]+=1\n",
    "\n",
    "# for i in test_df['building_id'].values:\n",
    "#     if i not in building_level.keys():\n",
    "#         a.append(np.nan)\n",
    "#         b.append(np.nan)\n",
    "#         c.append(np.nan)\n",
    "#     else:\n",
    "#         a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "#         b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "#         c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "# test_df['building_level_low']=a\n",
    "# test_df['building_level_medium']=b\n",
    "# test_df['building_level_high']=c\n",
    "\n",
    "# features_to_use.append('building_level_low') \n",
    "# features_to_use.append('building_level_medium') \n",
    "# features_to_use.append('building_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','price')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_created_hour Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_created_hour')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff_bedroom')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  bedrooms Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','bedrooms')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_photo_count Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_photo_count')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  Zero_building_id Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','Zero_building_id')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  top_25_building Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','top_25_building')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  feature_nofee Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','feature_nofee')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  longitude Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','longitude')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  latitude Done!\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','latitude')\n",
    "for f in new_feature:\n",
    "    if f not in features_to_use:\n",
    "        features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in new_feature:\n",
    "    features_to_use.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerRoom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerRoom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_features Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_features')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_6_price_diff_bedroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_created_hour Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_created_hour')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  ratio_bed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','ratio_bed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  pos Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','pos')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  bathrooms Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','bathrooms')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_ratio_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_ratio_bedroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_desc_wordcount Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_desc_wordcount')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_dist_from_center Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_dist_from_center')\n",
    "\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','price')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  building_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','building_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  building_id_mean_high Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','building_id_mean_high')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  manager_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','manager_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  manager_id_mean_high Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','manager_id_mean_high')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_nofee  vs  num_price Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'feature_nofee','num_price')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'num_building_null','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'building_id','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  feature_nofee Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','feature_nofee')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  manager_id_mean_med Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','manager_id_mean_med')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position  vs  price Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'position','price')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  median_price_bed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','median_price_bed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pricePerBed Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pricePerBed')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_pos_density Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_pos_density')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_ratio Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_ratio')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_6_price_diff_bedroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_6_price_diff_bedroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manager_id  vs  num_priceXroom Done!\n"
     ]
    }
   ],
   "source": [
    "# train_df, test_df, new_feature = CV_st(train_df,test_df,'manager_id','num_priceXroom')\n",
    "# for f in new_feature:\n",
    "#     if f not in features_to_use:\n",
    "#         features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manager_id_longitude_medium_min',\n",
       " 'manager_id_longitude_high_min',\n",
       " 'manager_id_latitude_low_median',\n",
       " 'manager_id_latitude_medium_median',\n",
       " 'manager_id_latitude_high_median',\n",
       " 'manager_id_latitude_low_mean',\n",
       " 'manager_id_latitude_medium_mean',\n",
       " 'manager_id_latitude_high_mean',\n",
       " 'manager_id_latitude_low_max',\n",
       " 'manager_id_latitude_medium_max',\n",
       " 'manager_id_latitude_high_max',\n",
       " 'manager_id_latitude_low_min',\n",
       " 'manager_id_latitude_medium_min',\n",
       " 'manager_id_latitude_high_min']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_use[-14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 222) (74659, 222)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n",
    "\n",
    "train_X.replace(np.inf, np.nan)\n",
    "test_X.replace(np.inf, np.nan)\n",
    "\n",
    "train_X.loc[:,'num_nan'] = train_X.isnull().sum(axis=1)\n",
    "test_X.loc[:,'num_nan'] = test_X.isnull().sum(axis=1)\n",
    "\n",
    "target_num_map = {'high':2, 'medium':1, 'low':0}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print train_X.shape, test_X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[0]\ttrain-mlogloss:0.907156+0.000620376\ttest-mlogloss:0.912654+0.000696885\n",
      "[10]\ttrain-mlogloss:0.533056+0.00131796\ttest-mlogloss:0.575999+0.00303284\n",
      "[20]\ttrain-mlogloss:0.472661+0.00146713\ttest-mlogloss:0.547372+0.0042181\n",
      "[30]\ttrain-mlogloss:0.434887+0.00160214\ttest-mlogloss:0.537361+0.00440458\n",
      "[40]\ttrain-mlogloss:0.406874+0.0019503\ttest-mlogloss:0.533256+0.00486721\n",
      "[50]\ttrain-mlogloss:0.381179+0.00233676\ttest-mlogloss:0.530998+0.00515014\n",
      "[60]\ttrain-mlogloss:0.3616+0.00254957\ttest-mlogloss:0.529678+0.00591926\n",
      "[70]\ttrain-mlogloss:0.341874+0.00284534\ttest-mlogloss:0.529622+0.00651918\n",
      "[80]\ttrain-mlogloss:0.323864+0.00320415\ttest-mlogloss:0.529633+0.00723308\n",
      "[90]\ttrain-mlogloss:0.307327+0.00272611\ttest-mlogloss:0.530297+0.00755162\n",
      "Stopping. Best iteration:\n",
      "[72]\ttrain-mlogloss:0.338725+0.00259565\ttest-mlogloss:0.529448+0.00668102\n",
      "\n",
      "0.5294482\n",
      "\n",
      "Training :375.89s\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "print cv_train(train_X,train_y,verbose_eval = 10, early_stop = 20)\n",
    "print '\\nTraining :{:0.2f}s'.format(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [80]\ttrain-mlogloss:0.382361+0.00375901\ttest-mlogloss:0.551796+0.00869628 no cv feature\n",
    "# [68]\ttrain-mlogloss:0.375532+0.00356265\ttest-mlogloss:0.536175+0.00677244 manager_level\n",
    "# [63]\ttrain-mlogloss:0.374778+0.0033805\ttest-mlogloss:0.532757+0.00591715 manager_id_price\n",
    "# [74]\ttrain-mlogloss:0.351646+0.00247557\ttest-mlogloss:0.531748+0.0057647 manager_id_num_created_hour\n",
    "# [75]\ttrain-mlogloss:0.343642+0.00376395\ttest-mlogloss:0.530804+0.00643943 manager_id_num_6_price_diff_bedroom\n",
    "# [64]\ttrain-mlogloss:0.3622+0.00462603\ttest-mlogloss:0.52977+0.00690296 manager_id_bedrooms\n",
    "# [70]\ttrain-mlogloss:0.348793+0.00420376\ttest-mlogloss:0.529611+0.00564118 manager_id_num_photo_count\n",
    "# [77]\ttrain-mlogloss:0.332559+0.00268203\ttest-mlogloss:0.529099+0.00596648 manager_id_Zero_building_id\n",
    "# [72]\ttrain-mlogloss:0.342933+0.00285729\ttest-mlogloss:0.529299+0.00573414 manager_id_feature_nofee\n",
    "# [77]\ttrain-mlogloss:0.332598+0.00263115\ttest-mlogloss:0.528572+0.00725079 manager_id_longitude\n",
    "\n",
    "\n",
    "# del\n",
    "# [76]\ttrain-mlogloss:0.337109+0.00469833\ttest-mlogloss:0.529907+0.00624846 manager_id_top_25_building\n",
    "# [61]\ttrain-mlogloss:0.362698+0.00230359\ttest-mlogloss:0.531051+0.00675724 manager_id_num_pricePerRoom\n",
    "# [75]\ttrain-mlogloss:0.337831+0.00452436\ttest-mlogloss:0.531305+0.00772622 manager_id_num_features\n",
    "# [79]\ttrain-mlogloss:0.331988+0.00347054\ttest-mlogloss:0.530181+0.00789851 manager_id_ratio_bed\n",
    "# [69]\ttrain-mlogloss:0.349364+0.00295152\ttest-mlogloss:0.531117+0.0070665 manager_id_num_priceXroom\n",
    "# [78]\ttrain-mlogloss:0.334089+0.00247944\ttest-mlogloss:0.530873+0.00635053 manager_id_num_6_price_diff\n",
    "# [73]\ttrain-mlogloss:0.341803+0.00279442\ttest-mlogloss:0.530613+0.00647791 manager_id_pos\n",
    "# [73]\ttrain-mlogloss:0.347647+0.0021376\ttest-mlogloss:0.53096+0.00579744 manager_id_bathrooms\n",
    "# [71]\ttrain-mlogloss:0.348451+0.00293866\ttest-mlogloss:0.53033+0.0060853 manager_id_num_bedrooms\n",
    "# [67]\ttrain-mlogloss:0.356988+0.00434169\ttest-mlogloss:0.531804+0.00627801 manager_id_num_6_price_ratio_bedroom\n",
    "# [74]\ttrain-mlogloss:0.347034+0.00154416\ttest-mlogloss:0.532934+0.00687755 manager_id_num_desc_wordcount\n",
    "# [64]\ttrain-mlogloss:0.36658+0.00283495\ttest-mlogloss:0.532886+0.00594258 manager_id_num_dist_from_center\n",
    "# [68]\ttrain-mlogloss:0.356656+0.0036973\ttest-mlogloss:0.533344+0.0061344 building_id_price\n",
    "# [72]\ttrain-mlogloss:0.361483+0.00316415\ttest-mlogloss:0.536714+0.00584744 building_level\n",
    "# [67]\ttrain-mlogloss:0.373729+0.00305937\ttest-mlogloss:0.536255+0.00708668 'building_id_mean_med','building_id_mean_high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [81]\ttrain-mlogloss:0.354253+0.00211099\ttest-mlogloss:0.536977+0.00608719 no cv feature\n",
    "# [65]\ttrain-mlogloss:0.367155+0.00211633\ttest-mlogloss:0.534893+0.0062382 price\n",
    "# [72]\ttrain-mlogloss:0.346812+0.00328497\ttest-mlogloss:0.53305+0.00688662 building_id  vs  price\n",
    "# [64]\ttrain-mlogloss:0.357396+0.00288563\ttest-mlogloss:0.531704+0.00625184 num_dist_from_center\n",
    "# [76]\ttrain-mlogloss:0.331548+0.00150892\ttest-mlogloss:0.530707+0.00719767 num_created_hour\n",
    "# [78]\ttrain-mlogloss:0.327187+0.00326905\ttest-mlogloss:0.530522+0.00662889 building_id  vs  num_created_hour\n",
    "# [69]\ttrain-mlogloss:0.341823+0.0009883\ttest-mlogloss:0.530539+0.00755474 num_desc_wordcount\n",
    "# del [64]\ttrain-mlogloss:0.348393+0.00437276\ttest-mlogloss:0.532399+0.00560396 building_id  vs  num_desc_wordcount\n",
    "# [78]\ttrain-mlogloss:0.322085+0.00255577\ttest-mlogloss:0.530573+0.00706681 num_6_price_diff_bedroom\n",
    "# [76]\ttrain-mlogloss:0.322108+0.00188804\ttest-mlogloss:0.53018+0.00868863 building_id  vs  num_6_price_diff_bedroom\n",
    "# [57]\ttrain-mlogloss:0.35853+0.0039214\ttest-mlogloss:0.530243+0.00654921 bedroom\n",
    "# del [80]\ttrain-mlogloss:0.313073+0.00177185\ttest-mlogloss:0.530923+0.00642305 building_id  vs  bedroom\n",
    "# [80]\ttrain-mlogloss:0.311336+0.00318315\ttest-mlogloss:0.530211+0.00570428 num_6_price_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.921154\n",
      "[20]\tvalidation_0-mlogloss:0.558762\n",
      "[40]\tvalidation_0-mlogloss:0.544735\n",
      "[60]\tvalidation_0-mlogloss:0.543177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.3,\n",
       "       gamma=0, learning_rate=0.3, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=None, n_estimators=77, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=1, subsample=0.7)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, train_size=.80, random_state=2016)\n",
    "rgr = xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            seed = 0, # use a fixed seed during tuning so we can reproduce the results\n",
    "            learning_rate = 0.3,\n",
    "            n_estimators = 77,\n",
    "            max_depth= 6,\n",
    "            nthread = -1,\n",
    "            colsample_bytree = 0.3,\n",
    "            subsample =0.7,\n",
    "            silent = 1\n",
    "        )\n",
    "rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=20,\n",
    "        verbose=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(rgr, feature_names=X_train.columns, OutputXlsxFile = '../FE/FI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[312]\ttrain-mlogloss:0.395492+0.00170774\ttest-mlogloss:0.527702+0.00663205\n",
      "\n",
      "3 \t0.5277018\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[141]\ttrain-mlogloss:0.396861+0.00183942\ttest-mlogloss:0.527621+0.00736431\n",
      "\n",
      "4 \t0.5276214\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[102]\ttrain-mlogloss:0.357724+0.00219303\ttest-mlogloss:0.528282+0.00654119\n",
      "\n",
      "5 \t0.5282822\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[57]\ttrain-mlogloss:0.35853+0.0039214\ttest-mlogloss:0.530243+0.00654921\n",
      "\n",
      "6 \t0.530243\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[59]\ttrain-mlogloss:0.283875+0.00317202\ttest-mlogloss:0.535384+0.00751206\n",
      "\n",
      "7 \t0.5353838\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[34]\ttrain-mlogloss:0.291909+0.00288588\ttest-mlogloss:0.538363+0.00620757\n",
      "\n",
      "8 \t0.538363\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[26]\ttrain-mlogloss:0.267564+0.0038825\ttest-mlogloss:0.541642+0.00515766\n",
      "\n",
      "9 \t0.541642\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[20]\ttrain-mlogloss:0.244491+0.00434577\ttest-mlogloss:0.546812+0.0055212\n",
      "\n",
      "10 \t0.5468122\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[20]\ttrain-mlogloss:0.195892+0.00375725\ttest-mlogloss:0.550402+0.00728209\n",
      "\n",
      "11 \t0.5504024\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[17]\ttrain-mlogloss:0.17067+0.00195634\ttest-mlogloss:0.557329+0.00518613\n",
      "\n",
      "12 \t0.557329\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[15]\ttrain-mlogloss:0.149427+0.00259833\ttest-mlogloss:0.562298+0.00633625\n",
      "\n",
      "13 \t0.5622976\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\ttrain-mlogloss:0.126763+0.00196544\ttest-mlogloss:0.569518+0.00782982\n",
      "\n",
      "14 \t0.5695176\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\ttrain-mlogloss:0.10453+0.00319293\ttest-mlogloss:0.572249+0.00606784\n",
      "\n",
      "15 \t0.5722486\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000\n",
    "for x in [3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "\n",
    "    tmp = cv_train(train_X,train_y,max_depth = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# max_depth = train_param\n",
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[186]\ttrain-mlogloss:0.367066+0.00186211\ttest-mlogloss:0.527145+0.0074838\n",
      "\n",
      "2 \t0.5271452\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[213]\ttrain-mlogloss:0.352706+0.00230145\ttest-mlogloss:0.526443+0.00732842\n",
      "\n",
      "4 \t0.5264426\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[159]\ttrain-mlogloss:0.390464+0.00279415\ttest-mlogloss:0.527391+0.00724981\n",
      "\n",
      "8 \t0.5273912\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[177]\ttrain-mlogloss:0.381805+0.00237731\ttest-mlogloss:0.526913+0.00594032\n",
      "\n",
      "12 \t0.5269126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[231]\ttrain-mlogloss:0.354327+0.00325227\ttest-mlogloss:0.525994+0.00717634\n",
      "\n",
      "16 \t0.5259938\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[159]\ttrain-mlogloss:0.398036+0.00265041\ttest-mlogloss:0.527198+0.00718988\n",
      "\n",
      "20 \t0.5271978\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[167]\ttrain-mlogloss:0.393573+0.00269923\ttest-mlogloss:0.526977+0.00679354\n",
      "\n",
      "24 \t0.5269766\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[179]\ttrain-mlogloss:0.388478+0.00255418\ttest-mlogloss:0.527911+0.00768457\n",
      "\n",
      "28 \t0.527911\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[189]\ttrain-mlogloss:0.385292+0.00262723\ttest-mlogloss:0.527001+0.00775232\n",
      "\n",
      "32 \t0.5270012\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[189]\ttrain-mlogloss:0.389593+0.00185931\ttest-mlogloss:0.527247+0.00665882\n",
      "\n",
      "40 \t0.5272468\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.387127+0.00289643\ttest-mlogloss:0.526661+0.00611834\n",
      "\n",
      "48 \t0.5266614\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[195]\ttrain-mlogloss:0.396135+0.00319157\ttest-mlogloss:0.526655+0.00698115\n",
      "\n",
      "64 \t0.5266546\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-mlogloss:0.404784+0.002127\ttest-mlogloss:0.527635+0.00737379\n",
      "\n",
      "80 \t0.5276348\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-mlogloss:0.40653+0.00239865\ttest-mlogloss:0.528011+0.00761088\n",
      "\n",
      "128 \t0.5280108\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20,24,28,32,40,48,64,80,128]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "    \n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[262]\ttrain-mlogloss:0.395672+0.00192811\ttest-mlogloss:0.532532+0.00697024\n",
      "\n",
      "0.05 \t0.5325316\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[251]\ttrain-mlogloss:0.38311+0.00308902\ttest-mlogloss:0.529113+0.00529031\n",
      "\n",
      "0.1 \t0.5291126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[212]\ttrain-mlogloss:0.387354+0.0026151\ttest-mlogloss:0.526019+0.00693851\n",
      "\n",
      "0.2 \t0.5260188\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.384431+0.00255739\ttest-mlogloss:0.526169+0.00587901\n",
      "\n",
      "0.3 \t0.5261692\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.381644+0.00252687\ttest-mlogloss:0.526699+0.00705682\n",
      "\n",
      "0.4 \t0.5266986\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-mlogloss:0.377252+0.00326099\ttest-mlogloss:0.527301+0.00618384\n",
      "\n",
      "0.5 \t0.5273014\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[218]\ttrain-mlogloss:0.367054+0.00132444\ttest-mlogloss:0.526421+0.00760733\n",
      "\n",
      "0.6 \t0.5264212\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[219]\ttrain-mlogloss:0.364027+0.00250635\ttest-mlogloss:0.526563+0.00715006\n",
      "\n",
      "0.7 \t0.5265626\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[168]\ttrain-mlogloss:0.392891+0.00148548\ttest-mlogloss:0.526076+0.00678444\n",
      "\n",
      "0.8 \t0.5260764\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[192]\ttrain-mlogloss:0.376406+0.00267377\ttest-mlogloss:0.526586+0.00671319\n",
      "\n",
      "0.9 \t0.5265856\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, colsample_bytree = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[114]\ttrain-mlogloss:0.429981+0.00163237\ttest-mlogloss:0.535683+0.0063162\n",
      "\n",
      "0.5 \t0.5356834\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[127]\ttrain-mlogloss:0.417173+0.00106604\ttest-mlogloss:0.532641+0.0071292\n",
      "\n",
      "0.6 \t0.5326408\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[117]\ttrain-mlogloss:0.422807+0.00248712\ttest-mlogloss:0.530766+0.00658263\n",
      "\n",
      "0.7 \t0.5307656\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[138]\ttrain-mlogloss:0.406591+0.002293\ttest-mlogloss:0.52841+0.00609406\n",
      "\n",
      "0.8 \t0.5284098\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.378796+0.00269963\ttest-mlogloss:0.528253+0.00591205\n",
      "\n",
      "0.9 \t0.528253\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = colsample_bytree,\n",
    "#         subsample = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, \n",
    "                   colsample_bytree = colsample_bytree, subsample = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[164]\ttrain-mlogloss:0.393296+0.00247187\ttest-mlogloss:0.526782+0.00697394\n",
      "\n",
      "0.3 \t0.5267822\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[186]\ttrain-mlogloss:0.378468+0.00338923\ttest-mlogloss:0.526772+0.00705368\n",
      "\n",
      "0.6 \t0.526772\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[175]\ttrain-mlogloss:0.38681+0.00159871\ttest-mlogloss:0.526846+0.00696989\n",
      "\n",
      "0.9 \t0.526846\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[167]\ttrain-mlogloss:0.390969+0.002374\ttest-mlogloss:0.527127+0.00787015\n",
      "\n",
      "1.2 \t0.527127\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[206]\ttrain-mlogloss:0.370844+0.00353107\ttest-mlogloss:0.526113+0.00799753\n",
      "\n",
      "1.5 \t0.5261126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[227]\ttrain-mlogloss:0.375197+0.0107539\ttest-mlogloss:0.526744+0.0071078\n",
      "\n",
      "1.8 \t0.5267444\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[193]\ttrain-mlogloss:0.386555+0.00438174\ttest-mlogloss:0.526173+0.00798028\n",
      "\n",
      "2.1 \t0.5261726\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[161]\ttrain-mlogloss:0.411347+0.00406703\ttest-mlogloss:0.526713+0.00664277\n",
      "\n",
      "2.4 \t0.5267126\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-mlogloss:0.419464+0.00941197\ttest-mlogloss:0.527696+0.00723329\n",
      "\n",
      "2.7 \t0.527696\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.425483+0.00607894\ttest-mlogloss:0.52768+0.00621536\n",
      "\n",
      "3.0 \t0.5276796\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "#     rgr = xgb.XGBClassifier(\n",
    "#         objective='multi:softprob',\n",
    "#         seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "#         learning_rate = learning_rate,\n",
    "#         n_estimators = 10000,\n",
    "#         max_depth= max_depth,\n",
    "#         nthread = -1,\n",
    "#         silent = False,\n",
    "#         min_child_weight = min_child_weight,\n",
    "#         colsample_bytree = colsample_bytree,\n",
    "#         subsample = subsample,\n",
    "#         gamma = x\n",
    "#     )\n",
    "#     rgr.fit(\n",
    "#         X_train,y_train,\n",
    "#         eval_set=[(X_val,y_val)],\n",
    "#         eval_metric='mlogloss',\n",
    "#         early_stopping_rounds=50,\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "    tmp = cv_train(train_X,train_y,max_depth = max_depth,min_child_weight = min_child_weight, \n",
    "                   colsample_bytree = colsample_bytree, subsample = subsample, gamma = x)\n",
    "    if  tmp < best_score:\n",
    "        best_score = tmp\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[683]\ttrain-mlogloss:0.385311+0.00256115\ttest-mlogloss:0.524352+0.0086797\n",
      "\n",
      "    1 | 25m35s | \u001b[35m  -0.52435\u001b[0m | \u001b[32m            0.7361\u001b[0m | \u001b[32m   0.6388\u001b[0m | \u001b[32m     4.7833\u001b[0m | \u001b[32m           42.5554\u001b[0m | \u001b[32m     0.7012\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[379]\ttrain-mlogloss:0.371498+0.00244822\ttest-mlogloss:0.523748+0.00712916\n",
      "\n",
      "    2 | 19m33s | \u001b[35m  -0.52375\u001b[0m | \u001b[32m            0.6535\u001b[0m | \u001b[32m   0.1591\u001b[0m | \u001b[32m     6.5813\u001b[0m | \u001b[32m           74.1271\u001b[0m | \u001b[32m     0.7255\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[398]\ttrain-mlogloss:0.368655+0.00274665\ttest-mlogloss:0.523218+0.00840303\n",
      "\n",
      "    3 | 12m51s | \u001b[35m  -0.52322\u001b[0m | \u001b[32m            0.3779\u001b[0m | \u001b[32m   0.9005\u001b[0m | \u001b[32m     6.2826\u001b[0m | \u001b[32m           68.0104\u001b[0m | \u001b[32m     0.9546\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[269]\ttrain-mlogloss:0.35037+0.00316001\ttest-mlogloss:0.524075+0.00772591\n",
      "\n",
      "    4 | 20m11s |   -0.52407 |             0.7917 |    1.3551 |      7.4449 |            50.3459 |      0.7147 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[672]\ttrain-mlogloss:0.38005+0.00201499\ttest-mlogloss:0.523778+0.00868596\n",
      "\n",
      "    5 | 23m41s |   -0.52378 |             0.7041 |    1.0773 |      4.1062 |            27.3607 |      0.8518 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.313795+0.00358439\ttest-mlogloss:0.524051+0.0072467\n",
      "\n",
      "    6 | 16m47s |   -0.52405 |             0.7083 |    0.1812 |      8.8685 |            28.0142 |      0.8443 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1401]\ttrain-mlogloss:0.428435+0.00404845\ttest-mlogloss:0.526492+0.00819496\n",
      "\n",
      "    7 | 38m32s |   -0.52649 |             0.7629 |    2.2796 |      3.4427 |            57.4304 |      0.8171 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[988]\ttrain-mlogloss:0.441255+0.00239757\ttest-mlogloss:0.527395+0.00804887\n",
      "\n",
      "    8 | 33m34s |   -0.52739 |             0.9578 |    2.1283 |      3.2961 |            74.8363 |      0.9400 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[670]\ttrain-mlogloss:0.39692+0.00264908\ttest-mlogloss:0.524955+0.00845361\n",
      "\n",
      "    9 | 30m20s |   -0.52496 |             0.9428 |    0.6881 |      4.3508 |            72.8161 |      0.7739 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[487]\ttrain-mlogloss:0.364805+0.00177709\ttest-mlogloss:0.522595+0.00857343\n",
      "\n",
      "   10 | 18m00s | \u001b[35m  -0.52260\u001b[0m | \u001b[32m            0.4586\u001b[0m | \u001b[32m   2.7413\u001b[0m | \u001b[32m     6.3533\u001b[0m | \u001b[32m           43.8608\u001b[0m | \u001b[32m     0.9465\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[307]\ttrain-mlogloss:0.283905+0.000514128\ttest-mlogloss:0.523373+0.00725717\n",
      "\n",
      "   11 | 27m39s |   -0.52337 |             0.7037 |    2.8816 |      9.9096 |             8.1048 |      0.7692 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1174]\ttrain-mlogloss:0.402384+0.00165035\ttest-mlogloss:0.52382+0.00809158\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.61732364e-05]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 14m22s |   -0.52382 |             0.2361 |    0.3772 |      3.0095 |             8.7551 |      0.9196 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[258]\ttrain-mlogloss:0.353133+0.00235446\ttest-mlogloss:0.524525+0.00757331\n",
      "\n",
      "   13 | 30m55s |   -0.52453 |             0.9911 |    2.9391 |      9.9607 |            67.0525 |      0.7844 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[494]\ttrain-mlogloss:0.330845+0.00187125\ttest-mlogloss:0.521793+0.00796687\n",
      "\n",
      "   14 | 13m54s | \u001b[35m  -0.52179\u001b[0m | \u001b[32m            0.2144\u001b[0m | \u001b[32m   2.9898\u001b[0m | \u001b[32m     8.4786\u001b[0m | \u001b[32m           18.8402\u001b[0m | \u001b[32m     0.7721\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[386]\ttrain-mlogloss:0.339613+0.000902377\ttest-mlogloss:0.523233+0.00718304\n",
      "\n",
      "   15 | 12m05s |   -0.52323 |             0.2042 |    2.8758 |      9.8409 |            36.8153 |      0.9384 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[159]\ttrain-mlogloss:0.284338+0.00268561\ttest-mlogloss:0.526823+0.00724934\n",
      "\n",
      "   16 | 06m30s |   -0.52682 |             0.2086 |    0.0041 |      9.9875 |            14.4153 |      0.8819 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2573]\ttrain-mlogloss:0.428487+0.00593941\ttest-mlogloss:0.524756+0.0079169\n",
      "\n",
      "   17 | 82m11s |   -0.52476 |             0.9286 |    2.9771 |      3.5201 |            16.5413 |      0.9989 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[459]\ttrain-mlogloss:0.33643+0.00143619\ttest-mlogloss:0.520883+0.00677963\n",
      "\n",
      "   18 | 14m30s | \u001b[35m  -0.52088\u001b[0m | \u001b[32m            0.2503\u001b[0m | \u001b[32m   2.9885\u001b[0m | \u001b[32m     8.5578\u001b[0m | \u001b[32m           25.5448\u001b[0m | \u001b[32m     0.7064\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2081]\ttrain-mlogloss:0.427473+0.00234265\ttest-mlogloss:0.524321+0.00766473\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -8.87013011e-05]), 'nit': 3, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 26m12s |   -0.52432 |             0.2673 |    2.8450 |      3.8539 |             8.0205 |      0.7101 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[242]\ttrain-mlogloss:0.340356+0.00241948\ttest-mlogloss:0.524328+0.00751484\n",
      "\n",
      "   20 | 27m22s |   -0.52433 |             0.9215 |    0.0042 |      9.8197 |            79.6447 |      0.8523 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[267]\ttrain-mlogloss:0.325633+0.00157508\ttest-mlogloss:0.524159+0.00733517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00020906]), 'nit': 6, 'funcalls': 60}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 09m15s |   -0.52416 |             0.2160 |    0.0679 |      9.3548 |            61.0663 |      0.9894 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[238]\ttrain-mlogloss:0.305969+0.00234088\ttest-mlogloss:0.524582+0.0072658\n",
      "\n",
      "   22 | 29m34s |   -0.52458 |             0.9920 |    2.8891 |      9.6503 |            21.7810 |      0.9462 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1734]\ttrain-mlogloss:0.438638+0.00255533\ttest-mlogloss:0.525621+0.00777304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00027419]), 'nit': 6, 'funcalls': 60}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 20m54s |   -0.52562 |             0.2447 |    2.8440 |      3.0257 |            34.4976 |      0.7415 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-mlogloss:0.326697+0.00184619\ttest-mlogloss:0.523681+0.00706285\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -7.78603682e-05]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 09m14s |   -0.52368 |             0.2120 |    1.9111 |      9.9499 |            44.9476 |      0.7173 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[949]\ttrain-mlogloss:0.416746+0.00235725\ttest-mlogloss:0.524177+0.00773736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00013188]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 13m23s |   -0.52418 |             0.2044 |    2.9839 |      4.7569 |            23.0357 |      0.9776 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[699]\ttrain-mlogloss:0.351606+0.0013223\ttest-mlogloss:0.522573+0.00768256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.0006963]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 19m27s |   -0.52257 |             0.2011 |    2.9888 |      9.9404 |            75.9973 |      0.7020 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1719]\ttrain-mlogloss:0.443561+0.00269171\ttest-mlogloss:0.526098+0.00765076\n",
      "\n",
      "   27 | 21m27s |   -0.52610 |             0.2579 |    2.9177 |      3.2913 |            47.1084 |      0.7582 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[568]\ttrain-mlogloss:0.321001+0.00131872\ttest-mlogloss:0.522378+0.0079632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.000118]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 16m50s |   -0.52238 |             0.2065 |    2.9887 |      9.5413 |            29.2594 |      0.7878 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[518]\ttrain-mlogloss:0.345841+0.00149309\ttest-mlogloss:0.522665+0.00721251\n",
      "\n",
      "   29 | 15m28s |   -0.52266 |             0.2036 |    2.9264 |      9.5995 |            55.4239 |      0.8143 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[285]\ttrain-mlogloss:0.329229+0.00192084\ttest-mlogloss:0.523928+0.00766047\n",
      "\n",
      "   30 | 11m14s |   -0.52393 |             0.2617 |    0.0178 |      9.9774 |            72.3185 |      0.9314 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[612]\ttrain-mlogloss:0.379803+0.00217832\ttest-mlogloss:0.524067+0.00782232\n",
      "\n",
      "   31 | 11m04s |   -0.52407 |             0.2081 |    0.1241 |      5.3841 |            79.6966 |      0.7217 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[466]\ttrain-mlogloss:0.358186+0.0026531\ttest-mlogloss:0.522156+0.00754007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00013273]), 'nit': 3, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 | 16m19s |   -0.52216 |             0.3361 |    2.9502 |      7.0673 |            40.0506 |      0.7166 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1240]\ttrain-mlogloss:0.402477+0.00193444\ttest-mlogloss:0.525229+0.00842347\n",
      "\n",
      "   33 | 14m38s |   -0.52523 |             0.2180 |    0.1209 |      3.0021 |            19.2456 |      0.7396 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[717]\ttrain-mlogloss:0.354072+0.00129081\ttest-mlogloss:0.520578+0.00757638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00010343]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34 | 14m57s | \u001b[35m  -0.52058\u001b[0m | \u001b[32m            0.2072\u001b[0m | \u001b[32m   2.9495\u001b[0m | \u001b[32m     6.9237\u001b[0m | \u001b[32m           13.7162\u001b[0m | \u001b[32m     0.7173\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[458]\ttrain-mlogloss:0.325628+0.00128807\ttest-mlogloss:0.52091+0.00684728\n",
      "\n",
      "   35 | 14m17s |   -0.52091 |             0.2379 |    2.9856 |      8.9818 |            12.2886 |      0.7533 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[554]\ttrain-mlogloss:0.332979+0.00097861\ttest-mlogloss:0.520635+0.00822586\n",
      "\n",
      "   36 | 13m44s |   -0.52063 |             0.2088 |    2.8794 |      7.2221 |            10.0298 |      0.7043 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1072]\ttrain-mlogloss:0.410152+0.00186211\ttest-mlogloss:0.52324+0.00787234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00022721]), 'nit': 5, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   37 | 15m05s |   -0.52324 |             0.2022 |    2.9717 |      4.1377 |            13.1347 |      0.7430 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-mlogloss:0.324518+0.00134196\ttest-mlogloss:0.523164+0.00766708\n",
      "\n",
      "   38 | 31m03s |   -0.52316 |             0.9989 |    2.9543 |      7.0194 |            11.9620 |      0.7634 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[336]\ttrain-mlogloss:0.325949+0.00248427\ttest-mlogloss:0.522305+0.00839462\n",
      "\n",
      "   39 | 09m37s |   -0.52231 |             0.2721 |    0.3063 |      6.7152 |             8.0713 |      0.7114 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[563]\ttrain-mlogloss:0.342942+0.0017896\ttest-mlogloss:0.523256+0.00660181\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00031449]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40 | 16m19s |   -0.52326 |             0.2027 |    2.5613 |      9.5142 |            79.9297 |      0.7890 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[812]\ttrain-mlogloss:0.400845+0.00256943\ttest-mlogloss:0.52363+0.00775763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00032815]), 'nit': 3, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41 | 14m37s |   -0.52363 |             0.2135 |    2.8322 |      5.3262 |            63.4206 |      0.7262 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[596]\ttrain-mlogloss:0.370898+0.00193831\ttest-mlogloss:0.521622+0.00796374\n",
      "\n",
      "   42 | 12m28s |   -0.52162 |             0.2011 |    2.8887 |      6.5776 |            27.9240 |      0.8063 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[609]\ttrain-mlogloss:0.334307+0.00154035\ttest-mlogloss:0.520606+0.00790453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0002301]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43 | 14m58s |   -0.52061 |             0.2080 |    2.9926 |      7.0594 |             8.7081 |      0.9898 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[768]\ttrain-mlogloss:0.350951+0.00157895\ttest-mlogloss:0.5212+0.0079181\n",
      "\n",
      "   44 | 16m09s |   -0.52120 |             0.2159 |    2.9281 |      6.4299 |            15.7984 |      0.9352 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[459]\ttrain-mlogloss:0.317504+0.0012678\ttest-mlogloss:0.521366+0.00774674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  8.62735906e-05]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45 | 13m02s |   -0.52137 |             0.2057 |    2.8321 |      8.3789 |            10.7667 |      0.8651 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[565]\ttrain-mlogloss:0.30838+0.00172829\ttest-mlogloss:0.522001+0.00718506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00145797]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46 | 17m12s |   -0.52200 |             0.2071 |    2.9937 |      9.6745 |            15.4274 |      0.9495 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[408]\ttrain-mlogloss:0.339893+0.00236381\ttest-mlogloss:0.522586+0.00807417\n",
      "\n",
      "   47 | 10m50s |   -0.52259 |             0.2536 |    0.0966 |      6.5241 |            33.2196 |      0.7376 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[970]\ttrain-mlogloss:0.380553+0.00163913\ttest-mlogloss:0.522622+0.00759671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00059564]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00020028]), 'nit': 7, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48 | 18m57s |   -0.52262 |             0.2013 |    2.9689 |      6.5378 |            70.6606 |      0.7252 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[586]\ttrain-mlogloss:0.319867+0.000900069\ttest-mlogloss:0.520595+0.00714989\n",
      "\n",
      "   49 | 19m07s |   -0.52060 |             0.3144 |    2.9352 |      7.2775 |             8.9591 |      0.7007 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[368]\ttrain-mlogloss:0.348874+0.00094506\ttest-mlogloss:0.521468+0.00740512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00165902]), 'nit': 3, 'funcalls': 57}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50 | 10m26s |   -0.52147 |             0.2218 |    2.7296 |      7.8701 |            14.6104 |      0.7245 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma): #\n",
    "    params = dict()\n",
    "    params['objective']='multi:softprob'\n",
    "    params['eval_metric']='mlogloss',\n",
    "    params['num_class']=3\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = 0.99# max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=10000, nfold=5,\n",
    "        metrics = 'mlogloss',\n",
    "        seed=1234,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-mlogloss-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (3,10),\n",
    "        'min_child_weight': (8,80),\n",
    "        'colsample_bytree': (0.2,1),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0,3)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=10, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.923701</td>\n",
       "      <td>13.716162</td>\n",
       "      <td>0.207217</td>\n",
       "      <td>0.717320</td>\n",
       "      <td>2.949494</td>\n",
       "      <td>-0.520578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7.277497</td>\n",
       "      <td>8.959092</td>\n",
       "      <td>0.314392</td>\n",
       "      <td>0.700725</td>\n",
       "      <td>2.935174</td>\n",
       "      <td>-0.520595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.059368</td>\n",
       "      <td>8.708084</td>\n",
       "      <td>0.208029</td>\n",
       "      <td>0.989820</td>\n",
       "      <td>2.992590</td>\n",
       "      <td>-0.520606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.222086</td>\n",
       "      <td>10.029754</td>\n",
       "      <td>0.208806</td>\n",
       "      <td>0.704291</td>\n",
       "      <td>2.879361</td>\n",
       "      <td>-0.520635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.557833</td>\n",
       "      <td>25.544802</td>\n",
       "      <td>0.250313</td>\n",
       "      <td>0.706447</td>\n",
       "      <td>2.988536</td>\n",
       "      <td>-0.520883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.981806</td>\n",
       "      <td>12.288637</td>\n",
       "      <td>0.237923</td>\n",
       "      <td>0.753265</td>\n",
       "      <td>2.985601</td>\n",
       "      <td>-0.520910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6.429874</td>\n",
       "      <td>15.798436</td>\n",
       "      <td>0.215938</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>2.928114</td>\n",
       "      <td>-0.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8.378914</td>\n",
       "      <td>10.766739</td>\n",
       "      <td>0.205679</td>\n",
       "      <td>0.865052</td>\n",
       "      <td>2.832059</td>\n",
       "      <td>-0.521366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7.870134</td>\n",
       "      <td>14.610424</td>\n",
       "      <td>0.221777</td>\n",
       "      <td>0.724519</td>\n",
       "      <td>2.729583</td>\n",
       "      <td>-0.521468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6.577559</td>\n",
       "      <td>27.923958</td>\n",
       "      <td>0.201063</td>\n",
       "      <td>0.806311</td>\n",
       "      <td>2.888736</td>\n",
       "      <td>-0.521622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "23   6.923701         13.716162          0.207217   0.717320  2.949494   \n",
       "38   7.277497          8.959092          0.314392   0.700725  2.935174   \n",
       "32   7.059368          8.708084          0.208029   0.989820  2.992590   \n",
       "25   7.222086         10.029754          0.208806   0.704291  2.879361   \n",
       "7    8.557833         25.544802          0.250313   0.706447  2.988536   \n",
       "24   8.981806         12.288637          0.237923   0.753265  2.985601   \n",
       "33   6.429874         15.798436          0.215938   0.935211  2.928114   \n",
       "34   8.378914         10.766739          0.205679   0.865052  2.832059   \n",
       "39   7.870134         14.610424          0.221777   0.724519  2.729583   \n",
       "31   6.577559         27.923958          0.201063   0.806311  2.888736   \n",
       "\n",
       "       score  \n",
       "23 -0.520578  \n",
       "38 -0.520595  \n",
       "32 -0.520606  \n",
       "25 -0.520635  \n",
       "7  -0.520883  \n",
       "24 -0.520910  \n",
       "33 -0.521200  \n",
       "34 -0.521366  \n",
       "39 -0.521468  \n",
       "31 -0.521622  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.to_csv(data_path + 'train_CV_MS_52571.csv',index=False)\n",
    "test_X.to_csv(data_path + 'test_CV_MS_52571.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=1234)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'multi:softprob')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.02)\n",
    "        est.set_params(n_estimators=100000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]      \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(2,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(2,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 5 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.207217,\n",
      "       gamma=2.949494, learning_rate=0.02, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=13, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 1 fold 1\n",
      "best round 4052\n",
      "('Score: ', 0.52759085893433222)\n",
      "Model 1 fold 1 fitting finished in 1156.973s\n",
      "Model 1 fold 2\n",
      "best round 3687\n",
      "('Score: ', 0.51287864554181262)\n",
      "Model 1 fold 2 fitting finished in 1061.310s\n",
      "Model 1 fold 3\n",
      "best round 6012\n",
      "('Score: ', 0.50368627371380925)\n",
      "Model 1 fold 3 fitting finished in 1626.184s\n",
      "Model 1 fold 4\n",
      "best round 5425\n",
      "('Score: ', 0.51214417568970272)\n",
      "Model 1 fold 4 fitting finished in 1469.969s\n",
      "Model 1 fold 5\n",
      "best round 5905\n",
      "('Score: ', 0.49936669306178966)\n",
      "Model 1 fold 5 fitting finished in 1587.994s\n",
      "Model 1 fold 6\n",
      "best round 2615\n",
      "('Score: ', 0.51719954958321612)\n",
      "Model 1 fold 6 fitting finished in 780.861s\n",
      "Model 1 fold 7\n",
      "best round 3589\n",
      "('Score: ', 0.51627375353577398)\n",
      "Model 1 fold 7 fitting finished in 1026.691s\n",
      "Model 1 fold 8\n",
      "best round 3064\n",
      "('Score: ', 0.50809764890404163)\n",
      "Model 1 fold 8 fitting finished in 894.825s\n",
      "Model 1 fold 9\n",
      "best round 4072\n",
      "('Score: ', 0.53397896911909637)\n",
      "Model 1 fold 9 fitting finished in 1170.519s\n",
      "Model 1 fold 10\n",
      "best round 4683\n",
      "('Score: ', 0.52648674184112942)\n",
      "Model 1 fold 10 fitting finished in 1292.764s\n",
      "Score for model 1 is 0.515770\n",
      "Model 2: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.314392,\n",
      "       gamma=2.935174, learning_rate=0.02, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=8, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 2 fold 1\n",
      "best round 4137\n",
      "('Score: ', 0.52726569715085536)\n",
      "Model 2 fold 1 fitting finished in 1800.024s\n",
      "Model 2 fold 2\n",
      "best round 3801\n",
      "('Score: ', 0.51185327578419482)\n",
      "Model 2 fold 2 fitting finished in 1670.753s\n",
      "Model 2 fold 3\n",
      "best round 3267\n",
      "('Score: ', 0.50149414017541449)\n",
      "Model 2 fold 3 fitting finished in 1447.796s\n",
      "Model 2 fold 4\n",
      "best round 5146\n",
      "('Score: ', 0.51138358717764121)\n",
      "Model 2 fold 4 fitting finished in 2151.439s\n",
      "Model 2 fold 5\n",
      "best round 4600\n",
      "('Score: ', 0.49829909346118195)\n",
      "Model 2 fold 5 fitting finished in 1955.498s\n",
      "Model 2 fold 6\n",
      "best round 2495\n",
      "('Score: ', 0.5162797932855876)\n",
      "Model 2 fold 6 fitting finished in 1166.519s\n",
      "Model 2 fold 7\n",
      "best round 2513\n",
      "('Score: ', 0.51622246586882914)\n",
      "Model 2 fold 7 fitting finished in 1171.874s\n",
      "Model 2 fold 8\n",
      "best round 3261\n",
      "('Score: ', 0.50735666661946222)\n",
      "Model 2 fold 8 fitting finished in 1449.922s\n",
      "Model 2 fold 9\n",
      "best round 2126\n",
      "('Score: ', 0.53240770129272252)\n",
      "Model 2 fold 9 fitting finished in 1026.661s\n",
      "Model 2 fold 10\n",
      "best round 3219\n",
      "('Score: ', 0.52711884371566609)\n",
      "Model 2 fold 10 fitting finished in 1432.392s\n",
      "Score for model 2 is 0.514968\n",
      "Model 3: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.208029,\n",
      "       gamma=2.99259, learning_rate=0.02, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=8, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 3 fold 1\n",
      "best round 3384\n",
      "('Score: ', 0.52575844156556684)\n",
      "Model 3 fold 1 fitting finished in 1120.097s\n",
      "Model 3 fold 2\n",
      "best round 2819\n",
      "('Score: ', 0.51274939049045865)\n",
      "Model 3 fold 2 fitting finished in 966.801s\n",
      "Model 3 fold 3\n",
      "best round 5347\n",
      "('Score: ', 0.50249344312364652)\n",
      "Model 3 fold 3 fitting finished in 1672.292s\n",
      "Model 3 fold 4\n",
      "best round 4824\n",
      "('Score: ', 0.51128481398844361)\n",
      "Model 3 fold 4 fitting finished in 1526.251s\n",
      "Model 3 fold 5\n",
      "best round 5816\n",
      "('Score: ', 0.49804300311132055)\n",
      "Model 3 fold 5 fitting finished in 1797.284s\n",
      "Model 3 fold 6\n",
      "best round 2539\n",
      "('Score: ', 0.51665319618457906)\n",
      "Model 3 fold 6 fitting finished in 882.968s\n",
      "Model 3 fold 7\n",
      "best round 2499\n",
      "('Score: ', 0.51611514502799594)\n",
      "Model 3 fold 7 fitting finished in 873.598s\n",
      "Model 3 fold 8\n",
      "best round 4387\n",
      "('Score: ', 0.50698704606872047)\n",
      "Model 3 fold 8 fitting finished in 1399.493s\n",
      "Model 3 fold 9\n",
      "best round 2506\n",
      "('Score: ', 0.53388905643608209)\n",
      "Model 3 fold 9 fitting finished in 877.950s\n",
      "Model 3 fold 10\n",
      "best round 4787\n",
      "('Score: ', 0.52606376468615246)\n",
      "Model 3 fold 10 fitting finished in 1509.743s\n",
      "Score for model 3 is 0.515004\n",
      "Model 4: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.208806,\n",
      "       gamma=2.879361, learning_rate=0.02, max_delta_step=0, max_depth=7,\n",
      "       min_child_weight=10, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 4 fold 1\n",
      "best round 3411\n",
      "('Score: ', 0.5261635898782876)\n",
      "Model 4 fold 1 fitting finished in 1133.746s\n",
      "Model 4 fold 2\n",
      "best round 3800\n",
      "('Score: ', 0.51223490128772153)\n",
      "Model 4 fold 2 fitting finished in 1235.982s\n",
      "Model 4 fold 3\n",
      "best round 4634\n",
      "('Score: ', 0.50262514788683466)\n",
      "Model 4 fold 3 fitting finished in 1497.062s\n",
      "Model 4 fold 4\n",
      "best round 4321\n",
      "('Score: ', 0.51175575674623808)\n",
      "Model 4 fold 4 fitting finished in 1380.887s\n",
      "Model 4 fold 5\n",
      "best round 3665\n",
      "('Score: ', 0.49924060780354024)\n",
      "Model 4 fold 5 fitting finished in 1205.094s\n",
      "Model 4 fold 6\n",
      "best round 2343\n",
      "('Score: ', 0.51661356062107111)\n",
      "Model 4 fold 6 fitting finished in 824.549s\n",
      "Model 4 fold 7\n",
      "best round 2507\n",
      "('Score: ', 0.51610617150733684)\n",
      "Model 4 fold 7 fitting finished in 875.145s\n",
      "Model 4 fold 8\n",
      "best round 3062\n",
      "('Score: ', 0.50678918015227348)\n",
      "Model 4 fold 8 fitting finished in 1029.162s\n",
      "Model 4 fold 9\n",
      "best round 2295\n",
      "('Score: ', 0.53417378429422457)\n",
      "Model 4 fold 9 fitting finished in 817.097s\n",
      "Model 4 fold 10\n",
      "best round 4704\n",
      "('Score: ', 0.52578521408056622)\n",
      "Model 4 fold 10 fitting finished in 1486.594s\n",
      "Score for model 4 is 0.515149\n",
      "Model 5: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.250313,\n",
      "       gamma=2.988536, learning_rate=0.02, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=25, missing=None, n_estimators=100000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99)\n",
      "Model 5 fold 1\n",
      "best round 3742\n",
      "('Score: ', 0.52688827995197907)\n",
      "Model 5 fold 1 fitting finished in 1562.890s\n",
      "Model 5 fold 2\n",
      "best round 3442\n",
      "('Score: ', 0.51136690636681337)\n",
      "Model 5 fold 2 fitting finished in 1452.229s\n",
      "Model 5 fold 3\n",
      "best round 3788\n",
      "('Score: ', 0.50322775302770406)\n",
      "Model 5 fold 3 fitting finished in 1583.234s\n",
      "Model 5 fold 4\n",
      "best round 3475\n",
      "('Score: ', 0.51273378992057261)\n",
      "Model 5 fold 4 fitting finished in 1469.834s\n",
      "Model 5 fold 5\n",
      "best round 3730\n",
      "('Score: ', 0.49941825351957725)\n",
      "Model 5 fold 5 fitting finished in 1563.169s\n",
      "Model 5 fold 6\n",
      "best round 2265\n",
      "('Score: ', 0.51607209683151534)\n",
      "Model 5 fold 6 fitting finished in 1032.791s\n",
      "Model 5 fold 7\n",
      "best round 2861\n",
      "('Score: ', 0.5153661659157367)\n",
      "Model 5 fold 7 fitting finished in 1251.594s\n",
      "Model 5 fold 8\n",
      "best round 2816\n",
      "('Score: ', 0.50791403686937009)\n",
      "Model 5 fold 8 fitting finished in 1229.596s\n",
      "Model 5 fold 9\n",
      "best round 2734\n",
      "('Score: ', 0.53245729391566665)\n",
      "Model 5 fold 9 fitting finished in 1205.052s\n",
      "Model 5 fold 10\n",
      "best round 3407\n",
      "('Score: ', 0.52559120993414699)\n",
      "Model 5 fold 10 fitting finished in 1457.385s\n",
      "Score for model 5 is 0.515104\n",
      "Score for blended models is 0.515199\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "             xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 13,\n",
    "                              colsample_bytree = 0.207217 ,\n",
    "                              subsample = 0.99 ,\n",
    "                              gamma = 2.949494),\n",
    "             xgb.XGBClassifier(max_depth = 7,\n",
    "                              min_child_weight = 8,\n",
    "                              colsample_bytree = 0.314392,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 2.935174),\n",
    "             xgb.XGBClassifier(max_depth = 7,\n",
    "                              min_child_weight = 8,\n",
    "                              colsample_bytree = 0.208029,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 2.992590),         \n",
    "             xgb.XGBClassifier(max_depth = 7,\n",
    "                              min_child_weight = 10,\n",
    "                              colsample_bytree = 0.208806,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 2.879361),  \n",
    "             xgb.XGBClassifier(max_depth = 8,\n",
    "                              min_child_weight = 25,\n",
    "                              colsample_bytree = 0.250313,\n",
    "                              subsample = 0.99,\n",
    "                              gamma = 2.988536)              \n",
    "             ]\n",
    "\n",
    "#  \t \tmax_depth \tmin_child_weight \tcolsample_bytree \tsubsample \tgamma \tscore\n",
    "# 23 \t6.923701 \t13.716162 \t \t \t0.207217 \t \t \t0.717320 \t2.949494 \t-0.520578\n",
    "# 38 \t7.277497 \t8.959092 \t \t \t0.314392 \t \t \t0.700725 \t2.935174 \t-0.520595\n",
    "# 32 \t7.059368 \t8.708084 \t \t \t0.208029 \t \t \t0.989820 \t2.992590 \t-0.520606\n",
    "# 25 \t7.222086 \t10.029754 \t \t \t0.208806 \t \t \t0.704291 \t2.879361 \t-0.520635\n",
    "# 7 \t8.557833 \t25.544802 \t \t \t0.250313 \t \t \t0.706447 \t2.988536 \t-0.520883\n",
    "\n",
    "\n",
    "(train_blend_x_xgb,\n",
    " test_blend_x_xgb_mean,\n",
    " test_blend_x_xgb_gmean,\n",
    " blend_scores_xgb,\n",
    " best_rounds_xgb) = xgb_blend(estimators,\n",
    "                              train_X,train_y,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_blend_x_xgb = pd.DataFrame(train_blend_x_xgb)\n",
    "train_blend_x_xgb.columns = [\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]\n",
    "train_blend_x_xgb[\"listing_id\"] = train_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_mean = pd.DataFrame(test_blend_x_xgb_mean)\n",
    "test_blend_x_xgb_mean.columns = [\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]\n",
    "test_blend_x_xgb_mean[\"listing_id\"] = test_X.listing_id.values\n",
    "\n",
    "test_blend_x_xgb_gmean = pd.DataFrame(test_blend_x_xgb_gmean)\n",
    "test_blend_x_xgb_gmean.columns = [\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]\n",
    "test_blend_x_xgb_gmean[\"listing_id\"] = test_X.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_train = train_X_0322[['listing_id']].merge(train_blend_x_xgb,on = 'listing_id', how = 'left')[[\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]].values\n",
    "tmp_test_mean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_mean,on = 'listing_id', how = 'left')[[\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]].values\n",
    "tmp_test_gmean = test_X_0322[['listing_id']].merge(test_blend_x_xgb_gmean,on = 'listing_id', how = 'left')[[\"low0\", \"medium0\", \"high0\",\"low1\", \"medium1\", \"high1\",\n",
    "                             \"low2\", \"medium2\", \"high2\",\"low3\", \"medium3\", \"high3\",\n",
    "                             \"low4\", \"medium4\", \"high4\",]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51577033  0.51496813  0.51500373  0.51514879  0.51510358]\n",
      "[ 4310.4  3456.5  3890.8  3474.2  3226. ]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_CV_MS_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_mean_CV_MS_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_gmean_CV_MS_BM_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb,axis=0))\n",
    "print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,tmp_train, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,tmp_test_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,tmp_test_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "sub_name = '../output/sub_XGB_mean_CV_MS222_10blend_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(tmp_test_mean[:,3:6])\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = test_X_0322.listing_id.values\n",
    "out_df.to_csv(sub_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
