{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.stats.mstats import gmean\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import skew, boxcox,boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 322) (74659, 322) (49352,)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../input/\"\n",
    "train_X = pd.read_csv(data_path + 'train_BM_MB_add03052240.csv')\n",
    "test_X = pd.read_csv(data_path + 'test_BM_MB_add03052240.csv')\n",
    "train_y = np.ravel(pd.read_csv(data_path + 'labels_BrandenMurray.csv'))\n",
    "sub_id = test_X.listing_id.astype('int32').values\n",
    "# all_features = features_to_use + desc_sparse_cols + feat_sparse_cols\n",
    "print train_X.shape, test_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284\n"
     ]
    }
   ],
   "source": [
    "y_low =[]\n",
    "for i in range(train_X.shape[0]):\n",
    "    y_low.append(1 if train_y[i] == 0 else 0)\n",
    "    \n",
    "y_low = np.array(y_low)  \n",
    "print np.sum(y_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 322)\n",
      "(9871, 322)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, y_low, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.389768 775\n",
      "4 \t0.387766 647\n",
      "5 \t0.386815 470\n",
      "6 \t0.388625 244\n",
      "7 \t0.388481 284\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \t0.39298 164\n",
      "9 \t0.394179 167\n"
     ]
    }
   ],
   "source": [
    "for x in [8,9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t0.388387 458\n",
      "4 \t0.387306 390\n",
      "8 \t0.386451 364\n",
      "12 \t0.387574 431\n",
      "16 \t0.3882 401\n",
      "20 \t0.388992 430\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 \t0.387114 494\n",
      "28 \t0.387909 409\n",
      "32 \t0.389136 560\n"
     ]
    }
   ],
   "source": [
    "for x in [24,28,32]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 \t0.393649 811\n",
      "0.1 \t0.386903 795\n",
      "0.2 \t0.38595 460\n",
      "0.3 \t0.383998 407\n",
      "0.4 \t0.385639 486\n",
      "0.5 \t0.385972 483\n",
      "0.6 \t0.387992 388\n",
      "0.7 \t0.385474 553\n",
      "0.8 \t0.384517 494\n",
      "0.9 \t0.385831 405\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.391043 386\n",
      "0.6 \t0.388263 325\n",
      "0.7 \t0.385446 449\n",
      "0.8 \t0.38811 448\n",
      "0.9 \t0.385152 452\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.38524 566\n",
      "0.6 \t0.385303 444\n",
      "0.9 \t0.386186 409\n",
      "1.2 \t0.385752 465\n",
      "1.5 \t0.385982 407\n",
      "1.8 \t0.384223 498\n",
      "2.1 \t0.384146 462\n",
      "2.4 \t0.38645 536\n",
      "2.7 \t0.386064 420\n",
      "3.0 \t0.387325 346\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.3 \t0.528756 371\n",
    "# 0.6 \t0.530068 353\n",
    "# 0.9 \t0.530043 275\n",
    "# 1.2 \t0.530065 388\n",
    "# 1.5 \t0.529657 331\n",
    "# 1.8 \t0.529906 328\n",
    "# 2.1 \t0.528338 393\n",
    "# 2.4 \t0.529364 372\n",
    "# 2.7 \t0.527919 456\n",
    "# 3.0 \t0.528962 417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[728]\ttrain-logloss:0.318965+0.000298937\ttest-logloss:0.38847+0.0024191\n",
      "\n",
      "    1 | 05m06s | \u001b[35m  -0.38847\u001b[0m | \u001b[32m            0.3385\u001b[0m | \u001b[32m   1.4743\u001b[0m | \u001b[32m     4.3969\u001b[0m | \u001b[32m           30.8001\u001b[0m | \u001b[32m     0.8516\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[376]\ttrain-logloss:0.29796+0.00110782\ttest-logloss:0.387419+0.00227476\n",
      "\n",
      "    2 | 05m50s | \u001b[35m  -0.38742\u001b[0m | \u001b[32m            0.6816\u001b[0m | \u001b[32m   1.9351\u001b[0m | \u001b[32m     5.3094\u001b[0m | \u001b[32m            5.7416\u001b[0m | \u001b[32m     0.8066\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[350]\ttrain-logloss:0.269321+0.00119499\ttest-logloss:0.387256+0.00185888\n",
      "\n",
      "    3 | 04m41s | \u001b[35m  -0.38726\u001b[0m | \u001b[32m            0.4588\u001b[0m | \u001b[32m   0.3509\u001b[0m | \u001b[32m     6.7833\u001b[0m | \u001b[32m            5.8225\u001b[0m | \u001b[32m     0.7612\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[600]\ttrain-logloss:0.312831+0.000881718\ttest-logloss:0.387873+0.00271435\n",
      "\n",
      "    4 | 07m46s |   -0.38787 |             0.7347 |    1.3037 |      4.7766 |            14.7585 |      0.7085 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[310]\ttrain-logloss:0.278785+0.00178672\ttest-logloss:0.387811+0.00280748\n",
      "\n",
      "    5 | 06m28s |   -0.38781 |             0.6813 |    2.3731 |      7.3912 |            25.1307 |      0.9634 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  5.45660480e-05]), 'nit': 5, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[324]\ttrain-logloss:0.248693+0.00337335\ttest-logloss:0.388228+0.00273798\n",
      "\n",
      "    6 | 04m07s |   -0.38823 |             0.2570 |    0.6062 |      7.4735 |             5.5443 |      0.9632 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[262]\ttrain-logloss:0.299482+0.00129517\ttest-logloss:0.391794+0.00165781\n",
      "\n",
      "    7 | 07m49s |   -0.39179 |             0.8926 |    0.3489 |      7.8022 |            31.8117 |      0.6087 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[654]\ttrain-logloss:0.311678+0.000877563\ttest-logloss:0.387039+0.00181198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -6.67813139e-05]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 09m43s | \u001b[35m  -0.38704\u001b[0m | \u001b[32m            0.8620\u001b[0m | \u001b[32m   0.0115\u001b[0m | \u001b[32m     4.2145\u001b[0m | \u001b[32m           22.5101\u001b[0m | \u001b[32m     0.9581\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[803]\ttrain-logloss:0.300608+0.00140029\ttest-logloss:0.387062+0.00253108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -8.10339207e-05]), 'nit': 6, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 04m55s |   -0.38706 |             0.2262 |    0.3218 |      4.0504 |             4.1954 |      0.7397 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[497]\ttrain-logloss:0.328352+0.000756206\ttest-logloss:0.390853+0.00235392\n",
      "\n",
      "   10 | 08m11s |   -0.39085 |             0.8824 |    2.3783 |      4.0122 |            22.3863 |      0.6186 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[639]\ttrain-logloss:0.306761+0.000471879\ttest-logloss:0.386068+0.0025347\n",
      "\n",
      "   11 | 08m41s | \u001b[35m  -0.38607\u001b[0m | \u001b[32m            0.7687\u001b[0m | \u001b[32m   0.0030\u001b[0m | \u001b[32m     4.3620\u001b[0m | \u001b[32m            8.3544\u001b[0m | \u001b[32m     0.9866\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[336]\ttrain-logloss:0.289622+0.00131615\ttest-logloss:0.390133+0.00211692\n",
      "\n",
      "   12 | 03m31s |   -0.39013 |             0.2182 |    0.0133 |      7.8665 |            20.3799 |      0.6522 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[439]\ttrain-logloss:0.286885+0.00133549\ttest-logloss:0.387569+0.00178726\n",
      "\n",
      "   13 | 08m27s |   -0.38757 |             0.8892 |    0.0210 |      5.3780 |             4.0149 |      0.9886 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[831]\ttrain-logloss:0.314762+0.00061501\ttest-logloss:0.387724+0.00241209\n",
      "\n",
      "   14 | 04m59s |   -0.38772 |             0.2340 |    0.0193 |      4.0640 |            27.6260 |      0.9505 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[253]\ttrain-logloss:0.264684+0.00146671\ttest-logloss:0.388121+0.00252873\n",
      "\n",
      "   15 | 07m28s |   -0.38812 |             0.8651 |    0.1531 |      7.9665 |            11.9430 |      0.8767 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[480]\ttrain-logloss:0.323527+0.00101304\ttest-logloss:0.388892+0.00244942\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  8.28604691e-05]), 'nit': 5, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 07m31s |   -0.38889 |             0.8174 |    0.0011 |      4.1763 |            12.0946 |      0.6670 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[820]\ttrain-logloss:0.300991+0.00034811\ttest-logloss:0.386418+0.00201665\n",
      "\n",
      "   17 | 07m25s |   -0.38642 |             0.4661 |    0.0772 |      4.0678 |            17.6464 |      0.9901 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[782]\ttrain-logloss:0.303133+0.000998155\ttest-logloss:0.386062+0.00210512\n",
      "\n",
      "   18 | 05m13s | \u001b[35m  -0.38606\u001b[0m | \u001b[32m            0.2904\u001b[0m | \u001b[32m   0.0751\u001b[0m | \u001b[32m     4.6980\u001b[0m | \u001b[32m            6.4395\u001b[0m | \u001b[32m     0.9887\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[611]\ttrain-logloss:0.291693+0.00104105\ttest-logloss:0.387079+0.00183107\n",
      "\n",
      "   19 | 04m08s |   -0.38708 |             0.2039 |    2.3871 |      5.0039 |             9.8472 |      0.9701 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[304]\ttrain-logloss:0.297085+0.00199466\ttest-logloss:0.387991+0.00275008\n",
      "\n",
      "   20 | 03m34s |   -0.38799 |             0.2698 |    2.2530 |      7.8259 |            29.3403 |      0.9997 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[316]\ttrain-logloss:0.273186+0.000950064\ttest-logloss:0.387335+0.00203391\n",
      "\n",
      "   21 | 08m46s |   -0.38734 |             0.8917 |    0.0064 |      7.2062 |            25.9170 |      0.9845 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[341]\ttrain-logloss:0.274506+0.00116978\ttest-logloss:0.387328+0.0022122\n",
      "\n",
      "   22 | 07m57s |   -0.38733 |             0.8702 |    0.1446 |      6.2976 |             8.2072 |      0.9947 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[276]\ttrain-logloss:0.270409+0.00277392\ttest-logloss:0.387758+0.00272579\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.000783]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -4.18189738e-05]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 07m38s |   -0.38776 |             0.8433 |    2.3496 |      7.9671 |            16.0185 |      0.9537 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[600]\ttrain-logloss:0.303791+0.00104357\ttest-logloss:0.388908+0.00201635\n",
      "\n",
      "   24 | 09m54s |   -0.38891 |             0.8989 |    0.2711 |      4.0468 |             6.4124 |      0.6144 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[423]\ttrain-logloss:0.283377+0.00199258\ttest-logloss:0.387398+0.00301243\n",
      "\n",
      "   25 | 03m47s |   -0.38740 |             0.2267 |    0.0808 |      6.2003 |            15.4702 |      0.9930 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[739]\ttrain-logloss:0.317339+0.000913263\ttest-logloss:0.38747+0.002559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00010221]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 04m54s |   -0.38747 |             0.2614 |    2.2310 |      4.0952 |            17.5443 |      0.9932 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[796]\ttrain-logloss:0.305274+0.000468474\ttest-logloss:0.386951+0.00225619\n",
      "\n",
      "   27 | 11m50s |   -0.38695 |             0.8990 |    2.2081 |      4.9470 |            28.3733 |      0.9805 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[744]\ttrain-logloss:0.312185+0.000751908\ttest-logloss:0.386993+0.00176623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00011342]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 04m31s |   -0.38699 |             0.2275 |    0.0220 |      4.6321 |             9.6112 |      0.9972 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[782]\ttrain-logloss:0.29954+0.000548602\ttest-logloss:0.386928+0.00187042\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -6.83109392e-05]), 'nit': 4, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 06m03s |   -0.38693 |             0.3463 |    2.2058 |      4.0017 |             4.2505 |      0.9322 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[788]\ttrain-logloss:0.308055+0.000789245\ttest-logloss:0.387922+0.00258852\n",
      "\n",
      "   30 | 10m51s |   -0.38792 |             0.7876 |    2.3949 |      4.5742 |            31.9361 |      0.9433 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[939]\ttrain-logloss:0.308478+0.0035556\ttest-logloss:0.386348+0.00196064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00125454]), 'nit': 3, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 | 06m08s |   -0.38635 |             0.2850 |    2.3411 |      4.0639 |             7.9416 |      0.9999 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[843]\ttrain-logloss:0.300769+0.00395044\ttest-logloss:0.386569+0.00222769\n",
      "\n",
      "   32 | 09m58s |   -0.38657 |             0.6673 |    2.3495 |      4.1148 |            12.9803 |      0.9987 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[597]\ttrain-logloss:0.321669+0.000558272\ttest-logloss:0.387577+0.00287125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00123897]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 09m09s |   -0.38758 |             0.8573 |    0.0118 |      4.3225 |            30.9361 |      0.9911 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[490]\ttrain-logloss:0.292768+0.000522952\ttest-logloss:0.38615+0.00245278\n",
      "\n",
      "   34 | 09m33s |   -0.38615 |             0.8838 |    0.1358 |      5.0210 |            19.8662 |      0.9847 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[396]\ttrain-logloss:0.274398+0.000938045\ttest-logloss:0.386871+0.00202166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -6.88782820e-05]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 09m24s |   -0.38687 |             0.8933 |    0.7168 |      6.0019 |            17.1897 |      0.9977 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=y_low) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='binary:logistic'\n",
    "    params['eval_metric']='logloss',\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=100000, nfold=5,\n",
    "        metrics = 'logloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-logloss-mean'].values[-1]\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (4,8),\n",
    "        'min_child_weight': (4,32),\n",
    "        'colsample_bytree': (0.2,0.9),\n",
    "        'subsample': (0.6,1),\n",
    "        'gamma': (0,2.4)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.697965</td>\n",
       "      <td>6.439472</td>\n",
       "      <td>0.290381</td>\n",
       "      <td>0.988734</td>\n",
       "      <td>0.075128</td>\n",
       "      <td>-0.386062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.361961</td>\n",
       "      <td>8.354384</td>\n",
       "      <td>0.768667</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>-0.386068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.020978</td>\n",
       "      <td>19.866215</td>\n",
       "      <td>0.883846</td>\n",
       "      <td>0.984659</td>\n",
       "      <td>0.135751</td>\n",
       "      <td>-0.386150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.063896</td>\n",
       "      <td>7.941608</td>\n",
       "      <td>0.285013</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>2.341089</td>\n",
       "      <td>-0.386348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.067804</td>\n",
       "      <td>17.646352</td>\n",
       "      <td>0.466116</td>\n",
       "      <td>0.990075</td>\n",
       "      <td>0.077247</td>\n",
       "      <td>-0.386418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.114844</td>\n",
       "      <td>12.980270</td>\n",
       "      <td>0.667338</td>\n",
       "      <td>0.998706</td>\n",
       "      <td>2.349501</td>\n",
       "      <td>-0.386569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.001897</td>\n",
       "      <td>17.189702</td>\n",
       "      <td>0.893320</td>\n",
       "      <td>0.997712</td>\n",
       "      <td>0.716788</td>\n",
       "      <td>-0.386871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.001672</td>\n",
       "      <td>4.250499</td>\n",
       "      <td>0.346327</td>\n",
       "      <td>0.932208</td>\n",
       "      <td>2.205770</td>\n",
       "      <td>-0.386928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.946982</td>\n",
       "      <td>28.373279</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.980545</td>\n",
       "      <td>2.208064</td>\n",
       "      <td>-0.386951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.632058</td>\n",
       "      <td>9.611172</td>\n",
       "      <td>0.227460</td>\n",
       "      <td>0.997227</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>-0.386993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "12   4.697965          6.439472          0.290381   0.988734  0.075128   \n",
       "5    4.361961          8.354384          0.768667   0.986623  0.003024   \n",
       "28   5.020978         19.866215          0.883846   0.984659  0.135751   \n",
       "25   4.063896          7.941608          0.285013   0.999906  2.341089   \n",
       "11   4.067804         17.646352          0.466116   0.990075  0.077247   \n",
       "26   4.114844         12.980270          0.667338   0.998706  2.349501   \n",
       "29   6.001897         17.189702          0.893320   0.997712  0.716788   \n",
       "23   4.001672          4.250499          0.346327   0.932208  2.205770   \n",
       "21   4.946982         28.373279          0.899002   0.980545  2.208064   \n",
       "22   4.632058          9.611172          0.227460   0.997227  0.022040   \n",
       "\n",
       "       score  \n",
       "12 -0.386062  \n",
       "5  -0.386068  \n",
       "28 -0.386150  \n",
       "25 -0.386348  \n",
       "11 -0.386418  \n",
       "26 -0.386569  \n",
       "29 -0.386871  \n",
       "23 -0.386928  \n",
       "21 -0.386951  \n",
       "22 -0.386993  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xgb_blend(estimators, train_x, train_y, test_x, fold, early_stopping_rounds=0):\n",
    "    N_params = len(estimators)\n",
    "    print (\"Blend %d estimators for %d folds\" % (N_params, fold))\n",
    "    skf = KFold(n_splits=fold,random_state=seed)\n",
    "    N_class = len(set(train_y))\n",
    "        \n",
    "    train_blend_x = np.zeros((train_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_mean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    test_blend_x_gmean = np.zeros((test_x.shape[0], N_class*N_params))\n",
    "    scores = np.zeros ((fold,N_params))\n",
    "    best_rounds = np.zeros ((fold, N_params))\n",
    "    \n",
    "    for j, est in enumerate(estimators):\n",
    "        est.set_params(objective = 'binary:logistic')\n",
    "        est.set_params(silent = False)\n",
    "        est.set_params(learning_rate = 0.01)\n",
    "        est.set_params(n_estimators=1000000)\n",
    "        \n",
    "        print (\"Model %d: %s\" %(j+1, est))\n",
    "\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], N_class*fold))\n",
    "    \n",
    "        for i, (train_index, val_index) in enumerate(skf.split(train_x)):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            fold_start = time.time() \n",
    "            train_x_fold = train_x.iloc[train_index]\n",
    "            train_y_fold = train_y[train_index]\n",
    "            val_x_fold = train_x.iloc[val_index]\n",
    "            val_y_fold = train_y[val_index]      \n",
    "\n",
    "            est.fit(train_x_fold,train_y_fold,\n",
    "                    eval_set = [(val_x_fold, val_y_fold)],\n",
    "                    eval_metric = 'logloss',\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=False)\n",
    "            best_round=est.best_iteration\n",
    "            best_rounds[i,j]=best_round\n",
    "            print (\"best round %d\" % (best_round))\n",
    "            val_y_predict_fold = est.predict_proba(val_x_fold,ntree_limit=best_round)\n",
    "            score = log_loss(val_y_fold, val_y_predict_fold)\n",
    "            print (\"Score: \", score)\n",
    "            scores[i,j]=score\n",
    "            train_blend_x[val_index, (j*N_class):(j+1)*N_class] = val_y_predict_fold\n",
    "            \n",
    "            test_blend_x_j[:,(i*N_class):(i+1)*N_class] = est.predict_proba(test_x,ntree_limit=best_round)\n",
    "            print (\"Model %d fold %d fitting finished in %0.3fs\" % (j+1,i+1, time.time() - fold_start))\n",
    "            \n",
    "        test_blend_x_mean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([test_blend_x_j[:,range(0,N_class*fold,N_class)].mean(1),\n",
    "                          test_blend_x_j[:,range(1,N_class*fold,N_class)].mean(1)]).T\n",
    "        \n",
    "        test_blend_x_gmean[:,(j*N_class):(j+1)*N_class] = \\\n",
    "                np.stack([gmean(test_blend_x_j[:,range(0,N_class*fold,N_class)], axis=1),\n",
    "                          gmean(test_blend_x_j[:,range(1,N_class*fold,N_class)], axis=1)]).T\n",
    "            \n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(scores[:,j])))\n",
    "    print (\"Score for blended models is %f\" % (np.mean(scores)))\n",
    "    return (train_blend_x, test_blend_x_mean, test_blend_x_gmean, scores,best_rounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.290381,\n",
      "       gamma=0.075128, learning_rate=0.01, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=6, missing=None, n_estimators=1000000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.988734)\n",
      "Model 1 fold 1\n",
      "best round 8144\n",
      "('Score: ', 0.3789331444729267)\n",
      "Model 1 fold 1 fitting finished in 639.601s\n",
      "Model 1 fold 2\n",
      "best round 7940\n",
      "('Score: ', 0.36865721507830884)\n",
      "Model 1 fold 2 fitting finished in 623.079s\n",
      "Model 1 fold 3\n",
      "best round 7523\n",
      "('Score: ', 0.38109002739399661)\n",
      "Model 1 fold 3 fitting finished in 593.384s\n",
      "Model 1 fold 4\n",
      "best round 10223\n",
      "('Score: ', 0.36724477967345648)\n",
      "Model 1 fold 4 fitting finished in 798.468s\n",
      "Model 1 fold 5\n",
      "best round 7520\n",
      "('Score: ', 0.38253252385543984)\n",
      "Model 1 fold 5 fitting finished in 588.520s\n",
      "Model 1 fold 6\n",
      "best round 8744\n",
      "('Score: ', 0.38296750903866883)\n",
      "Model 1 fold 6 fitting finished in 687.892s\n",
      "Model 1 fold 7\n",
      "best round 8218\n",
      "('Score: ', 0.38858317515395041)\n",
      "Model 1 fold 7 fitting finished in 645.807s\n",
      "Model 1 fold 8\n",
      "best round 8720\n",
      "('Score: ', 0.39569315763326529)\n",
      "Model 1 fold 8 fitting finished in 749.834s\n",
      "Model 1 fold 9\n",
      "best round 9604\n",
      "('Score: ', 0.39579261333575988)\n",
      "Model 1 fold 9 fitting finished in 746.654s\n",
      "Model 1 fold 10\n",
      "best round 7578\n",
      "('Score: ', 0.39397986842556709)\n",
      "Model 1 fold 10 fitting finished in 590.306s\n",
      "Score for model 1 is 0.383547\n",
      "Score for blended models is 0.383547\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "            xgb.XGBClassifier(max_depth = 4,\n",
    "                              min_child_weight = 6,\n",
    "                              colsample_bytree = 0.290381 ,\n",
    "                              subsample = 0.988734 ,\n",
    "                              gamma = 0.075128)          \n",
    "             ]\n",
    "#  \t \tmax_depth \tmin_child_weight \tcolsample_bytree \tsubsample \tgamma \t \tscore\n",
    "# 12 \t4.697965 \t6.439472 \t \t \t0.290381 \t \t \t0.988734 \t0.075128 \t-0.386062\n",
    "# 5 \t4.361961 \t8.354384 \t \t \t0.768667 \t \t \t0.986623 \t0.003024 \t-0.386068\n",
    "# 28 \t5.020978 \t19.866215 \t \t \t0.883846 \t \t \t0.984659 \t0.135751 \t-0.386150\n",
    "\n",
    "\n",
    "(train_blend_x_xgb_low,\n",
    " test_blend_x_xgb_mean_low,\n",
    " test_blend_x_xgb_gmean_low,\n",
    " blend_scores_xgb_low,\n",
    " best_rounds_xgb_low) = xgb_blend(estimators,\n",
    "                              train_X,y_low,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46335066,  0.53660144],\n",
       "       [ 0.35060727,  0.64935041],\n",
       "       [ 0.37533208,  0.62466067],\n",
       "       ..., \n",
       "       [ 0.37050161,  0.62949031],\n",
       "       [ 0.33564321,  0.66435498],\n",
       "       [ 0.44256736,  0.55742242]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb_gmean_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3835474]\n",
      "[ 8421.4]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "name_train_blend = '../output/train_blend_xgb_low_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_low_mean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_low_gmean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb_low,axis=0))\n",
    "print (np.mean(best_rounds_xgb_low,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb_low, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_xgb_mean_low, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_xgb_gmean_low, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11229\n"
     ]
    }
   ],
   "source": [
    "y_medium =[]\n",
    "for i in range(train_X.shape[0]):\n",
    "    y_medium.append(1 if train_y[i] == 1 else 0)\n",
    "    \n",
    "y_medium = np.array(y_medium)  \n",
    "print np.sum(y_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 322)\n",
      "(9871, 322)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, y_medium, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.427114 556\n",
      "4 \t0.426936 326\n",
      "5 \t0.425979 341\n",
      "6 \t0.425824 214\n",
      "7 \t0.428035 175\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \t0.428427 138\n",
      "9 \t0.429057 75\n",
      "10 \t0.431596 79\n"
     ]
    }
   ],
   "source": [
    "for x in [8,9,10]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t0.427491 207\n",
      "4 \t0.426616 192\n",
      "8 \t0.426349 213\n",
      "12 \t0.425813 235\n",
      "16 \t0.426269 225\n",
      "20 \t0.425559 261\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 \t0.425494 285\n",
      "28 \t0.425878 249\n",
      "32 \t0.425939 250\n",
      "36 \t0.426459 222\n",
      "40 \t0.425027 228\n"
     ]
    }
   ],
   "source": [
    "for x in [24,28,32,36,40]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 \t0.425912 321\n",
      "60 \t0.424979 284\n",
      "70 \t0.426524 395\n",
      "80 \t0.427186 208\n"
     ]
    }
   ],
   "source": [
    "for x in [50,60,70,80]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 \t0.431676 711\n",
      "0.1 \t0.427319 487\n",
      "0.2 \t0.425365 444\n",
      "0.3 \t0.424966 430\n",
      "0.4 \t0.42466 241\n",
      "0.5 \t0.424707 380\n",
      "0.6 \t0.425919 229\n",
      "0.7 \t0.426333 258\n",
      "0.8 \t0.425071 339\n",
      "0.9 \t0.425589 184\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.431324 235\n",
      "0.6 \t0.429724 241\n",
      "0.7 \t0.428098 292\n",
      "0.8 \t0.426616 378\n",
      "0.9 \t0.425089 271\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.424238 325\n",
      "0.6 \t0.424873 362\n",
      "0.9 \t0.424794 293\n",
      "1.2 \t0.425611 362\n",
      "1.5 \t0.424802 375\n",
      "1.8 \t0.424455 352\n",
      "2.1 \t0.426117 230\n",
      "2.4 \t0.425576 374\n",
      "2.7 \t0.425873 312\n",
      "3.0 \t0.425292 303\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[388]\ttrain-logloss:0.358773+0.000627986\ttest-logloss:0.426161+0.00431845\n",
      "\n",
      "    1 | 07m03s | \u001b[35m  -0.42616\u001b[0m | \u001b[32m            0.3466\u001b[0m | \u001b[32m   1.6306\u001b[0m | \u001b[32m     5.5204\u001b[0m | \u001b[32m           28.0399\u001b[0m | \u001b[32m     0.9024\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[183]\ttrain-logloss:0.35721+0.0014735\ttest-logloss:0.426745+0.0036933\n",
      "\n",
      "    2 | 04m56s |   -0.42675 |             0.3380 |    0.5470 |      7.8976 |            24.2751 |      0.8069 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[154]\ttrain-logloss:0.347146+0.00117464\ttest-logloss:0.425664+0.00291182\n",
      "\n",
      "    3 | 04m31s | \u001b[35m  -0.42566\u001b[0m | \u001b[32m            0.7156\u001b[0m | \u001b[32m   0.4057\u001b[0m | \u001b[32m     7.5836\u001b[0m | \u001b[32m           15.8769\u001b[0m | \u001b[32m     0.7465\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[358]\ttrain-logloss:0.377837+0.000715617\ttest-logloss:0.427395+0.00370146\n",
      "\n",
      "    4 | 05m39s |   -0.42740 |             0.8726 |    1.2860 |      4.9973 |            24.4800 |      0.6523 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[158]\ttrain-logloss:0.37395+0.00119441\ttest-logloss:0.428736+0.00431196\n",
      "\n",
      "    5 | 05m21s |   -0.42874 |             0.9837 |    0.4768 |      7.8591 |            53.7478 |      0.6877 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[395]\ttrain-logloss:0.344122+0.000586768\ttest-logloss:0.426792+0.00326197\n",
      "\n",
      "    6 | 05m25s |   -0.42679 |             0.3129 |    1.9428 |      7.9591 |            69.9164 |      0.9453 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[162]\ttrain-logloss:0.334714+0.000603453\ttest-logloss:0.426556+0.00334912\n",
      "\n",
      "    7 | 04m41s |   -0.42656 |             0.7037 |    2.0171 |      7.9484 |             8.0902 |      0.6922 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[527]\ttrain-logloss:0.365249+0.000901175\ttest-logloss:0.425653+0.00325423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00016515]), 'nit': 6, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 03m36s | \u001b[35m  -0.42565\u001b[0m | \u001b[32m            0.2508\u001b[0m | \u001b[32m   0.0278\u001b[0m | \u001b[32m     4.5962\u001b[0m | \u001b[32m            8.3928\u001b[0m | \u001b[32m     0.9973\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[643]\ttrain-logloss:0.377441+0.000992163\ttest-logloss:0.427881+0.00354449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.14636005e-05]), 'nit': 5, 'funcalls': 54}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00012244]), 'nit': 5, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 03m57s |   -0.42788 |             0.2256 |    0.2465 |      4.0341 |            69.9510 |      0.9968 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[502]\ttrain-logloss:0.371334+0.000583189\ttest-logloss:0.425545+0.00245458\n",
      "\n",
      "   10 | 03m14s | \u001b[35m  -0.42555\u001b[0m | \u001b[32m            0.2146\u001b[0m | \u001b[32m   2.0760\u001b[0m | \u001b[32m     4.2445\u001b[0m | \u001b[32m           14.4548\u001b[0m | \u001b[32m     0.9245\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[661]\ttrain-logloss:0.368665+0.000838283\ttest-logloss:0.426622+0.0032114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00029747]), 'nit': 5, 'funcalls': 56}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00035729]), 'nit': 6, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 04m14s |   -0.42662 |             0.2436 |    2.0778 |      4.0620 |            41.3385 |      0.9412 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[272]\ttrain-logloss:0.347235+0.00163009\ttest-logloss:0.424918+0.00358875\n",
      "\n",
      "   12 | 03m08s | \u001b[35m  -0.42492\u001b[0m | \u001b[32m            0.2476\u001b[0m | \u001b[32m   0.4003\u001b[0m | \u001b[32m     7.8826\u001b[0m | \u001b[32m           36.3112\u001b[0m | \u001b[32m     0.9922\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-logloss:0.349396+0.00150577\ttest-logloss:0.426233+0.00364197\n",
      "\n",
      "   13 | 06m35s |   -0.42623 |             0.9352 |    2.0559 |      7.9671 |            34.6124 |      0.9537 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[477]\ttrain-logloss:0.375997+0.00104637\ttest-logloss:0.426222+0.00313204\n",
      "\n",
      "   14 | 03m45s |   -0.42622 |             0.3178 |    0.0521 |      4.1670 |            33.6046 |      0.9743 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[164]\ttrain-logloss:0.363201+0.0016479\ttest-logloss:0.428157+0.0035957\n",
      "\n",
      "   15 | 02m07s |   -0.42816 |             0.2016 |    0.4874 |      7.9580 |            13.0222 |      0.6207 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[210]\ttrain-logloss:0.355642+0.00159187\ttest-logloss:0.425848+0.00240647\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -7.22287770e-05]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 05m26s |   -0.42585 |             0.7221 |    0.0770 |      7.9884 |            43.0463 |      0.9828 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[325]\ttrain-logloss:0.352969+0.00194674\ttest-logloss:0.427305+0.00371601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00035871]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 | 03m54s |   -0.42730 |             0.2840 |    0.0568 |      7.9479 |            64.1360 |      0.9865 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[435]\ttrain-logloss:0.367616+0.000803206\ttest-logloss:0.424737+0.003349\n",
      "\n",
      "   18 | 06m44s | \u001b[35m  -0.42474\u001b[0m | \u001b[32m            0.8499\u001b[0m | \u001b[32m   0.0026\u001b[0m | \u001b[32m     4.3620\u001b[0m | \u001b[32m           17.6419\u001b[0m | \u001b[32m     0.9866\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[199]\ttrain-logloss:0.344222+0.00119354\ttest-logloss:0.424839+0.00362764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00046031]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00020465]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 03m33s |   -0.42484 |             0.4236 |    1.8921 |      7.9035 |            18.4537 |      0.9963 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[441]\ttrain-logloss:0.364617+0.00145002\ttest-logloss:0.424751+0.00325523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00010444]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.49065728e-05]), 'nit': 7, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -9.24555201e-05]), 'nit': 5, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 07m47s |   -0.42475 |             0.9746 |    2.0829 |      4.6732 |             8.5282 |      0.9946 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[654]\ttrain-logloss:0.369632+0.000920136\ttest-logloss:0.426423+0.00389196\n",
      "\n",
      "   21 | 06m53s |   -0.42642 |             0.4559 |    2.0627 |      4.0547 |            59.2859 |      0.9929 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[179]\ttrain-logloss:0.339394+0.00122724\ttest-logloss:0.424844+0.00317741\n",
      "\n",
      "   22 | 06m08s |   -0.42484 |             0.9731 |    1.8626 |      7.7017 |            15.2944 |      0.9987 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[195]\ttrain-logloss:0.336653+0.00135039\ttest-logloss:0.425028+0.00340688\n",
      "\n",
      "   23 | 06m06s |   -0.42503 |             0.9063 |    0.2033 |      7.1872 |            18.0017 |      0.9962 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[533]\ttrain-logloss:0.377113+0.000819386\ttest-logloss:0.426812+0.00353955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.49413053e-05]), 'nit': 4, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 03m57s |   -0.42681 |             0.2893 |    0.0796 |      4.0172 |            48.0601 |      0.9857 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[439]\ttrain-logloss:0.36498+0.00123905\ttest-logloss:0.424943+0.00341856\n",
      "\n",
      "   25 | 07m39s |   -0.42494 |             0.9795 |    2.0901 |      4.7571 |            17.7530 |      0.9777 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[334]\ttrain-logloss:0.344931+0.00230646\ttest-logloss:0.426215+0.00318002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00025656]), 'nit': 3, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 03m34s |   -0.42621 |             0.2299 |    2.0216 |      7.6593 |            46.7562 |      0.9772 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-logloss:0.348093+0.00166594\ttest-logloss:0.425047+0.0030231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -3.32569034e-05]), 'nit': 5, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 | 05m14s |   -0.42505 |             0.6706 |    0.0098 |      7.9210 |            31.6615 |      0.9905 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[423]\ttrain-logloss:0.367148+0.00102646\ttest-logloss:0.424996+0.00295159\n",
      "\n",
      "   28 | 07m05s |   -0.42500 |             0.9299 |    0.1161 |      4.1818 |            13.6859 |      0.9938 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[507]\ttrain-logloss:0.38268+0.00110085\ttest-logloss:0.427509+0.00354797\n",
      "\n",
      "   29 | 04m04s |   -0.42751 |             0.3297 |    2.0656 |      4.3555 |            65.5540 |      0.9967 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[217]\ttrain-logloss:0.35519+0.00150823\ttest-logloss:0.42516+0.00325883\n",
      "\n",
      "   30 | 04m21s |   -0.42516 |             0.5166 |    0.0968 |      7.9234 |            39.1613 |      0.9980 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-logloss:0.350247+0.00138142\ttest-logloss:0.424444+0.0031304\n",
      "\n",
      "   31 | 07m27s | \u001b[35m  -0.42444\u001b[0m | \u001b[32m            0.9868\u001b[0m | \u001b[32m   0.0349\u001b[0m | \u001b[32m     5.3138\u001b[0m | \u001b[32m           16.4388\u001b[0m | \u001b[32m     0.9860\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[269]\ttrain-logloss:0.345419+0.000930194\ttest-logloss:0.425033+0.00242359\n",
      "\n",
      "   32 | 04m04s |   -0.42503 |             0.4342 |    1.2471 |      6.1054 |            16.6125 |      0.9957 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[462]\ttrain-logloss:0.363669+0.000884691\ttest-logloss:0.424964+0.00343518\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00200359]), 'nit': 4, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 07m58s |   -0.42496 |             0.9323 |    0.1641 |      4.6704 |            16.4485 |      0.9914 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[333]\ttrain-logloss:0.351626+0.00115566\ttest-logloss:0.426568+0.00304313\n",
      "\n",
      "   34 | 03m42s |   -0.42657 |             0.2255 |    1.4992 |      7.9005 |            58.3629 |      0.9993 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[600]\ttrain-logloss:0.36381+0.000675577\ttest-logloss:0.425611+0.0029427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00432842]), 'nit': 5, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 04m08s |   -0.42561 |             0.2441 |    0.2925 |      4.2201 |            20.7939 |      0.9994 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=y_medium) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='binary:logistic'\n",
    "    params['eval_metric']='logloss',\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=100000, nfold=5,\n",
    "        metrics = 'logloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-logloss-mean'].values[-1]\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (4,8),\n",
    "        'min_child_weight': (8,70),\n",
    "        'colsample_bytree': (0.2,1),\n",
    "        'subsample': (0.6,1),\n",
    "        'gamma': (0,2.1)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.313812</td>\n",
       "      <td>16.438783</td>\n",
       "      <td>0.986761</td>\n",
       "      <td>0.986039</td>\n",
       "      <td>0.034931</td>\n",
       "      <td>-0.424444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.361961</td>\n",
       "      <td>17.641851</td>\n",
       "      <td>0.849905</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>-0.424737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.673238</td>\n",
       "      <td>8.528189</td>\n",
       "      <td>0.974644</td>\n",
       "      <td>0.994641</td>\n",
       "      <td>2.082919</td>\n",
       "      <td>-0.424751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.903491</td>\n",
       "      <td>18.453651</td>\n",
       "      <td>0.423628</td>\n",
       "      <td>0.996315</td>\n",
       "      <td>1.892103</td>\n",
       "      <td>-0.424839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.701706</td>\n",
       "      <td>15.294353</td>\n",
       "      <td>0.973098</td>\n",
       "      <td>0.998693</td>\n",
       "      <td>1.862606</td>\n",
       "      <td>-0.424844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.882566</td>\n",
       "      <td>36.311186</td>\n",
       "      <td>0.247632</td>\n",
       "      <td>0.992209</td>\n",
       "      <td>0.400293</td>\n",
       "      <td>-0.424918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.757087</td>\n",
       "      <td>17.753006</td>\n",
       "      <td>0.979509</td>\n",
       "      <td>0.977735</td>\n",
       "      <td>2.090111</td>\n",
       "      <td>-0.424943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.670405</td>\n",
       "      <td>16.448451</td>\n",
       "      <td>0.932301</td>\n",
       "      <td>0.991362</td>\n",
       "      <td>0.164141</td>\n",
       "      <td>-0.424964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.181783</td>\n",
       "      <td>13.685928</td>\n",
       "      <td>0.929936</td>\n",
       "      <td>0.993809</td>\n",
       "      <td>0.116063</td>\n",
       "      <td>-0.424996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.187237</td>\n",
       "      <td>18.001715</td>\n",
       "      <td>0.906303</td>\n",
       "      <td>0.996229</td>\n",
       "      <td>0.203309</td>\n",
       "      <td>-0.425028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "25   5.313812         16.438783          0.986761   0.986039  0.034931   \n",
       "12   4.361961         17.641851          0.849905   0.986623  0.002646   \n",
       "14   4.673238          8.528189          0.974644   0.994641  2.082919   \n",
       "13   7.903491         18.453651          0.423628   0.996315  1.892103   \n",
       "16   7.701706         15.294353          0.973098   0.998693  1.862606   \n",
       "6    7.882566         36.311186          0.247632   0.992209  0.400293   \n",
       "19   4.757087         17.753006          0.979509   0.977735  2.090111   \n",
       "27   4.670405         16.448451          0.932301   0.991362  0.164141   \n",
       "22   4.181783         13.685928          0.929936   0.993809  0.116063   \n",
       "17   7.187237         18.001715          0.906303   0.996229  0.203309   \n",
       "\n",
       "       score  \n",
       "25 -0.424444  \n",
       "12 -0.424737  \n",
       "14 -0.424751  \n",
       "13 -0.424839  \n",
       "16 -0.424844  \n",
       "6  -0.424918  \n",
       "19 -0.424943  \n",
       "27 -0.424964  \n",
       "22 -0.424996  \n",
       "17 -0.425028  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.986761,\n",
      "       gamma=0.034931, learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=16, missing=None, n_estimators=1000000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.986039)\n",
      "Model 1 fold 1\n",
      "best round 3668\n",
      "('Score: ', 0.42200046767533278)\n",
      "Model 1 fold 1 fitting finished in 943.465s\n",
      "Model 1 fold 2\n",
      "best round 4558\n",
      "('Score: ', 0.40364797993256535)\n",
      "Model 1 fold 2 fitting finished in 1149.859s\n",
      "Model 1 fold 3\n",
      "best round 3037\n",
      "('Score: ', 0.4236135890042792)\n",
      "Model 1 fold 3 fitting finished in 787.461s\n",
      "Model 1 fold 4\n",
      "best round 4561\n",
      "('Score: ', 0.41215656720418348)\n",
      "Model 1 fold 4 fitting finished in 1145.537s\n",
      "Model 1 fold 5\n",
      "best round 2795\n",
      "('Score: ', 0.42871615508426603)\n",
      "Model 1 fold 5 fitting finished in 730.797s\n",
      "Model 1 fold 6\n",
      "best round 2622\n",
      "('Score: ', 0.41904672679061183)\n",
      "Model 1 fold 6 fitting finished in 691.319s\n",
      "Model 1 fold 7\n",
      "best round 2932\n",
      "('Score: ', 0.4283365926904516)\n",
      "Model 1 fold 7 fitting finished in 761.145s\n",
      "Model 1 fold 8\n",
      "best round 3904\n",
      "('Score: ', 0.43200023745861232)\n",
      "Model 1 fold 8 fitting finished in 993.398s\n",
      "Model 1 fold 9\n",
      "best round 3321\n",
      "('Score: ', 0.42599247538767648)\n",
      "Model 1 fold 9 fitting finished in 860.331s\n",
      "Model 1 fold 10\n",
      "best round 4406\n",
      "('Score: ', 0.42670703332468346)\n",
      "Model 1 fold 10 fitting finished in 1109.492s\n",
      "Score for model 1 is 0.422222\n",
      "Score for blended models is 0.422222\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "            xgb.XGBClassifier(max_depth = 5,\n",
    "                              min_child_weight = 16,\n",
    "                              colsample_bytree = 0.986761 ,\n",
    "                              subsample = 0.986039 ,\n",
    "                              gamma = 0.034931)          \n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "(train_blend_x_xgb_medium,\n",
    " test_blend_x_xgb_mean_medium,\n",
    " test_blend_x_xgb_gmean_medium,\n",
    " blend_scores_xgb_medium,\n",
    " best_rounds_xgb_medium) = xgb_blend(estimators,\n",
    "                              train_X,y_medium,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.42222178]\n",
      "[ 3580.4]\n"
     ]
    }
   ],
   "source": [
    "name_train_blend = '../output/train_blend_xgb_medium_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_medium_mean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_medium_gmean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb_medium,axis=0))\n",
    "print (np.mean(best_rounds_xgb_medium,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb_medium, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_xgb_mean_medium, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_xgb_gmean_medium, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3839\n"
     ]
    }
   ],
   "source": [
    "y_high =[]\n",
    "for i in range(train_X.shape[0]):\n",
    "    y_high.append(1 if train_y[i] == 2 else 0)\n",
    "    \n",
    "y_high = np.array(y_high)  \n",
    "print np.sum(y_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39481, 322)\n",
      "(9871, 322)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, y_high, train_size=.80, random_state=1234)\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "# xgtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t0.177956 678\n",
      "4 \t0.176987 375\n",
      "5 \t0.178364 301\n",
      "6 \t0.179676 151\n",
      "7 \t0.179587 172\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "best_score = 1000\n",
    "train_param = 0\n",
    "for x in [3,4,5,6,7]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= x,\n",
    "        nthread = -1,\n",
    "        silent = False\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_depth = train_param\n",
    "print max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t0.176475 476\n",
      "4 \t0.177699 451\n",
      "8 \t0.178257 433\n",
      "12 \t0.180064 479\n",
      "16 \t0.179598 329\n",
      "20 \t0.179727 313\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [2,4,8,12,16,20]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "min_child_weight = train_param\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 \t0.178819 940\n",
      "0.1 \t0.178555 481\n",
      "0.2 \t0.177601 371\n",
      "0.3 \t0.175319 479\n",
      "0.4 \t0.175341 517\n",
      "0.5 \t0.176634 492\n",
      "0.6 \t0.17636 555\n",
      "0.7 \t0.177471 524\n",
      "0.8 \t0.178216 345\n",
      "0.9 \t0.177681 500\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = train_param\n",
    "print colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t0.178006 396\n",
      "0.6 \t0.177543 414\n",
      "0.7 \t0.177879 388\n",
      "0.8 \t0.177606 492\n",
      "0.9 \t0.176629 382\n"
     ]
    }
   ],
   "source": [
    "train_param = 1\n",
    "for x in [0.5,0.6,0.7,0.8,0.9]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "subsample = train_param\n",
    "print subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 \t0.176517 342\n",
      "0.6 \t0.175433 424\n",
      "0.9 \t0.17573 444\n",
      "1.2 \t0.175961 486\n",
      "1.5 \t0.176763 389\n",
      "1.8 \t0.176244 560\n",
      "2.1 \t0.175417 433\n",
      "2.4 \t0.176278 511\n",
      "2.7 \t0.176951 486\n",
      "3.0 \t0.176515 395\n"
     ]
    }
   ],
   "source": [
    "train_param = 0\n",
    "for x in [0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3.0]:\n",
    "    rgr = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed = 1234, # use a fixed seed during tuning so we can reproduce the results\n",
    "        learning_rate = learning_rate,\n",
    "        n_estimators = 10000,\n",
    "        max_depth= max_depth,\n",
    "        nthread = -1,\n",
    "        silent = False,\n",
    "        min_child_weight = min_child_weight,\n",
    "        colsample_bytree = colsample_bytree,\n",
    "        subsample = subsample,\n",
    "        gamma = x\n",
    "    )\n",
    "    rgr.fit(\n",
    "        X_train,y_train,\n",
    "        eval_set=[(X_val,y_val)],\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    if rgr.best_score < best_score:\n",
    "        best_score = rgr.best_score\n",
    "        train_param = x\n",
    "        \n",
    "\n",
    "    print x, '\\t', rgr.best_score, rgr.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "gamma = train_param\n",
    "print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[395]\ttrain-logloss:0.10991+0.000920344\ttest-logloss:0.171475+0.00697916\n",
      "\n",
      "    1 | 02m51s | \u001b[35m  -0.17148\u001b[0m | \u001b[32m            0.2550\u001b[0m | \u001b[32m   2.3294\u001b[0m | \u001b[32m     5.5204\u001b[0m | \u001b[32m            4.5555\u001b[0m | \u001b[32m     0.9268\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[209]\ttrain-logloss:0.090857+0.00129286\ttest-logloss:0.173007+0.00720012\n",
      "\n",
      "    2 | 02m12s |   -0.17301 |             0.2518 |    0.7814 |      7.8976 |             3.8875 |      0.8552 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-logloss:0.0946596+0.00105068\ttest-logloss:0.172658+0.00724233\n",
      "\n",
      "    3 | 02m34s |   -0.17266 |             0.3933 |    0.5795 |      7.5836 |             2.3975 |      0.8099 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[418]\ttrain-logloss:0.122902+0.0016401\ttest-logloss:0.172426+0.00718046\n",
      "\n",
      "    4 | 03m50s |   -0.17243 |             0.4522 |    1.8372 |      4.9973 |             3.9239 |      0.7392 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[188]\ttrain-logloss:0.104224+0.00108795\ttest-logloss:0.172989+0.00678304\n",
      "\n",
      "    5 | 03m25s |   -0.17299 |             0.4939 |    0.6812 |      7.8591 |             9.1165 |      0.7657 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-logloss:0.090862+0.000834205\ttest-logloss:0.172608+0.00702364\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00013056]), 'nit': 5, 'funcalls': 57}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6 | 04m20s |   -0.17261 |             0.4121 |    2.9446 |      7.9800 |             1.2759 |      0.8209 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[432]\ttrain-logloss:0.131106+0.001821\ttest-logloss:0.172601+0.00671771\n",
      "\n",
      "    7 | 02m50s |   -0.17260 |             0.2096 |    0.3522 |      4.0341 |            11.9913 |      0.9976 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[264]\ttrain-logloss:0.105272+0.00142327\ttest-logloss:0.172168+0.00669271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  5.10794462e-05]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 03m07s |   -0.17217 |             0.2448 |    2.9061 |      7.9759 |            11.9230 |      0.9290 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[568]\ttrain-logloss:0.11638+0.00129432\ttest-logloss:0.171954+0.0070856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.06474124e-05]), 'nit': 4, 'funcalls': 54}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00028104]), 'nit': 5, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 03m34s |   -0.17195 |             0.2224 |    0.4372 |      4.0260 |             1.2505 |      0.9796 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[522]\ttrain-logloss:0.120977+0.00112013\ttest-logloss:0.172069+0.00699078\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00049786]), 'nit': 5, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.42549898e-05]), 'nit': 4, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10 | 05m03s |   -0.17207 |             0.4547 |    2.9764 |      4.1430 |             8.7463 |      0.9578 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[657]\ttrain-logloss:0.1165+0.00160376\ttest-logloss:0.171697+0.00704755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00080788]), 'nit': 8, 'funcalls': 57}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 06m22s |   -0.17170 |             0.4905 |    2.9756 |      4.6732 |             1.0937 |      0.9960 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[286]\ttrain-logloss:0.103074+0.00130976\ttest-logloss:0.171855+0.00707092\n",
      "\n",
      "   12 | 04m35s |   -0.17185 |             0.4980 |    2.9914 |      6.7088 |             6.4224 |      0.9916 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[525]\ttrain-logloss:0.123516+0.00142058\ttest-logloss:0.172031+0.007291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -8.20849623e-05]), 'nit': 7, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 | 03m41s |   -0.17203 |             0.2702 |    2.7764 |      4.0515 |             5.3757 |      0.9901 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[334]\ttrain-logloss:0.109252+0.000367714\ttest-logloss:0.171845+0.00662825\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00024412]), 'nit': 8, 'funcalls': 59}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00014104]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 02m57s |   -0.17184 |             0.2002 |    2.9781 |      6.4376 |             8.6927 |      0.9748 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[581]\ttrain-logloss:0.122602+0.00149505\ttest-logloss:0.172046+0.00723664\n",
      "\n",
      "   15 | 05m49s |   -0.17205 |             0.3121 |    2.9644 |      4.2976 |            11.7543 |      0.9802 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[328]\ttrain-logloss:0.102556+0.000794462\ttest-logloss:0.171938+0.00698952\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00079087]), 'nit': 4, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0001827]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 05m57s |   -0.17194 |             0.2235 |    2.9580 |      6.4603 |             2.9046 |      0.9967 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[499]\ttrain-logloss:0.119596+0.00130554\ttest-logloss:0.171646+0.00634939\n",
      "\n",
      "   17 | 08m39s |   -0.17165 |             0.4331 |    0.1608 |      4.9610 |             5.8846 |      0.9953 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[271]\ttrain-logloss:0.0948834+0.00106594\ttest-logloss:0.171673+0.00649617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00020811]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 05m05s |   -0.17167 |             0.2203 |    0.0851 |      6.3433 |             1.1436 |      0.9923 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[269]\ttrain-logloss:0.107222+0.00135952\ttest-logloss:0.171361+0.0067242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -5.10373793e-05]), 'nit': 6, 'funcalls': 50}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 02m47s | \u001b[35m  -0.17136\u001b[0m | \u001b[32m            0.2322\u001b[0m | \u001b[32m   1.9047\u001b[0m | \u001b[32m     6.0060\u001b[0m | \u001b[32m            6.4164\u001b[0m | \u001b[32m     0.9998\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[359]\ttrain-logloss:0.107731+0.000745078\ttest-logloss:0.171857+0.00724305\n",
      "\n",
      "   20 | 04m47s |   -0.17186 |             0.4982 |    1.6629 |      5.4155 |             4.9300 |      0.9863 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[575]\ttrain-logloss:0.1201+0.00111413\ttest-logloss:0.172123+0.00709367\n",
      "\n",
      "   21 | 03m33s |   -0.17212 |             0.2074 |    0.0822 |      4.2652 |             8.3653 |      0.9938 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[244]\ttrain-logloss:0.0977132+0.00141945\ttest-logloss:0.17175+0.00651168\n",
      "\n",
      "   22 | 03m05s |   -0.17175 |             0.2526 |    0.0029 |      7.8759 |            11.8411 |      0.9853 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[482]\ttrain-logloss:0.109979+0.00129907\ttest-logloss:0.172015+0.00682953\n",
      "\n",
      "   23 | 03m46s |   -0.17201 |             0.2257 |    2.9905 |      5.5518 |             6.1160 |      0.9558 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[950]\ttrain-logloss:0.116448+0.00126878\ttest-logloss:0.171714+0.0070368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -1.97015339e-05]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -3.66870727e-05]), 'nit': 3, 'funcalls': 47}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 05m43s |   -0.17171 |             0.2364 |    2.9264 |      4.0639 |             2.5485 |      0.9999 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[271]\ttrain-logloss:0.103837+0.00170992\ttest-logloss:0.171359+0.00740406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.34927866e-05]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 02m50s | \u001b[35m  -0.17136\u001b[0m | \u001b[32m            0.2320\u001b[0m | \u001b[32m   0.2116\u001b[0m | \u001b[32m     6.5459\u001b[0m | \u001b[32m            6.8521\u001b[0m | \u001b[32m     0.9980\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[312]\ttrain-logloss:0.107192+0.00144881\ttest-logloss:0.172138+0.00688724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -3.29473173e-05]), 'nit': 6, 'funcalls': 56}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 02m58s |   -0.17214 |             0.2028 |    1.6719 |      6.2454 |            11.2442 |      0.9876 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[362]\ttrain-logloss:0.111562+0.00110633\ttest-logloss:0.17147+0.00672054\n",
      "\n",
      "   27 | 03m22s |   -0.17147 |             0.2613 |    0.5547 |      5.5618 |             6.3850 |      0.9888 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[286]\ttrain-logloss:0.0984188+0.000790129\ttest-logloss:0.171823+0.00691752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ -2.58826442e-05]), 'nit': 6, 'funcalls': 53}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([ 0.00067708]), 'nit': 4, 'funcalls': 48}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 02m50s |   -0.17182 |             0.2071 |    1.9347 |      6.9774 |             1.0441 |      0.9981 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[389]\ttrain-logloss:0.110262+0.00149144\ttest-logloss:0.17156+0.00677134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  1.89420534e-05]), 'nit': 5, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 03m11s |   -0.17156 |             0.2236 |    0.7930 |      5.8304 |             6.8156 |      0.9996 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[337]\ttrain-logloss:0.116402+0.00127411\ttest-logloss:0.171859+0.00648323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.0011534]), 'nit': 6, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 02m49s |   -0.17186 |             0.2034 |    0.0873 |      5.7365 |             5.2219 |      0.9451 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[160]\ttrain-logloss:0.091836+0.00106874\ttest-logloss:0.172203+0.00675766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00016956]), 'nit': 6, 'funcalls': 51}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([  6.10933203e-05]), 'nit': 6, 'funcalls': 56}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31 | 03m03s |   -0.17220 |             0.3822 |    0.0227 |      7.9737 |             1.1807 |      0.9788 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[259]\ttrain-logloss:0.106361+0.000989804\ttest-logloss:0.171354+0.0066011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00059224]), 'nit': 4, 'funcalls': 52}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00168675]), 'nit': 5, 'funcalls': 55}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32 | 03m02s | \u001b[35m  -0.17135\u001b[0m | \u001b[32m            0.2549\u001b[0m | \u001b[32m   1.5280\u001b[0m | \u001b[32m     6.7523\u001b[0m | \u001b[32m            6.3547\u001b[0m | \u001b[32m     0.9987\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[393]\ttrain-logloss:0.10844+0.000894609\ttest-logloss:0.171926+0.00665133\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xujin/AI/anaconda2/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'task': 'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-0.00578458]), 'nit': 3, 'funcalls': 49}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33 | 03m37s |   -0.17193 |             0.2571 |    0.1684 |      5.9816 |             6.3250 |      0.9860 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[225]\ttrain-logloss:0.104176+0.00125071\ttest-logloss:0.172456+0.00668234\n",
      "\n",
      "   34 | 02m58s |   -0.17246 |             0.2062 |    2.5126 |      7.7983 |             7.7306 |      0.9928 | \n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[554]\ttrain-logloss:0.118528+0.00124893\ttest-logloss:0.1722+0.00679825\n",
      "\n",
      "   35 | 04m01s |   -0.17220 |             0.2422 |    0.2084 |      4.0440 |             4.0712 |      0.9859 | \n"
     ]
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_X, label=y_high) \n",
    "\n",
    "def xgb_evaluate(min_child_weight, colsample_bytree, max_depth, subsample, gamma):\n",
    "    params = dict()\n",
    "    params['objective']='binary:logistic'\n",
    "    params['eval_metric']='logloss',\n",
    "    params['silent']=1\n",
    "    params['eta'] = 0.1\n",
    "    params['verbose_eval'] = True\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    \n",
    "    cv_result = xgb.cv(\n",
    "        params, xgtrain, \n",
    "        num_boost_round=100000, nfold=5,\n",
    "        metrics = 'logloss',\n",
    "        seed=seed,callbacks=[xgb.callback.early_stop(50)]\n",
    "    )\n",
    "    \n",
    "    return -cv_result['test-logloss-mean'].values[-1]\n",
    "\n",
    "xgb_BO = BayesianOptimization(\n",
    "    xgb_evaluate, \n",
    "    {\n",
    "        'max_depth': (4,8),\n",
    "        'min_child_weight': (1,12),\n",
    "        'colsample_bytree': (0.2,0.5),\n",
    "        'subsample': (0.7,1),\n",
    "        'gamma': (0,3)\n",
    "    }\n",
    ")\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.752262</td>\n",
       "      <td>6.354654</td>\n",
       "      <td>0.254931</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>1.528020</td>\n",
       "      <td>-0.171354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.545895</td>\n",
       "      <td>6.852102</td>\n",
       "      <td>0.232032</td>\n",
       "      <td>0.998036</td>\n",
       "      <td>0.211568</td>\n",
       "      <td>-0.171359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.005964</td>\n",
       "      <td>6.416440</td>\n",
       "      <td>0.232230</td>\n",
       "      <td>0.999790</td>\n",
       "      <td>1.904689</td>\n",
       "      <td>-0.171361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.561770</td>\n",
       "      <td>6.384971</td>\n",
       "      <td>0.261301</td>\n",
       "      <td>0.988807</td>\n",
       "      <td>0.554664</td>\n",
       "      <td>-0.171470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.830431</td>\n",
       "      <td>6.815567</td>\n",
       "      <td>0.223637</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.792956</td>\n",
       "      <td>-0.171560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.961041</td>\n",
       "      <td>5.884557</td>\n",
       "      <td>0.433066</td>\n",
       "      <td>0.995258</td>\n",
       "      <td>0.160786</td>\n",
       "      <td>-0.171646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.343270</td>\n",
       "      <td>1.143575</td>\n",
       "      <td>0.220345</td>\n",
       "      <td>0.992262</td>\n",
       "      <td>0.085061</td>\n",
       "      <td>-0.171673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.673238</td>\n",
       "      <td>1.093711</td>\n",
       "      <td>0.490491</td>\n",
       "      <td>0.995980</td>\n",
       "      <td>2.975598</td>\n",
       "      <td>-0.171697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.063896</td>\n",
       "      <td>2.548489</td>\n",
       "      <td>0.236434</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>2.926362</td>\n",
       "      <td>-0.171714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.875869</td>\n",
       "      <td>11.841097</td>\n",
       "      <td>0.252646</td>\n",
       "      <td>0.985262</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>-0.171750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  subsample     gamma  \\\n",
       "26   6.752262          6.354654          0.254931   0.998690  1.528020   \n",
       "19   6.545895          6.852102          0.232032   0.998036  0.211568   \n",
       "13   6.005964          6.416440          0.232230   0.999790  1.904689   \n",
       "21   5.561770          6.384971          0.261301   0.988807  0.554664   \n",
       "23   5.830431          6.815567          0.223637   0.999604  0.792956   \n",
       "11   4.961041          5.884557          0.433066   0.995258  0.160786   \n",
       "12   6.343270          1.143575          0.220345   0.992262  0.085061   \n",
       "5    4.673238          1.093711          0.490491   0.995980  2.975598   \n",
       "18   4.063896          2.548489          0.236434   0.999930  2.926362   \n",
       "16   7.875869         11.841097          0.252646   0.985262  0.002860   \n",
       "\n",
       "       score  \n",
       "26 -0.171354  \n",
       "19 -0.171359  \n",
       "13 -0.171361  \n",
       "21 -0.171470  \n",
       "23 -0.171560  \n",
       "11 -0.171646  \n",
       "12 -0.171673  \n",
       "5  -0.171697  \n",
       "18 -0.171714  \n",
       "16 -0.171750  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bo_scores = pd.DataFrame([[s[0]['max_depth'],\n",
    "                               s[0]['min_child_weight'],\n",
    "                               s[0]['colsample_bytree'],\n",
    "                               s[0]['subsample'],\n",
    "                               s[0]['gamma'],\n",
    "                               s[1]] for s in zip(xgb_BO.res['all']['params'],xgb_BO.res['all']['values'])],\n",
    "                            columns = ['max_depth',\n",
    "                                       'min_child_weight',\n",
    "                                       'colsample_bytree',\n",
    "                                       'subsample',\n",
    "                                       'gamma',\n",
    "                                       'score'])\n",
    "xgb_bo_scores=xgb_bo_scores.sort_values('score',ascending=False)\n",
    "xgb_bo_scores.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blend 1 estimators for 10 folds\n",
      "Model 1: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.254931,\n",
      "       gamma=1.52802, learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=6, missing=None, n_estimators=1000000, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=False, subsample=0.99869)\n",
      "Model 1 fold 1\n",
      "best round 3593\n",
      "('Score: ', 0.16040199197453783)\n",
      "Model 1 fold 1 fitting finished in 409.761s\n",
      "Model 1 fold 2\n",
      "best round 4291\n",
      "('Score: ', 0.16251477268862224)\n",
      "Model 1 fold 2 fitting finished in 438.511s\n",
      "Model 1 fold 3\n",
      "best round 3230\n",
      "('Score: ', 0.1726481059179639)\n",
      "Model 1 fold 3 fitting finished in 338.188s\n",
      "Model 1 fold 4\n",
      "best round 4015\n",
      "('Score: ', 0.15474943988224088)\n",
      "Model 1 fold 4 fitting finished in 413.777s\n",
      "Model 1 fold 5\n",
      "best round 3320\n",
      "('Score: ', 0.17994719447588786)\n",
      "Model 1 fold 5 fitting finished in 345.771s\n",
      "Model 1 fold 6\n",
      "best round 2799\n",
      "('Score: ', 0.16513287584688019)\n",
      "Model 1 fold 6 fitting finished in 296.985s\n",
      "Model 1 fold 7\n",
      "best round 2567\n",
      "('Score: ', 0.17068274446354981)\n",
      "Model 1 fold 7 fitting finished in 275.847s\n",
      "Model 1 fold 8\n",
      "best round 3215\n",
      "('Score: ', 0.17833750042590452)\n",
      "Model 1 fold 8 fitting finished in 336.215s\n",
      "Model 1 fold 9\n",
      "best round 3806\n",
      "('Score: ', 0.1739359317454918)\n",
      "Model 1 fold 9 fitting finished in 393.368s\n",
      "Model 1 fold 10\n",
      "best round 3229\n",
      "('Score: ', 0.16281922497778245)\n",
      "Model 1 fold 10 fitting finished in 337.275s\n",
      "Score for model 1 is 0.168117\n",
      "Score for blended models is 0.168117\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "            xgb.XGBClassifier(max_depth = 6,\n",
    "                              min_child_weight = 6,\n",
    "                              colsample_bytree = 0.254931 ,\n",
    "                              subsample = 0.998690 ,\n",
    "                              gamma = 1.528020)          \n",
    "             ]\n",
    "\n",
    "\n",
    "\n",
    "(train_blend_x_xgb_high,\n",
    " test_blend_x_xgb_mean_high,\n",
    " test_blend_x_xgb_gmean_high,\n",
    " blend_scores_xgb_high,\n",
    " best_rounds_xgb_high) = xgb_blend(estimators,\n",
    "                              train_X,y_high,\n",
    "                              test_X,\n",
    "                              10,\n",
    "                              300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16811698]\n",
      "[ 3406.5]\n"
     ]
    }
   ],
   "source": [
    "name_train_blend = '../output/train_blend_xgb_high_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_high_mean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_high_gmean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "print (np.mean(blend_scores_xgb_high,axis=0))\n",
    "print (np.mean(best_rounds_xgb_high,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb_high, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_xgb_mean_high, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_xgb_gmean_high, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4236182 ,  0.5763818 ],\n",
       "       [ 0.59415483,  0.40584514],\n",
       "       [ 0.21109557,  0.78890443],\n",
       "       [ 0.19131744,  0.80868256],\n",
       "       [ 0.02921897,  0.97078103],\n",
       "       [ 0.28675401,  0.71324599],\n",
       "       [ 0.24010056,  0.75989944],\n",
       "       [ 0.64165068,  0.35834932],\n",
       "       [ 0.04677582,  0.95322418],\n",
       "       [ 0.02284718,  0.97715282]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb_low[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61000806,  0.38999194],\n",
       "       [ 0.53991276,  0.46008724],\n",
       "       [ 0.79552573,  0.20447429],\n",
       "       [ 0.72584319,  0.27415678],\n",
       "       [ 0.95426494,  0.04573506],\n",
       "       [ 0.80321014,  0.19678983],\n",
       "       [ 0.75437468,  0.2456253 ],\n",
       "       [ 0.69807637,  0.30192366],\n",
       "       [ 0.87716442,  0.1228356 ],\n",
       "       [ 0.97949302,  0.02050696]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb_medium[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.67689753e-01,   3.23102176e-02],\n",
       "       [  9.45587397e-01,   5.44126257e-02],\n",
       "       [  9.81699526e-01,   1.83004793e-02],\n",
       "       [  9.96058881e-01,   3.94109543e-03],\n",
       "       [  9.99282181e-01,   7.17798830e-04],\n",
       "       [  9.63740110e-01,   3.62598673e-02],\n",
       "       [  9.90686119e-01,   9.31385346e-03],\n",
       "       [  7.30362296e-01,   2.69637674e-01],\n",
       "       [  9.84624028e-01,   1.53759578e-02],\n",
       "       [  9.99281704e-01,   7.18279567e-04]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb_high[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb = np.vstack([train_blend_x_xgb_low[:,1],train_blend_x_xgb_medium[:,1],train_blend_x_xgb_high[:,1]]).T\n",
    "train_blend_x_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.76381803e-01,   3.89991939e-01,   3.23102176e-02],\n",
       "       [  4.05845135e-01,   4.60087240e-01,   5.44126257e-02],\n",
       "       [  7.88904428e-01,   2.04474285e-01,   1.83004793e-02],\n",
       "       [  8.08682561e-01,   2.74156779e-01,   3.94109543e-03],\n",
       "       [  9.70781028e-01,   4.57350612e-02,   7.17798830e-04],\n",
       "       [  7.13245988e-01,   1.96789831e-01,   3.62598673e-02],\n",
       "       [  7.59899437e-01,   2.45625302e-01,   9.31385346e-03],\n",
       "       [  3.58349323e-01,   3.01923662e-01,   2.69637674e-01],\n",
       "       [  9.53224182e-01,   1.22835599e-01,   1.53759578e-02],\n",
       "       [  9.77152824e-01,   2.05069594e-02,   7.18279567e-04]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_blend_x_xgb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52698075424503898"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train_y,train_blend_x_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb_mean = np.vstack([test_blend_x_xgb_mean_low[:,1],test_blend_x_xgb_mean_medium[:,1],test_blend_x_xgb_mean_high[:,1]]).T\n",
    "test_blend_x_xgb_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 3)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blend_x_xgb_gmean = np.vstack([test_blend_x_xgb_gmean_low[:,1],test_blend_x_xgb_gmean_medium[:,1],test_blend_x_xgb_gmean_high[:,1]]).T\n",
    "test_blend_x_xgb_gmean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_train_blend = '../output/train_blend_xgb_ovr_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_mean = '../output/test_blend_xgb_ovr_mean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "name_test_blend_gmean = '../output/test_blend_xgb_ovr_gmean_BM_0322_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "\n",
    "# print (np.mean(blend_scores_xgb,axis=0))\n",
    "# print (np.mean(best_rounds_xgb,axis=0))\n",
    "np.savetxt(name_train_blend,train_blend_x_xgb, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_mean,test_blend_x_xgb_mean, delimiter=\",\")\n",
    "np.savetxt(name_test_blend_gmean,test_blend_x_xgb_gmean, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [ 0.52385999  0.52420308  0.52429754  0.52366222  0.52450185]\n",
    "# [ 2866.7  3979.7  3102.9  2783.1  4450.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now = datetime.now()\n",
    "sub_name = '../output/sub_xgb_ovr_BM_0322_mean_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "\n",
    "out_df = pd.DataFrame(test_blend_x_xgb_mean)\n",
    "out_df.columns = [\"low\", \"medium\", \"high\"]\n",
    "out_df[\"listing_id\"] = sub_id\n",
    "out_df.to_csv(sub_name, index=False)\n",
    "\n",
    "\n",
    "# ypreds.columns = cols\n",
    "\n",
    "# df = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "# ypreds['listing_id'] = df[\"listing_id\"]\n",
    "\n",
    "# ypreds.to_csv('my_preds.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
